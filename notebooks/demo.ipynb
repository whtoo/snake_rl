{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习 Atari 游戏 Demo - Rainbow DQN\n",
    "\n",
    "本笔记本演示如何使用 **Rainbow DQN** 算法训练智能体玩 Atari 游戏 Assault-v5。\n",
    "\n",
    "Rainbow DQN 集成了多种 DQN 改进技术：Double DQN、Dueling DQN、Prioritized Replay、Multi-step Learning、Noisy Networks 和 Distributional DQN。\n",
    "\n",
    "## 目录\n",
    "1. [环境设置](#环境设置)\n",
    "2. [数据预处理](#数据预处理)\n",
    "3. [模型架构](#模型架构)\n",
    "4. [训练过程](#训练过程)\n",
    "5. [结果可视化](#结果可视化)\n",
    "6. [模型评估](#模型评估)\n",
    "7. [总结](#总结)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from IPython.display import HTML, display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 导入自定义模块\n",
    "from model import DQN, DuelingDQN, RainbowDQN  # 添加 Rainbow DQN\n",
    "from agent import DQNAgent, RainbowAgent  # 添加 Rainbow Agent\n",
    "from utils import make_env, plot_rewards, record_video\n",
    "\n",
    "# 设置matplotlib中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_built() \\\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"使用设备: {device}\")\n",
    "print(\"环境设置完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境设置\n",
    "\n",
    "首先创建游戏环境并查看其基本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建环境\n",
    "env_name = 'ALE/Assault-v5'\n",
    "env = make_env(env_name)\n",
    "\n",
    "print(f\"环境名称: {env_name}\")\n",
    "print(f\"观测空间: {env.observation_space}\")\n",
    "print(f\"动作空间: {env.action_space}\")\n",
    "print(f\"动作数量: {env.action_space.n}\")\n",
    "\n",
    "# 重置环境并显示初始状态\n",
    "state, _ = env.reset()\n",
    "print(f\"状态形状: {state.shape}\")\n",
    "print(f\"状态数据类型: {state.dtype}\")\n",
    "print(f\"状态值范围: [{state.min()}, {state.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "查看预处理后的游戏画面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示预处理后的状态\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for i in range(4):\n",
    "    axes[i].imshow(state[i], cmap='gray')\n",
    "    axes[i].set_title(f'帧 {i+1}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('预处理后的游戏状态（4帧堆叠）')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型架构\n",
    "\n",
    "创建并查看 Rainbow DQN 模型的架构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置设备\n",
    "device = torch.device('mps' if torch.backends.mps.is_built() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'使用设备: {device}')\n",
    "\n",
    "# 创建模型\n",
    "input_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# 创建 Rainbow DQN 模型\n",
    "rainbow_model = RainbowDQN(\n",
    "    input_shape=input_shape, \n",
    "    n_actions=n_actions,\n",
    "    use_noisy=True,          # 使用噪声网络\n",
    "    use_distributional=True, # 使用分布式Q学习\n",
    "    n_atoms=51,              # 分布式Q学习的原子数\n",
    "    v_min=-10,               # 值函数最小值\n",
    "    v_max=10                 # 值函数最大值\n",
    ")\n",
    "\n",
    "print(\"Rainbow DQN模型架构:\")\n",
    "print(rainbow_model)\n",
    "\n",
    "# 创建简化版 Rainbow DQN（仅双网络和优先回放）\n",
    "rainbow_simple = RainbowDQN(\n",
    "    input_shape=input_shape, \n",
    "    n_actions=n_actions,\n",
    "    use_noisy=False,         # 不使用噪声网络\n",
    "    use_distributional=False # 不使用分布式Q学习\n",
    ")\n",
    "\n",
    "print(\"\\n简化版 Rainbow DQN模型:\")\n",
    "print(rainbow_simple)\n",
    "\n",
    "# 计算模型参数数量\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nRainbow DQN参数数量: {count_parameters(rainbow_model):,}\")\n",
    "print(f\"简化版 Rainbow DQN参数数量: {count_parameters(rainbow_simple):,}\")\n",
    "\n",
    "# Rainbow DQN 特性说明\n",
    "print(\"\\n=== Rainbow DQN 特性 ===\")\n",
    "print(f\"✅ 噪声网络: {'启用' if rainbow_model.use_noisy else '禁用'}\")\n",
    "print(f\"✅ 分布式Q学习: {'启用' if rainbow_model.use_distributional else '禁用'}\")\n",
    "print(f\"✅ Dueling 架构: 已启用\")\n",
    "print(f\"✅ Double DQN: 已启用\")\n",
    "print(f\"✅ 优先回放: 将在智能体中启用\")\n",
    "print(f\"✅ 多步学习: 将在智能体中启用\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练过程\n",
    "\n",
    "演示如何训练 Rainbow DQN 智能体（简化版本，用于演示）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 Rainbow DQN 智能体\n",
    "model = RainbowDQN(\n",
    "    input_shape=input_shape, \n",
    "    n_actions=n_actions,\n",
    "    use_noisy=True,          # 使用噪声网络替代 epsilon-greedy\n",
    "    use_distributional=True, # 使用分布式Q学习\n",
    "    n_atoms=51,\n",
    "    v_min=-10,\n",
    "    v_max=10\n",
    ")\n",
    "\n",
    "target_model = RainbowDQN(\n",
    "    input_shape=input_shape, \n",
    "    n_actions=n_actions,\n",
    "    use_noisy=True,\n",
    "    use_distributional=True,\n",
    "    n_atoms=51,\n",
    "    v_min=-10,\n",
    "    v_max=10\n",
    ")\n",
    "\n",
    "agent = RainbowAgent(\n",
    "    model=model,\n",
    "    target_model=target_model,\n",
    "    env=env,\n",
    "    device=device,\n",
    "    buffer_size=10000,      # 较小的缓冲区用于演示\n",
    "    batch_size=32,\n",
    "    gamma=0.99,\n",
    "    lr=1e-4,\n",
    "    target_update=100,\n",
    "    n_step=3,               # 3步学习\n",
    "    use_noisy=True,         # 使用噪声网络\n",
    "    use_distributional=True, # 使用分布式Q学习\n",
    "    prioritized_replay=True  # 使用优先经验回放\n",
    ")\n",
    "\n",
    "print(\"Rainbow DQN智能体创建完成！\")\n",
    "print(f\"- 使用噪声网络: {agent.use_noisy}\")\n",
    "print(f\"- 使用分布式Q学习: {agent.use_distributional}\")\n",
    "print(f\"- 使用优先回放: {agent.prioritized_replay}\")\n",
    "print(f\"- N步学习: {agent.n_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rainbow DQN 训练循环（支持 Rainbow 特性）\n",
    "def train_rainbow_demo(agent, env, n_episodes=50):\n",
    "    rewards = []\n",
    "    losses = []\n",
    "    td_errors = []  # 记录TD误差用于监控优先回放效果\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes), desc=\"Rainbow训练中\"):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = 0\n",
    "        episode_td_error = 0\n",
    "        steps = 0\n",
    "        \n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not (done or truncated) and steps < 1000:  # 限制步数\n",
    "            # Rainbow 智能体选择动作（使用噪声网络或 epsilon-greedy）\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            \n",
    "            # 使用 Rainbow 特有的经验存储（支持 N 步学习）\n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            \n",
    "            # 更新模型\n",
    "            loss = agent.update_model()\n",
    "            if loss is not None:\n",
    "                episode_loss += loss\n",
    "            \n",
    "            # 更新目标网络\n",
    "            if agent.steps_done % agent.target_update == 0:\n",
    "                agent.update_target_model()\n",
    "            \n",
    "            # 为噪声网络重新采样噪声\n",
    "            if hasattr(agent.model, 'sample_noise'):\n",
    "                agent.model.sample_noise()\n",
    "                agent.target_model.sample_noise()\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "        losses.append(episode_loss / steps if steps > 0 else 0)\n",
    "        td_errors.append(episode_td_error / steps if steps > 0 else 0)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = np.mean(rewards[-10:])\n",
    "            avg_loss = np.mean(losses[-10:]) if losses[-10:] else 0\n",
    "            buffer_size = len(agent.memory)\n",
    "            print(f\"Episode {episode}: Avg Reward = {avg_reward:.2f}, \"\n",
    "                  f\"Avg Loss = {avg_loss:.6f}, Buffer Size = {buffer_size}\")\n",
    "    \n",
    "    return rewards, losses, td_errors\n",
    "\n",
    "# 运行 Rainbow 演示训练\n",
    "print(\"开始 Rainbow 演示训练...（这可能需要几分钟）\")\n",
    "rewards, losses, td_errors = train_rainbow_demo(agent, env, n_episodes=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果可视化\n",
    "\n",
    "可视化 Rainbow DQN 训练过程中的奖励、损失和 Rainbow 特有指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制 Rainbow DQN 训练结果\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# 奖励曲线\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(rewards, alpha=0.7, label='回合奖励', color='blue')\n",
    "# 计算移动平均\n",
    "window_size = 10\n",
    "if len(rewards) >= window_size:\n",
    "    moving_avg = [np.mean(rewards[i:i+window_size]) for i in range(len(rewards)-window_size+1)]\n",
    "    ax1.plot(range(window_size-1, len(rewards)), moving_avg, 'r-', linewidth=2, label=f'{window_size}回合移动平均')\n",
    "\n",
    "ax1.set_xlabel('回合')\n",
    "ax1.set_ylabel('奖励')\n",
    "ax1.set_title('Rainbow DQN 训练奖励曲线')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 损失曲线\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(losses, 'g-', alpha=0.7, label='训练损失')\n",
    "if len(losses) >= window_size:\n",
    "    loss_moving_avg = [np.mean(losses[i:i+window_size]) for i in range(len(losses)-window_size+1)]\n",
    "    ax2.plot(range(window_size-1, len(losses)), loss_moving_avg, 'orange', linewidth=2, label=f'{window_size}回合平均损失')\n",
    "\n",
    "ax2.set_xlabel('回合')\n",
    "ax2.set_ylabel('平均损失')\n",
    "ax2.set_title('Rainbow DQN 训练损失曲线')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Rainbow 特有指标：经验回放缓冲区大小变化\n",
    "ax3 = axes[1, 0]\n",
    "buffer_sizes = [min(i * 100, len(agent.memory)) for i in range(len(rewards))]  # 估算缓冲区增长\n",
    "ax3.plot(buffer_sizes, 'purple', alpha=0.8, label='缓冲区大小')\n",
    "ax3.set_xlabel('回合')\n",
    "ax3.set_ylabel('经验数量')\n",
    "ax3.set_title('优先经验回放缓冲区增长')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 探索策略对比：Rainbow 使用噪声网络，传统 DQN 使用 epsilon-greedy\n",
    "ax4 = axes[1, 1]\n",
    "if agent.use_noisy:\n",
    "    # 噪声网络：固定探索\n",
    "    exploration = [1.0] * len(rewards)\n",
    "    ax4.plot(exploration, 'red', linewidth=2, label='噪声网络（固定探索）')\n",
    "    ax4.set_title('Rainbow 探索策略：噪声网络')\n",
    "else:\n",
    "    # epsilon-greedy 衰减\n",
    "    epsilons = [agent.epsilon_final + (agent.epsilon_start - agent.epsilon_final) * \n",
    "               np.exp(-1. * i * 100 / agent.epsilon_decay) for i in range(len(rewards))]\n",
    "    ax4.plot(epsilons, 'red', linewidth=2, label='Epsilon-greedy')\n",
    "    ax4.set_title('传统 DQN 探索策略：Epsilon-greedy')\n",
    "\n",
    "ax4.set_xlabel('回合')\n",
    "ax4.set_ylabel('探索率')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 打印 Rainbow DQN 统计信息\n",
    "print(\"=\" * 50)\n",
    "print(\"Rainbow DQN 训练统计:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"平均奖励: {np.mean(rewards):.2f}\")\n",
    "print(f\"最高奖励: {np.max(rewards):.2f}\")\n",
    "print(f\"最低奖励: {np.min(rewards):.2f}\")\n",
    "print(f\"奖励标准差: {np.std(rewards):.2f}\")\n",
    "print(f\"最终经验缓冲区大小: {len(agent.memory)}\")\n",
    "print(f\"使用噪声网络: {'是' if agent.use_noisy else '否'}\")\n",
    "print(f\"使用分布式Q学习: {'是' if agent.use_distributional else '否'}\")\n",
    "print(f\"使用优先回放: {'是' if agent.prioritized_replay else '否'}\")\n",
    "\n",
    "# Rainbow 与传统 DQN 的优势分析\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Rainbow DQN 优势分析:\")\n",
    "print(\"=\" * 50)\n",
    "if agent.use_noisy:\n",
    "    print(\"✅ 噪声网络: 替代 epsilon-greedy，提供更稳定的探索\")\n",
    "if agent.use_distributional:\n",
    "    print(\"✅ 分布式Q学习: 学习完整的价值分布，而非单一期望值\")\n",
    "if agent.prioritized_replay:\n",
    "    print(\"✅ 优先经验回放: 重点学习重要经验，提高样本效率\")\n",
    "print(f\"✅ N步学习: 使用 {agent.n_step} 步回报，减少偏差\")\n",
    "print(\"✅ Double DQN: 减少过估计问题\")\n",
    "print(\"✅ Dueling 架构: 分离状态价值和动作优势\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估\n",
    "\n",
    "评估训练后的 Rainbow DQN 智能体性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估 Rainbow 智能体\n",
    "def evaluate_rainbow_agent(agent, env, n_episodes=5):\n",
    "    eval_rewards = []\n",
    "    eval_steps = []\n",
    "    \n",
    "    print(\"开始评估 Rainbow DQN 智能体...\")\n",
    "    print(f\"评估配置: 噪声网络={'开启' if agent.use_noisy else '关闭'}, \"\n",
    "          f\"分布式Q学习={'开启' if agent.use_distributional else '关闭'}\")\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        # 评估时禁用噪声（如果使用噪声网络）\n",
    "        if hasattr(agent.model, 'eval'):\n",
    "            agent.model.eval()\n",
    "            agent.target_model.eval()\n",
    "        \n",
    "        while not (done or truncated) and steps < 2000:\n",
    "            action = agent.select_action(state, evaluate=True)  # 评估模式\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        # 恢复训练模式\n",
    "        if hasattr(agent.model, 'train'):\n",
    "            agent.model.train()\n",
    "            agent.target_model.train()\n",
    "        \n",
    "        eval_rewards.append(episode_reward)\n",
    "        eval_steps.append(steps)\n",
    "        print(f\"评估回合 {episode+1}: 奖励 = {episode_reward:.2f}, 步数 = {steps}\")\n",
    "    \n",
    "    return eval_rewards, eval_steps\n",
    "\n",
    "# 运行 Rainbow 评估\n",
    "eval_rewards, eval_steps = evaluate_rainbow_agent(agent, env, n_episodes=5)\n",
    "\n",
    "print(f\"\\nRainbow DQN 评估结果:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"平均奖励: {np.mean(eval_rewards):.2f} ± {np.std(eval_rewards):.2f}\")\n",
    "print(f\"最高奖励: {np.max(eval_rewards):.2f}\")\n",
    "print(f\"最低奖励: {np.min(eval_rewards):.2f}\")\n",
    "print(f\"平均步数: {np.mean(eval_steps):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建评估结果可视化\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# 评估奖励分布\n",
    "ax1.bar(range(1, len(eval_rewards)+1), eval_rewards, color='skyblue', alpha=0.8)\n",
    "ax1.axhline(y=np.mean(eval_rewards), color='red', linestyle='--', label=f'平均值: {np.mean(eval_rewards):.1f}')\n",
    "ax1.set_xlabel('评估回合')\n",
    "ax1.set_ylabel('奖励')\n",
    "ax1.set_title('Rainbow DQN 评估奖励分布')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 评估步数分布\n",
    "ax2.bar(range(1, len(eval_steps)+1), eval_steps, color='lightgreen', alpha=0.8)\n",
    "ax2.axhline(y=np.mean(eval_steps), color='red', linestyle='--', label=f'平均值: {np.mean(eval_steps):.1f}')\n",
    "ax2.set_xlabel('评估回合')\n",
    "ax2.set_ylabel('步数')\n",
    "ax2.set_title('Rainbow DQN 评估存活步数')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本演示展示了如何使用 **Rainbow DQN** 算法训练智能体玩 Atari 游戏。Rainbow DQN 是目前最先进的基于价值的强化学习算法之一，集成了多种 DQN 改进技术。\n",
    "\n",
    "### 主要步骤包括:\n",
    "\n",
    "1. **环境预处理**: 将原始游戏画面转换为适合神经网络处理的格式\n",
    "2. **Rainbow 模型设计**: 集成多种先进技术的神经网络架构\n",
    "3. **多组件训练**: 协调多个改进组件的训练过程\n",
    "4. **综合评估**: 评估 Rainbow 算法的整体性能\n",
    "\n",
    "### Rainbow DQN 的六大核心组件:\n",
    "\n",
    "1. **🎯 Double DQN**: 使用主网络选择动作，目标网络评估价值，减少过估计偏差\n",
    "2. **⚔️ Dueling DQN**: 将Q值分解为状态价值函数V(s)和优势函数A(s,a)，提高学习效率\n",
    "3. **🔄 优先经验回放**: 根据TD误差优先采样重要经验，提高样本效率\n",
    "4. **📈 多步学习**: 使用n步回报减少偏差，加速收敛\n",
    "5. **🔊 噪声网络**: 用参数噪声替代ε-贪婪探索，提供更稳定的探索策略\n",
    "6. **📊 分布式Q学习**: 学习价值分布而非期望值，提供更丰富的价值信息\n",
    "\n",
    "### Rainbow DQN 相比传统方法的优势:\n",
    "\n",
    "#### 🚀 **性能优势**:\n",
    "- **样本效率更高**: 优先回放和多步学习减少所需训练样本\n",
    "- **探索更稳定**: 噪声网络提供一致的探索，避免探索率调参困难\n",
    "- **收敛更快**: 多个组件协同作用，加速训练收敛\n",
    "- **性能更强**: 在多数 Atari 游戏上达到超人类表现\n",
    "\n",
    "#### 🔧 **技术优势**:\n",
    "- **偏差更小**: Double DQN 和多步学习减少估计偏差\n",
    "- **信息更丰富**: 分布式Q学习捕获价值的不确定性\n",
    "- **训练更稳定**: 目标网络和噪声网络提供稳定的训练过程\n",
    "- **通用性更强**: 六个组件可根据需要灵活组合\n",
    "\n",
    "#### 📊 **实际应用优势**:\n",
    "- **调参更简单**: 噪声网络消除 epsilon 调参需求\n",
    "- **可解释性更强**: Dueling 架构和分布式学习提供更多洞察\n",
    "- **扩展性更好**: 组件化设计便于集成新技术\n",
    "\n",
    "### 性能对比总结:\n",
    "\n",
    "| 算法 | 样本效率 | 最终性能 | 训练稳定性 | 探索效果 | 实现复杂度 |\n",
    "|------|----------|----------|------------|----------|------------|\n",
    "| DQN | ⭐⭐ | ⭐⭐ | ⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ |\n",
    "| Dueling DQN | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐ |\n",
    "| **Rainbow DQN** | **⭐⭐⭐⭐⭐** | **⭐⭐⭐⭐⭐** | **⭐⭐⭐⭐** | **⭐⭐⭐⭐⭐** | **⭐⭐⭐** |\n",
    "\n",
    "### 进一步改进建议:\n",
    "\n",
    "🔧 **训练优化**:\n",
    "- 增加训练回合数 (1000+ episodes)\n",
    "- 使用更大的经验回放缓冲区 (100k+ experiences)\n",
    "- 调整学习率调度策略\n",
    "- 使用梯度裁剪防止训练不稳定\n",
    "\n",
    "⚙️ **模型优化**:\n",
    "- 尝试不同的网络架构 (ResNet, Attention)\n",
    "- 调整分布式Q学习的原子数量\n",
    "- 优化噪声网络参数\n",
    "- 实验不同的n步长度\n",
    "\n",
    "🎮 **环境优化**:\n",
    "- 使用帧跳跃 (frame skipping) 加速训练\n",
    "- 实验不同的奖励设计\n",
    "- 尝试其他 Atari 游戏\n",
    "- 使用环境集成技术\n",
    "\n",
    "---\n",
    "\n",
    "**Rainbow DQN 代表了深度强化学习在游戏AI领域的一个重要里程碑，它证明了组合多种改进技术可以显著提升算法性能。这种"集百家之长"的设计思路为后续强化学习算法发展奠定了基础。**\n",
    "\n",
    "### 使用 Rainbow DQN 进行实际训练:\n",
    "\n",
    "要获得更好的性能，可以使用以下命令进行完整训练：\n",
    "\n",
    "```bash\n",
    "# 在项目根目录下运行\n",
    "python run_training.py rainbow 1000 --use_noisy --use_distributional --prioritized_replay\n",
    "```\n",
    "\n",
    "这将启动一个完整的 Rainbow DQN 训练会话，使用所有 Rainbow 组件。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snake_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
