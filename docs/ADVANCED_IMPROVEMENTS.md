# é«˜çº§ä»£ç è´¨é‡æ”¹è¿›å®æ–½æŒ‡å—

## ğŸ¯ ç«‹å³å¯å®æ–½çš„æ”¹è¿›

### 1. é…ç½®ç®¡ç†ç³»ç»Ÿ

#### åˆ›å»ºé…ç½®ç±»
```python
# src/config.py
from dataclasses import dataclass
from typing import Optional
import json
import os

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»"""
    # åŸºç¡€å‚æ•°
    episodes: int = 1000
    batch_size: int = 32
    learning_rate: float = 1e-4
    gamma: float = 0.99
    
    # ç½‘ç»œå‚æ•°
    buffer_size: int = 100000
    target_update: int = 1000
    
    # æ¢ç´¢å‚æ•°
    epsilon_start: float = 1.0
    epsilon_final: float = 0.01
    epsilon_decay: int = 10000
    
    # Rainbowç‰¹å®šå‚æ•°
    n_step: int = 3
    use_noisy: bool = True
    use_distributional: bool = False
    n_atoms: int = 51
    v_min: float = -10
    v_max: float = 10
    
    # è®­ç»ƒæ§åˆ¶
    eval_interval: int = 100
    save_interval: int = 500
    log_interval: int = 10
    
    # è·¯å¾„é…ç½®
    save_dir: str = "models"
    log_dir: str = "logs"
    
    @classmethod
    def from_file(cls, config_path: str) -> 'TrainingConfig':
        """ä»JSONæ–‡ä»¶åŠ è½½é…ç½®"""
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config_dict = json.load(f)
            return cls(**config_dict)
        return cls()
    
    def save_to_file(self, config_path: str):
        """ä¿å­˜é…ç½®åˆ°JSONæ–‡ä»¶"""
        os.makedirs(os.path.dirname(config_path), exist_ok=True)
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(self.__dict__, f, indent=2, ensure_ascii=False)
    
    def update_from_args(self, args):
        """ä»å‘½ä»¤è¡Œå‚æ•°æ›´æ–°é…ç½®"""
        for key, value in vars(args).items():
            if hasattr(self, key) and value is not None:
                setattr(self, key, value)
```

#### ä¿®æ”¹è®­ç»ƒè„šæœ¬ä½¿ç”¨é…ç½®
```python
# åœ¨ train.py ä¸­çš„ä¿®æ”¹
def main():
    args = parse_args()
    
    # åŠ è½½é…ç½®
    config = TrainingConfig.from_file('configs/default.json')
    config.update_from_args(args)
    
    # ä¿å­˜å½“å‰é…ç½®
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    config.save_to_file(f'configs/training_{timestamp}.json')
    
    # ä½¿ç”¨é…ç½®è¿›è¡Œè®­ç»ƒ
    train(config)
```

### 2. å¢å¼ºçš„å¼‚å¸¸å¤„ç†å’Œæ¢å¤æœºåˆ¶

#### åˆ›å»ºè®­ç»ƒçŠ¶æ€ç®¡ç†å™¨
```python
# src/training_state.py
import pickle
import os
from datetime import datetime
from typing import Dict, Any, Optional

class TrainingStateManager:
    """è®­ç»ƒçŠ¶æ€ç®¡ç†å™¨"""
    
    def __init__(self, save_dir: str = "checkpoints"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
    
    def save_state(self, episode: int, agent, optimizer, metrics: Dict[str, Any], 
                   config, additional_data: Optional[Dict] = None):
        """ä¿å­˜è®­ç»ƒçŠ¶æ€"""
        state = {
            'episode': episode,
            'timestamp': datetime.now().isoformat(),
            'model_state_dict': agent.model.state_dict(),
            'target_model_state_dict': agent.target_model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'agent_steps': agent.steps_done,
            'metrics': metrics,
            'config': config.__dict__,
            'additional_data': additional_data or {}
        }
        
        checkpoint_path = os.path.join(self.save_dir, f"checkpoint_episode_{episode}.pkl")
        with open(checkpoint_path, 'wb') as f:
            pickle.dump(state, f)
        
        # ä¿æŒæœ€æ–°çš„æ£€æŸ¥ç‚¹é“¾æ¥
        latest_path = os.path.join(self.save_dir, "latest_checkpoint.pkl")
        if os.path.exists(latest_path):
            os.remove(latest_path)
        os.link(checkpoint_path, latest_path)
        
        return checkpoint_path
    
    def load_latest_state(self):
        """åŠ è½½æœ€æ–°çš„è®­ç»ƒçŠ¶æ€"""
        latest_path = os.path.join(self.save_dir, "latest_checkpoint.pkl")
        if os.path.exists(latest_path):
            with open(latest_path, 'rb') as f:
                return pickle.load(f)
        return None
    
    def cleanup_old_checkpoints(self, keep_last_n: int = 5):
        """æ¸…ç†æ—§çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        checkpoints = [f for f in os.listdir(self.save_dir) 
                      if f.startswith('checkpoint_episode_') and f.endswith('.pkl')]
        checkpoints.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))
        
        if len(checkpoints) > keep_last_n:
            for checkpoint in checkpoints[:-keep_last_n]:
                os.remove(os.path.join(self.save_dir, checkpoint))
```

#### å¢å¼ºçš„è®­ç»ƒå¾ªç¯
```python
# åœ¨ train.py ä¸­æ·»åŠ æ¢å¤åŠŸèƒ½
def train_with_recovery(config: TrainingConfig, resume: bool = False):
    """æ”¯æŒæ¢å¤çš„è®­ç»ƒå‡½æ•°"""
    state_manager = TrainingStateManager()
    
    # å°è¯•æ¢å¤è®­ç»ƒçŠ¶æ€
    start_episode = 0
    if resume:
        saved_state = state_manager.load_latest_state()
        if saved_state:
            print(f"æ¢å¤è®­ç»ƒä»ç¬¬ {saved_state['episode']} å›åˆå¼€å§‹")
            start_episode = saved_state['episode']
            # æ¢å¤æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€
            agent.model.load_state_dict(saved_state['model_state_dict'])
            agent.target_model.load_state_dict(saved_state['target_model_state_dict'])
            optimizer.load_state_dict(saved_state['optimizer_state_dict'])
            agent.steps_done = saved_state['agent_steps']
    
    try:
        # ä¸»è®­ç»ƒå¾ªç¯
        for episode in range(start_episode, config.episodes):
            # ... è®­ç»ƒé€»è¾‘ ...
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if episode % 100 == 0:
                state_manager.save_state(
                    episode, agent, optimizer, 
                    {'avg_reward': avg_reward, 'best_reward': best_reward},
                    config
                )
                state_manager.cleanup_old_checkpoints()
    
    except KeyboardInterrupt:
        print("\nè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ï¼Œä¿å­˜å½“å‰çŠ¶æ€...")
        state_manager.save_state(
            episode, agent, optimizer,
            {'avg_reward': avg_reward, 'best_reward': best_reward},
            config
        )
        print("çŠ¶æ€å·²ä¿å­˜ï¼Œå¯ä»¥ä½¿ç”¨ --resume å‚æ•°æ¢å¤è®­ç»ƒ")
    
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        # ä¿å­˜é”™è¯¯çŠ¶æ€
        state_manager.save_state(
            episode, agent, optimizer,
            {'error': str(e), 'avg_reward': avg_reward},
            config,
            {'error_traceback': traceback.format_exc()}
        )
        raise
```

### 3. æ€§èƒ½ç›‘æ§å’Œåˆ†æç³»ç»Ÿ

#### åˆ›å»ºæŒ‡æ ‡æ”¶é›†å™¨
```python
# src/metrics.py
import time
import psutil
import torch
from collections import defaultdict, deque
from typing import Dict, List, Any
import numpy as np

class MetricsCollector:
    """è®­ç»ƒæŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.metrics = defaultdict(lambda: deque(maxlen=window_size))
        self.episode_metrics = defaultdict(list)
        self.start_time = time.time()
    
    def log_scalar(self, name: str, value: float, episode: int):
        """è®°å½•æ ‡é‡æŒ‡æ ‡"""
        self.metrics[name].append(value)
        self.episode_metrics[name].append((episode, value))
    
    def log_system_metrics(self):
        """è®°å½•ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        # CPUå’Œå†…å­˜ä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent()
        memory_info = psutil.virtual_memory()
        
        self.metrics['system/cpu_percent'].append(cpu_percent)
        self.metrics['system/memory_percent'].append(memory_info.percent)
        self.metrics['system/memory_used_gb'].append(memory_info.used / 1024**3)
        
        # GPUæŒ‡æ ‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_allocated() / 1024**3
            gpu_cached = torch.cuda.memory_reserved() / 1024**3
            self.metrics['system/gpu_memory_gb'].append(gpu_memory)
            self.metrics['system/gpu_cached_gb'].append(gpu_cached)
    
    def log_training_speed(self, episodes_completed: int):
        """è®°å½•è®­ç»ƒé€Ÿåº¦"""
        elapsed_time = time.time() - self.start_time
        episodes_per_hour = episodes_completed / (elapsed_time / 3600)
        self.metrics['training/episodes_per_hour'].append(episodes_per_hour)
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡æ‘˜è¦"""
        summary = {}
        for name, values in self.metrics.items():
            if values:
                summary[name] = {
                    'current': values[-1],
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values)
                }
        return summary
    
    def detect_anomalies(self) -> List[str]:
        """æ£€æµ‹è®­ç»ƒå¼‚å¸¸"""
        anomalies = []
        
        # æ£€æµ‹å¥–åŠ±åœæ»
        if 'Train/Reward' in self.metrics and len(self.metrics['Train/Reward']) >= 50:
            recent_rewards = list(self.metrics['Train/Reward'])[-50:]
            if np.std(recent_rewards) < 0.1:  # å¥–åŠ±å˜åŒ–å¾ˆå°
                anomalies.append("å¥–åŠ±å¯èƒ½å·²åœæ»ï¼Œå»ºè®®æ£€æŸ¥å­¦ä¹ ç‡æˆ–æ¢ç´¢ç­–ç•¥")
        
        # æ£€æµ‹æŸå¤±å¼‚å¸¸
        if 'Train/Loss' in self.metrics and len(self.metrics['Train/Loss']) >= 10:
            recent_losses = list(self.metrics['Train/Loss'])[-10:]
            if any(loss > 100 for loss in recent_losses):  # æŸå¤±è¿‡å¤§
                anomalies.append("æ£€æµ‹åˆ°å¼‚å¸¸é«˜çš„æŸå¤±å€¼ï¼Œå¯èƒ½å­˜åœ¨æ¢¯åº¦çˆ†ç‚¸")
        
        # æ£€æµ‹å†…å­˜æ³„æ¼
        if 'system/memory_percent' in self.metrics and len(self.metrics['system/memory_percent']) >= 20:
            memory_usage = list(self.metrics['system/memory_percent'])[-20:]
            if memory_usage[-1] - memory_usage[0] > 20:  # å†…å­˜ä½¿ç”¨å¢é•¿è¶…è¿‡20%
                anomalies.append("æ£€æµ‹åˆ°å¯èƒ½çš„å†…å­˜æ³„æ¼")
        
        return anomalies
```

### 4. è‡ªåŠ¨åŒ–æµ‹è¯•å¥—ä»¶

#### åˆ›å»ºè®­ç»ƒæµç¨‹æµ‹è¯•
```python
# tests/test_training_integration.py
import pytest
import tempfile
import shutil
import os
from unittest.mock import patch, MagicMock

from src.config import TrainingConfig
from src.train import train_with_recovery
from src.agent import DQNAgent, RainbowAgent
from src.model import DQN, RainbowDQN

class TestTrainingIntegration:
    """è®­ç»ƒæµç¨‹é›†æˆæµ‹è¯•"""
    
    def setup_method(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.temp_dir = tempfile.mkdtemp()
        self.config = TrainingConfig(
            episodes=5,
            batch_size=16,
            eval_interval=2,
            save_interval=2,
            save_dir=os.path.join(self.temp_dir, 'models'),
            log_dir=os.path.join(self.temp_dir, 'logs')
        )
    
    def teardown_method(self):
        """æµ‹è¯•åæ¸…ç†"""
        shutil.rmtree(self.temp_dir)
    
    def test_dqn_training_pipeline(self):
        """æµ‹è¯•DQNè®­ç»ƒæµç¨‹"""
        with patch('src.utils.make_env') as mock_env:
            mock_env.return_value = self._create_mock_env()
            
            # è¿è¡ŒçŸ­æœŸè®­ç»ƒ
            train_with_recovery(self.config)
            
            # éªŒè¯æ¨¡å‹æ–‡ä»¶æ˜¯å¦ç”Ÿæˆ
            assert os.path.exists(self.config.save_dir)
            model_files = os.listdir(self.config.save_dir)
            assert len(model_files) > 0
    
    def test_rainbow_training_pipeline(self):
        """æµ‹è¯•Rainbowè®­ç»ƒæµç¨‹"""
        self.config.model = 'rainbow'
        with patch('src.utils.make_env') as mock_env:
            mock_env.return_value = self._create_mock_env()
            
            train_with_recovery(self.config)
            
            # éªŒè¯è®­ç»ƒå®Œæˆ
            assert os.path.exists(self.config.save_dir)
    
    def test_training_interruption_and_recovery(self):
        """æµ‹è¯•è®­ç»ƒä¸­æ–­å’Œæ¢å¤"""
        with patch('src.utils.make_env') as mock_env:
            mock_env.return_value = self._create_mock_env()
            
            # æ¨¡æ‹Ÿè®­ç»ƒä¸­æ–­
            with patch('builtins.input', side_effect=KeyboardInterrupt()):
                try:
                    train_with_recovery(self.config)
                except KeyboardInterrupt:
                    pass
            
            # éªŒè¯æ£€æŸ¥ç‚¹æ–‡ä»¶å­˜åœ¨
            checkpoint_dir = os.path.join(self.temp_dir, 'checkpoints')
            assert os.path.exists(checkpoint_dir)
            
            # æµ‹è¯•æ¢å¤è®­ç»ƒ
            train_with_recovery(self.config, resume=True)
    
    def _create_mock_env(self):
        """åˆ›å»ºæ¨¡æ‹Ÿç¯å¢ƒ"""
        mock_env = MagicMock()
        mock_env.action_space.n = 4
        mock_env.reset.return_value = [0] * 84 * 84 * 4  # æ¨¡æ‹ŸçŠ¶æ€
        mock_env.step.return_value = ([0] * 84 * 84 * 4, 1.0, False, {})
        return mock_env
```

### 5. å®æ—¶ç›‘æ§é¢æ¿

#### åˆ›å»ºStreamlitç›‘æ§åº”ç”¨
```python
# monitoring/dashboard.py
import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import json
import os
from datetime import datetime, timedelta

def load_training_logs(log_dir: str):
    """åŠ è½½è®­ç»ƒæ—¥å¿—"""
    # ä»TensorBoardæ—¥å¿—æˆ–JSONæ–‡ä»¶åŠ è½½æ•°æ®
    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æ—¥å¿—æ ¼å¼å®ç°
    pass

def create_reward_plot(df):
    """åˆ›å»ºå¥–åŠ±æ›²çº¿å›¾"""
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=('è®­ç»ƒå¥–åŠ±', 'è¯„ä¼°å¥–åŠ±'),
        vertical_spacing=0.1
    )
    
    # è®­ç»ƒå¥–åŠ±
    fig.add_trace(
        go.Scatter(x=df['episode'], y=df['train_reward'], 
                  name='è®­ç»ƒå¥–åŠ±', line=dict(color='blue')),
        row=1, col=1
    )
    
    # è¯„ä¼°å¥–åŠ±
    if 'eval_reward' in df.columns:
        fig.add_trace(
            go.Scatter(x=df['episode'], y=df['eval_reward'], 
                      name='è¯„ä¼°å¥–åŠ±', line=dict(color='red')),
            row=2, col=1
        )
    
    fig.update_layout(height=600, title_text="å¥–åŠ±è¶‹åŠ¿")
    return fig

def create_system_metrics_plot(df):
    """åˆ›å»ºç³»ç»ŸæŒ‡æ ‡å›¾"""
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('CPUä½¿ç”¨ç‡', 'å†…å­˜ä½¿ç”¨ç‡', 'GPUå†…å­˜', 'è®­ç»ƒé€Ÿåº¦'),
        vertical_spacing=0.1
    )
    
    # CPUä½¿ç”¨ç‡
    if 'cpu_percent' in df.columns:
        fig.add_trace(
            go.Scatter(x=df['episode'], y=df['cpu_percent'], name='CPU'),
            row=1, col=1
        )
    
    # å†…å­˜ä½¿ç”¨ç‡
    if 'memory_percent' in df.columns:
        fig.add_trace(
            go.Scatter(x=df['episode'], y=df['memory_percent'], name='å†…å­˜'),
            row=1, col=2
        )
    
    # GPUå†…å­˜
    if 'gpu_memory_gb' in df.columns:
        fig.add_trace(
            go.Scatter(x=df['episode'], y=df['gpu_memory_gb'], name='GPUå†…å­˜'),
            row=2, col=1
        )
    
    # è®­ç»ƒé€Ÿåº¦
    if 'episodes_per_hour' in df.columns:
        fig.add_trace(
            go.Scatter(x=df['episode'], y=df['episodes_per_hour'], name='å›åˆ/å°æ—¶'),
            row=2, col=2
        )
    
    fig.update_layout(height=600, title_text="ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡")
    return fig

def main():
    st.set_page_config(page_title="Snake RL è®­ç»ƒç›‘æ§", layout="wide")
    
    st.title("ğŸ Snake RL è®­ç»ƒç›‘æ§é¢æ¿")
    
    # ä¾§è¾¹æ é…ç½®
    st.sidebar.header("é…ç½®")
    log_dir = st.sidebar.text_input("æ—¥å¿—ç›®å½•", value="logs")
    auto_refresh = st.sidebar.checkbox("è‡ªåŠ¨åˆ·æ–°", value=True)
    refresh_interval = st.sidebar.slider("åˆ·æ–°é—´éš”(ç§’)", 5, 60, 10)
    
    if auto_refresh:
        st.sidebar.write(f"ä¸‹æ¬¡åˆ·æ–°: {refresh_interval}ç§’å")
        # è‡ªåŠ¨åˆ·æ–°é€»è¾‘
    
    # ä¸»è¦å†…å®¹åŒºåŸŸ
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("å½“å‰å›åˆ", "1000", "10")
    with col2:
        st.metric("å¹³å‡å¥–åŠ±", "15.6", "2.3")
    with col3:
        st.metric("æœ€ä½³å¥–åŠ±", "28.4", "1.2")
    
    # å›¾è¡¨åŒºåŸŸ
    tab1, tab2, tab3 = st.tabs(["è®­ç»ƒæŒ‡æ ‡", "ç³»ç»Ÿæ€§èƒ½", "å¼‚å¸¸æ£€æµ‹"])
    
    with tab1:
        # åŠ è½½å¹¶æ˜¾ç¤ºè®­ç»ƒæ•°æ®
        df = load_training_logs(log_dir)
        if df is not None:
            st.plotly_chart(create_reward_plot(df), use_container_width=True)
        else:
            st.warning("æœªæ‰¾åˆ°è®­ç»ƒæ—¥å¿—æ–‡ä»¶")
    
    with tab2:
        if df is not None:
            st.plotly_chart(create_system_metrics_plot(df), use_container_width=True)
    
    with tab3:
        st.subheader("å¼‚å¸¸æ£€æµ‹")
        # æ˜¾ç¤ºæ£€æµ‹åˆ°çš„å¼‚å¸¸
        anomalies = ["å¥–åŠ±åœæ»æ£€æµ‹", "å†…å­˜ä½¿ç”¨å¼‚å¸¸"]
        for anomaly in anomalies:
            st.warning(f"âš ï¸ {anomaly}")

if __name__ == "__main__":
    main()
```

## ğŸš€ å®æ–½æ­¥éª¤

### ç¬¬ä¸€é˜¶æ®µï¼ˆç«‹å³å®æ–½ï¼‰
1. **é…ç½®ç®¡ç†ç³»ç»Ÿ** - 1-2å¤©
   - åˆ›å»º `src/config.py`
   - ä¿®æ”¹ `train.py` ä½¿ç”¨é…ç½®ç±»
   - åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶

2. **å¼‚å¸¸å¤„ç†å¢å¼º** - 1å¤©
   - åˆ›å»º `src/training_state.py`
   - åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ å¼‚å¸¸å¤„ç†
   - å®ç°è®­ç»ƒæ¢å¤åŠŸèƒ½

### ç¬¬äºŒé˜¶æ®µï¼ˆ1å‘¨å†…ï¼‰
3. **æ€§èƒ½ç›‘æ§ç³»ç»Ÿ** - 2-3å¤©
   - åˆ›å»º `src/metrics.py`
   - é›†æˆåˆ°è®­ç»ƒå¾ªç¯
   - æ·»åŠ å¼‚å¸¸æ£€æµ‹é€»è¾‘

4. **æµ‹è¯•å¥—ä»¶** - 2å¤©
   - åˆ›å»ºé›†æˆæµ‹è¯•
   - æ·»åŠ CI/CDé…ç½®
   - è®¾ç½®è‡ªåŠ¨åŒ–æµ‹è¯•

### ç¬¬ä¸‰é˜¶æ®µï¼ˆ2å‘¨å†…ï¼‰
5. **ç›‘æ§é¢æ¿** - 3-5å¤©
   - åˆ›å»ºStreamlitåº”ç”¨
   - å®ç°å®æ—¶æ•°æ®åŠ è½½
   - æ·»åŠ äº¤äº’å¼å›¾è¡¨

## ğŸ“Š é¢„æœŸæ”¶ç›Š

- **å¯ç»´æŠ¤æ€§æå‡**: é…ç½®ç®¡ç†ä½¿å‚æ•°è°ƒæ•´æ›´ç®€å•
- **ç¨³å®šæ€§å¢å¼º**: å¼‚å¸¸å¤„ç†å’Œæ¢å¤æœºåˆ¶å‡å°‘è®­ç»ƒä¸­æ–­æŸå¤±
- **å¯è§‚æµ‹æ€§æ”¹å–„**: è¯¦ç»†çš„ç›‘æ§å’Œåˆ†æå¸®åŠ©ä¼˜åŒ–è®­ç»ƒ
- **å¼€å‘æ•ˆç‡**: è‡ªåŠ¨åŒ–æµ‹è¯•ç¡®ä¿ä»£ç è´¨é‡
- **ç”¨æˆ·ä½“éªŒ**: å¯è§†åŒ–é¢æ¿æä¾›ç›´è§‚çš„è®­ç»ƒçŠ¶æ€

è¿™äº›æ”¹è¿›å°†ä½¿æ‚¨çš„å¼ºåŒ–å­¦ä¹ é¡¹ç›®æ›´åŠ ä¸“ä¸šå’Œå¯é ï¼Œé€‚åˆé•¿æœŸç»´æŠ¤å’Œæ‰©å±•ã€‚