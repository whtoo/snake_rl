import os

# è®¾ç½®ç¯å¢ƒå˜é‡è§£å†³OpenMPå†²çªé—®é¢˜
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
# è®¾ç½®Qtç›¸å…³ç¯å¢ƒå˜é‡
os.environ["QT_QPA_PLATFORM_PLUGIN_PATH"] = ""

import torch
import numpy as np
import time
import argparse
from torch.utils.tensorboard import SummaryWriter
import signal  # æ–°å¢å¯¼å…¥
from .model import DQN, DuelingDQN, RainbowDQN
from .agent import DQNAgent, RainbowAgent
from .utils import make_env, plot_rewards
from .input_shield import input_shield_context, setup_signal_handlers


def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    """
    parser = argparse.ArgumentParser(description="DQNè®­ç»ƒè„šæœ¬")
    parser.add_argument("--env", type=str, default="ALE/Assault-v5", help="Gymç¯å¢ƒåç§°")
    parser.add_argument(
        "--model",
        type=str,
        default="rainbow",
        choices=["dqn", "dueling", "rainbow"],
        help="æ¨¡å‹ç±»å‹",
    )
    parser.add_argument("--episodes", type=int, default=100, help="è®­ç»ƒå›åˆæ•°")
    parser.add_argument(
        "--buffer_size", type=int, default=100000, help="ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°"
    )
    parser.add_argument("--batch_size", type=int, default=32, help="è®­ç»ƒæ‰¹é‡å¤§å°")
    parser.add_argument("--gamma", type=float, default=0.99, help="æŠ˜æ‰£å› å­")
    parser.add_argument("--lr", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--epsilon_start", type=float, default=1.0, help="åˆå§‹æ¢ç´¢ç‡")
    parser.add_argument("--epsilon_final", type=float, default=0.01, help="æœ€ç»ˆæ¢ç´¢ç‡")
    parser.add_argument(
        "--epsilon_decay", type=int, default=100000, help="æ¢ç´¢ç‡è¡°å‡å¸§æ•°"
    )
    parser.add_argument(
        "--target_update", type=int, default=1000, help="ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡"
    )
    parser.add_argument(
        "--prioritized_replay", action="store_true", help="æ˜¯å¦ä½¿ç”¨ä¼˜å…ˆç»éªŒå›æ”¾"
    )
    # Rainbow DQN ç‰¹æœ‰å‚æ•°
    # parser.add_argument("--n_step", type=int, default=3, help="Næ­¥å­¦ä¹ çš„æ­¥æ•°") # Replaced by base_n_step
    parser.add_argument("--use_noisy", action="store_true", help="æ˜¯å¦ä½¿ç”¨å™ªå£°ç½‘ç»œ")
    parser.add_argument(
        "--use_distributional", action="store_true", help="æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼Qå­¦ä¹ "
    )
    parser.add_argument("--n_atoms", type=int, default=51, help="åˆ†å¸ƒå¼Qå­¦ä¹ çš„åŸå­æ•°é‡")
    parser.add_argument("--v_min", type=float, default=-10, help="å€¼å‡½æ•°çš„æœ€å°å€¼")
    parser.add_argument("--v_max", type=float, default=10, help="å€¼å‡½æ•°çš„æœ€å¤§å€¼")

    # Adaptive N-step parameters
    parser.add_argument("--base_n_step", type=int, default=3, help="åŸºç¡€Næ­¥å­¦ä¹ çš„æ­¥æ•° (ç”¨äºAdaptiveNStepBuffer)")
    parser.add_argument("--max_n_step", type=int, default=10, help="æœ€å¤§Næ­¥å­¦ä¹ çš„æ­¥æ•° (ç”¨äºAdaptiveNStepBuffer)")
    parser.add_argument("--adapt_n_step_freq", type=int, default=1000, help="è‡ªé€‚åº”Næ­¥è°ƒæ•´é¢‘ç‡ (è®­ç»ƒæ­¥æ•°)")
    parser.add_argument("--td_error_threshold_low", type=float, default=0.1, help="ç”¨äºé™ä½Nçš„TDè¯¯å·®é˜ˆå€¼")
    parser.add_argument("--td_error_threshold_high", type=float, default=0.5, help="ç”¨äºå¢åŠ Nçš„TDè¯¯å·®é˜ˆå€¼")

    # Experience Augmentation parameters
    parser.add_argument("--use_state_augmentation", action="store_true", help="æ˜¯å¦å¯ç”¨çŠ¶æ€å¢å¼º")
    parser.add_argument("--aug_noise_scale", type=float, default=5.0, help="é«˜æ–¯å™ªå£°å¢å¼ºçš„æ ‡å‡†å·® (åŸºäº0-255çš„åƒç´ å€¼)") # Adjusted default based on uint8

    parser.add_argument(
        "--save_dir", type=str, default="checkpoints", help="æ¨¡å‹ä¿å­˜ç›®å½•"
    )
    parser.add_argument("--log_dir", type=str, default="logs", help="æ—¥å¿—ä¿å­˜ç›®å½•")
    parser.add_argument(
        "--save_interval", type=int, default=100, help="æ¨¡å‹ä¿å­˜é—´éš”ï¼ˆå›åˆï¼‰"
    )
    parser.add_argument(
        "--eval_interval", type=int, default=10, help="è¯„ä¼°é—´éš”ï¼ˆå›åˆï¼‰"
    )
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")

    return parser.parse_args()


def train(args):
    """
    è®­ç»ƒDQNæ™ºèƒ½ä½“

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    stop_training = False  # æ§åˆ¶è®­ç»ƒå¾ªç¯çš„æ ‡å¿—

    def stop_callback():
        nonlocal stop_training
        print("\nå‡†å¤‡ç»ˆæ­¢è®­ç»ƒ...")
        stop_training = True

    # è®¾ç½®ä¿¡å·å¤„ç†å™¨ï¼Œåªå…è®¸Ctrl+Cä¸­æ–­
    setup_signal_handlers(stop_callback)

    # è®¾ç½®éšæœºç§å­
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    # åˆ›å»ºç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)
    os.makedirs(args.log_dir, exist_ok=True)

    # åˆ›å»ºç¯å¢ƒ
    try:
        env = make_env(args.env)
        eval_env = make_env(args.env)
    except Exception as e:
        print(f"åˆ›å»ºç¯å¢ƒæ—¶å‡ºé”™: {e}")
        print("å°è¯•ä½¿ç”¨å¤‡ç”¨ç¯å¢ƒè®¾ç½®...")
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å¤‡ç”¨ç¯å¢ƒåˆ›å»ºé€»è¾‘
        raise

    # è®¾ç½®è®¾å¤‡
    device_str = "cpu"  # é»˜è®¤å€¼

    if torch.cuda.is_available():
        device_str = "cuda"
    elif torch.backends.mps.is_built():
        device_str = "mps"

    device = torch.device(device_str)
    print(f"Using device: {device}")
    print(f"Choose model: {args.model}")

    # åˆ›å»ºæ¨¡å‹
    input_shape = env.observation_space.shape
    n_actions = env.action_space.n

    if args.model == "dqn":
        model = DQN(input_shape, n_actions)
        target_model = DQN(input_shape, n_actions)
    elif args.model == "dueling":
        model = DuelingDQN(input_shape, n_actions)
        target_model = DuelingDQN(input_shape, n_actions)
    else:  # rainbow
        model = RainbowDQN(
            input_shape=input_shape,
            n_actions=n_actions,
            n_atoms=args.n_atoms,
            v_min=args.v_min,
            v_max=args.v_max,
            use_noisy=args.use_noisy,
            use_distributional=args.use_distributional,
        )
        target_model = RainbowDQN(
            input_shape=input_shape,
            n_actions=n_actions,
            n_atoms=args.n_atoms,
            v_min=args.v_min,
            v_max=args.v_max,
            use_noisy=args.use_noisy,
            use_distributional=args.use_distributional,
        )

    # åˆ›å»ºæ™ºèƒ½ä½“
    if args.model == "rainbow":
        augmentation_config = None
        if args.use_state_augmentation:
            augmentation_config = {'add_noise': {'scale': args.aug_noise_scale}}
            print(f"Augmentation enabled with config: {augmentation_config}")

        agent = RainbowAgent(
            model=model,
            target_model=target_model,
            env=env,
            device=device,
            buffer_size=args.buffer_size,
            batch_size=args.batch_size,
            gamma=args.gamma,
            lr=args.lr,
            epsilon_start=args.epsilon_start,
            epsilon_final=args.epsilon_final,
            epsilon_decay=args.epsilon_decay,
            target_update=args.target_update,
            prioritized_replay=args.prioritized_replay,
            # n_step=args.n_step, # Replaced by base_n_step for RainbowAgent with AdaptiveNStepBuffer
            base_n_step=args.base_n_step,
            max_n_step=args.max_n_step,
            adapt_n_step_freq=args.adapt_n_step_freq,
            td_error_threshold_low=args.td_error_threshold_low,
            td_error_threshold_high=args.td_error_threshold_high,
            use_noisy=args.use_noisy,
            use_distributional=args.use_distributional,
            n_atoms=args.n_atoms,
            v_min=args.v_min,
            v_max=args.v_max,
            augmentation_config=augmentation_config, # Pass augmentation config
        )
    else:
        agent = DQNAgent(
            model=model,
            target_model=target_model,
            env=env,
            device=device,
            buffer_size=args.buffer_size,
            batch_size=args.batch_size,
            gamma=args.gamma,
            lr=args.lr,
            epsilon_start=args.epsilon_start,
            epsilon_final=args.epsilon_final,
            epsilon_decay=args.epsilon_decay,
            target_update=args.target_update,
            prioritized_replay=args.prioritized_replay,
        )

    # åˆ›å»ºTensorBoardæ—¥å¿—è®°å½•å™¨
    writer = SummaryWriter(log_dir=args.log_dir)

    # è®­ç»ƒç»Ÿè®¡
    rewards = []
    avg_rewards = []
    best_avg_reward = -float("inf")

    # è®­ç»ƒå¾ªç¯
    total_steps = 0
    start_time = time.time()

    print("å¼€å§‹è®­ç»ƒã€‚æŒ‰ Ctrl+C ç»ˆæ­¢è®­ç»ƒã€‚")

    # ä½¿ç”¨è¾“å…¥å±è”½ä¸Šä¸‹æ–‡ï¼Œåªå…è®¸Ctrl+Cä¸­æ–­
    with input_shield_context():
        for episode in range(1, args.episodes + 1):
            if stop_training:
                print(f"åœ¨ç¬¬ {episode} å›åˆå¼€å§‹å‰ç»ˆæ­¢è®­ç»ƒã€‚")
                break

            state, _ = env.reset()
            episode_reward = 0
            episode_loss = 0
            episode_steps = 0

            done = False
            truncated = False

            # å•å›åˆå¾ªç¯
            while not (done or truncated):
                if stop_training:
                    print(f"åœ¨ç¬¬ {episode} å›åˆä¸­é€”ç»ˆæ­¢è®­ç»ƒã€‚")
                    break

                # é€‰æ‹©åŠ¨ä½œ
                action = agent.select_action(state)

                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, truncated, _ = env.step(action)

                # å­˜å‚¨ç»éªŒ
                if isinstance(agent, RainbowAgent):
                    agent.store_experience(state, action, reward, next_state, done)
                else:
                    agent.memory.push(state, action, reward, next_state, done)

                # æ›´æ–°æ¨¡å‹
                loss = agent.update_model(current_episode=episode, total_episodes=args.episodes)
                if loss is not None:
                    episode_loss += loss

                # æ›´æ–°ç›®æ ‡ç½‘ç»œ
                if total_steps % args.target_update == 0:
                    agent.update_target_model()

                # æ›´æ–°çŠ¶æ€å’Œç»Ÿè®¡
                state = next_state
                episode_reward += reward
                episode_steps += 1
                total_steps += 1

            if stop_training and not (
                done or truncated
            ):  # å¦‚æœæ˜¯å› ä¸ºCtrl+Cè·³å‡ºå†…éƒ¨å¾ªç¯
                break  # ä¹Ÿè·³å‡ºå¤–éƒ¨å›åˆå¾ªç¯

            # è®°å½•å›åˆç»Ÿè®¡
            rewards.append(episode_reward)
            avg_reward = np.mean(rewards[-100:])  # æœ€è¿‘100å›åˆçš„å¹³å‡å¥–åŠ±
            avg_rewards.append(avg_reward)

            # è®°å½•åˆ°TensorBoard
            writer.add_scalar("Train/Reward", episode_reward, episode)
            writer.add_scalar("Train/AvgReward", avg_reward, episode)
            writer.add_scalar(
                "Train/Loss",
                episode_loss / episode_steps if episode_steps > 0 else 0,
                episode,
            )
            # è®¡ç®—å½“å‰epsilonå€¼ï¼ˆä»…å¯¹DQNæœ‰æ•ˆï¼ŒRainbowä½¿ç”¨å™ªå£°ç½‘ç»œï¼‰
            if hasattr(agent, "epsilon_start"):
                current_epsilon = agent.epsilon_final + (
                    agent.epsilon_start - agent.epsilon_final
                ) * np.exp(-1.0 * agent.steps_done / agent.epsilon_decay)
                writer.add_scalar("Train/Epsilon", current_epsilon, episode)
            else:
                # Rainbowä½¿ç”¨å™ªå£°ç½‘ç»œï¼Œepsilonå›ºå®šä¸º0
                writer.add_scalar("Train/Epsilon", 0.0, episode)

            # æ‰“å°è¿›åº¦
            if episode % 10 == 0:
                elapsed_time = time.time() - start_time
                # è®¡ç®—å½“å‰æ¢ç´¢ç‡æ˜¾ç¤º
                if hasattr(agent, 'epsilon_start'):
                    current_epsilon = agent.epsilon_final + (agent.epsilon_start - agent.epsilon_final) * \
                                    np.exp(-1. * agent.steps_done / agent.epsilon_decay)
                    epsilon_str = f"æ¢ç´¢ç‡: {current_epsilon:.3f}, "
                else:
                    epsilon_str = "æ¢ç´¢ç‡: 0.000 (å™ªå£°ç½‘ç»œ), "
                
                print(
                    f"å›åˆ {episode}/{args.episodes}, "
                    f"å¥–åŠ±: {episode_reward:.2f}, "
                    f"å¹³å‡å¥–åŠ±: {avg_reward:.2f}, "
                    f"{epsilon_str}"
                    f"ç”¨æ—¶: {elapsed_time:.1f}s"
                )

            # è¯„ä¼°æ¨¡å‹
            if episode % args.eval_interval == 0:
                eval_reward = evaluate(agent, eval_env, device)
                writer.add_scalar("Eval/Reward", eval_reward, episode)
                print(f"è¯„ä¼°å¥–åŠ±: {eval_reward:.2f}")

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if eval_reward > best_avg_reward:
                    previous_best_avg_reward = best_avg_reward
                    best_avg_reward = eval_reward
                    agent.save_model(
                        os.path.join(args.save_dir, f"best_model_{args.model}.pth")
                    )
                    if previous_best_avg_reward == -float("inf"):
                        print(f"ğŸš€ Initial best score: {best_avg_reward:.2f}. Model saved.")
                    else:
                        print(f"ğŸš€ New best score! Previous: {previous_best_avg_reward:.2f}, Current: {best_avg_reward:.2f}. Model saved.")

            # å®šæœŸä¿å­˜æ¨¡å‹
            if episode % args.save_interval == 0:
                agent.save_model(
                    os.path.join(args.save_dir, f"{args.model}_episode_{episode}.pth")
                )
                print(f"åœ¨ç¬¬ {episode} å›åˆä¿å­˜äº†æ¨¡å‹ã€‚")

        if not stop_training:
            print("è®­ç»ƒå®Œæˆã€‚")
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            agent.save_model(
                os.path.join(args.save_dir, f"final_model_{args.model}.pth")
            )
            print("æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ã€‚")
        else:
            print("è®­ç»ƒè¢«ç”¨æˆ·æå‰ç»ˆæ­¢ã€‚")
            # è€ƒè™‘æ˜¯å¦åœ¨ç»ˆæ­¢æ—¶ä¹Ÿä¿å­˜å½“å‰æ¨¡å‹
            # agent.save_model(os.path.join(args.save_dir, f"interrupted_model_{args.model}_episode_{episode}.pth"))
            # print(f"å·²ä¿å­˜ä¸­æ–­æ—¶çš„æ¨¡å‹åˆ° episode {episode}ã€‚")

        # ç»˜åˆ¶å¥–åŠ±æ›²çº¿ (å³ä½¿ä¸­æ–­ä¹Ÿç»˜åˆ¶å·²æœ‰çš„æ•°æ®)
        if rewards:  # ç¡®ä¿æœ‰æ•°æ®å¯ç»˜åˆ¶
            plot_rewards(
                rewards,
                avg_rewards,
                title=f"{args.model.upper()} Training on {args.env} (Interrupted: {stop_training})",
                save_path=os.path.join(
                    args.log_dir,
                    f"{args.model}_rewards_{'interrupted' if stop_training else 'final'}.png",
                ),
            )
            print("å¥–åŠ±æ›²çº¿å·²ç»˜åˆ¶å¹¶ä¿å­˜ã€‚")

        # å…³é—­TensorBoardå†™å…¥å™¨
        writer.close()
        print("TensorBoardå†™å…¥å™¨å·²å…³é—­ã€‚")

        # æ¸…ç†ç¯å¢ƒèµ„æºï¼Œé¿å…pybind11 GILé—®é¢˜
        try:
            if hasattr(env, "close"):
                env.close()
            if hasattr(eval_env, "close"):
                eval_env.close()
            print("ç¯å¢ƒèµ„æºå·²æ¸…ç†ã€‚")
        except Exception as e:
            print(f"æ¸…ç†ç¯å¢ƒæ—¶å‡ºç°è­¦å‘Š: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºè¿™åªæ˜¯æ¸…ç†æ“ä½œ

    return rewards, avg_rewards


def evaluate(agent, env, device, n_episodes=5, max_steps=10000):
    """
    è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½

    å‚æ•°:
        agent: DQNæ™ºèƒ½ä½“
        env: æ¸¸æˆç¯å¢ƒ
        device: è®¡ç®—è®¾å¤‡
        n_episodes: è¯„ä¼°å›åˆæ•°
        max_steps: æ¯å›åˆæœ€å¤§æ­¥æ•°

    è¿”å›:
        å¹³å‡å¥–åŠ±
    """
    total_rewards = []

    for _ in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0

        for _ in range(max_steps):
            # é€‰æ‹©åŠ¨ä½œ (è¯„ä¼°æ¨¡å¼ï¼Œä¸ä½¿ç”¨æ¢ç´¢)
            action = agent.select_action(state, evaluate=True)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, truncated, _ = env.step(action)

            # æ›´æ–°çŠ¶æ€å’Œå¥–åŠ±
            state = next_state
            episode_reward += reward

            if done or truncated:
                break

        total_rewards.append(episode_reward)

    # è¿”å›å¹³å‡å¥–åŠ±
    return np.mean(total_rewards)


if __name__ == "__main__":
    args = parse_args()
    train(args)
