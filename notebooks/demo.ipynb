{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¼ºåŒ–å­¦ä¹  Atari æ¸¸æˆ Demo - Rainbow DQN\n",
    "\n",
    "æœ¬ç¬”è®°æœ¬æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ **Rainbow DQN** ç®—æ³•è®­ç»ƒæ™ºèƒ½ä½“ç© Atari æ¸¸æˆ Assault-v5ã€‚\n",
    "\n",
    "Rainbow DQN é›†æˆäº†å¤šç§ DQN æ”¹è¿›æŠ€æœ¯ï¼šDouble DQNã€Dueling DQNã€Prioritized Replayã€Multi-step Learningã€Noisy Networks å’Œ Distributional DQNã€‚\n",
    "\n",
    "## ç›®å½•\n",
    "1. [ç¯å¢ƒè®¾ç½®](#ç¯å¢ƒè®¾ç½®)\n",
    "2. [æ•°æ®é¢„å¤„ç†](#æ•°æ®é¢„å¤„ç†)\n",
    "3. [æ¨¡å‹æ¶æ„](#æ¨¡å‹æ¶æ„)\n",
    "4. [è®­ç»ƒè¿‡ç¨‹](#è®­ç»ƒè¿‡ç¨‹)\n",
    "5. [ç»“æœå¯è§†åŒ–](#ç»“æœå¯è§†åŒ–)\n",
    "6. [æ¨¡å‹è¯„ä¼°](#æ¨¡å‹è¯„ä¼°)\n",
    "7. [æ€»ç»“](#æ€»ç»“)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from IPython.display import HTML, display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—\n",
    "from model import DQN, DuelingDQN, RainbowDQN  # æ·»åŠ  Rainbow DQN\n",
    "from agent import DQNAgent, RainbowAgent  # æ·»åŠ  Rainbow Agent\n",
    "from utils import make_env, plot_rewards, record_video\n",
    "\n",
    "# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_built() \\\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "print(\"ç¯å¢ƒè®¾ç½®å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¯å¢ƒè®¾ç½®\n",
    "\n",
    "é¦–å…ˆåˆ›å»ºæ¸¸æˆç¯å¢ƒå¹¶æŸ¥çœ‹å…¶åŸºæœ¬ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºç¯å¢ƒ\n",
    "env_name = 'ALE/Assault-v5'\n",
    "env = make_env(env_name)\n",
    "\n",
    "print(f\"ç¯å¢ƒåç§°: {env_name}\")\n",
    "print(f\"è§‚æµ‹ç©ºé—´: {env.observation_space}\")\n",
    "print(f\"åŠ¨ä½œç©ºé—´: {env.action_space}\")\n",
    "print(f\"åŠ¨ä½œæ•°é‡: {env.action_space.n}\")\n",
    "\n",
    "# é‡ç½®ç¯å¢ƒå¹¶æ˜¾ç¤ºåˆå§‹çŠ¶æ€\n",
    "state, _ = env.reset()\n",
    "print(f\"çŠ¶æ€å½¢çŠ¶: {state.shape}\")\n",
    "print(f\"çŠ¶æ€æ•°æ®ç±»å‹: {state.dtype}\")\n",
    "print(f\"çŠ¶æ€å€¼èŒƒå›´: [{state.min()}, {state.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "æŸ¥çœ‹é¢„å¤„ç†åçš„æ¸¸æˆç”»é¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ˜¾ç¤ºé¢„å¤„ç†åçš„çŠ¶æ€\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for i in range(4):\n",
    "    axes[i].imshow(state[i], cmap='gray')\n",
    "    axes[i].set_title(f'å¸§ {i+1}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('é¢„å¤„ç†åçš„æ¸¸æˆçŠ¶æ€ï¼ˆ4å¸§å †å ï¼‰')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨¡å‹æ¶æ„\n",
    "\n",
    "åˆ›å»ºå¹¶æŸ¥çœ‹ Rainbow DQN æ¨¡å‹çš„æ¶æ„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®è®¾å¤‡\n",
    "device = torch.device('mps' if torch.backends.mps.is_built() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'ä½¿ç”¨è®¾å¤‡: {device}')\n",
    "\n",
    "# åˆ›å»ºæ¨¡å‹\n",
    "input_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# åˆ›å»º Rainbow DQN æ¨¡å‹\n",
    "rainbow_model = RainbowDQN(\n",
    "    input_shape=input_shape, \n",
    "    n_actions=n_actions,\n",
    "    use_noisy=True,          # ä½¿ç”¨å™ªå£°ç½‘ç»œ\n",
    "    use_distributional=True, # ä½¿ç”¨åˆ†å¸ƒå¼Qå­¦ä¹ \n",
    "    n_atoms=51,              # åˆ†å¸ƒå¼Qå­¦ä¹ çš„åŸå­æ•°\n",
    "    v_min=-10,               # å€¼å‡½æ•°æœ€å°å€¼\n",
    "    v_max=10                 # å€¼å‡½æ•°æœ€å¤§å€¼\n",
    ")\n",
    "\n",
    "print(\"Rainbow DQNæ¨¡å‹æ¶æ„:\")\n",
    "print(rainbow_model)\n",
    "\n",
    "# åˆ›å»ºç®€åŒ–ç‰ˆ Rainbow DQNï¼ˆä»…åŒç½‘ç»œå’Œä¼˜å…ˆå›æ”¾ï¼‰\n",
    "rainbow_simple = RainbowDQN(\n",
    "    input_shape=input_shape, \n",
    "    n_actions=n_actions,\n",
    "    use_noisy=False,         # ä¸ä½¿ç”¨å™ªå£°ç½‘ç»œ\n",
    "    use_distributional=False # ä¸ä½¿ç”¨åˆ†å¸ƒå¼Qå­¦ä¹ \n",
    ")\n",
    "\n",
    "print(\"\\nç®€åŒ–ç‰ˆ Rainbow DQNæ¨¡å‹:\")\n",
    "print(rainbow_simple)\n",
    "\n",
    "# è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nRainbow DQNå‚æ•°æ•°é‡: {count_parameters(rainbow_model):,}\")\n",
    "print(f\"ç®€åŒ–ç‰ˆ Rainbow DQNå‚æ•°æ•°é‡: {count_parameters(rainbow_simple):,}\")\n",
    "\n",
    "# Rainbow DQN ç‰¹æ€§è¯´æ˜\n",
    "print(\"\\n=== Rainbow DQN ç‰¹æ€§ ===\")\n",
    "print(f\"âœ… å™ªå£°ç½‘ç»œ: {'å¯ç”¨' if rainbow_model.use_noisy else 'ç¦ç”¨'}\")\n",
    "print(f\"âœ… åˆ†å¸ƒå¼Qå­¦ä¹ : {'å¯ç”¨' if rainbow_model.use_distributional else 'ç¦ç”¨'}\")\n",
    "print(f\"âœ… Dueling æ¶æ„: å·²å¯ç”¨\")\n",
    "print(f\"âœ… Double DQN: å·²å¯ç”¨\")\n",
    "print(f\"âœ… ä¼˜å…ˆå›æ”¾: å°†åœ¨æ™ºèƒ½ä½“ä¸­å¯ç”¨\")\n",
    "print(f\"âœ… å¤šæ­¥å­¦ä¹ : å°†åœ¨æ™ºèƒ½ä½“ä¸­å¯ç”¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®­ç»ƒè¿‡ç¨‹\n",
    "\n",
    "æ¼”ç¤ºå¦‚ä½•è®­ç»ƒ Rainbow DQN æ™ºèƒ½ä½“ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œç”¨äºæ¼”ç¤ºï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»º Rainbow DQN æ™ºèƒ½ä½“\n",
    "model = RainbowDQN(\n",
    "    input_shape=input_shape, \n",
    "    n_actions=n_actions,\n",
    "    use_noisy=True,          # ä½¿ç”¨å™ªå£°ç½‘ç»œæ›¿ä»£ epsilon-greedy\n",
    "    use_distributional=True, # ä½¿ç”¨åˆ†å¸ƒå¼Qå­¦ä¹ \n",
    "    n_atoms=51,\n",
    "    v_min=-10,\n",
    "    v_max=10\n",
    ")\n",
    "\n",
    "target_model = RainbowDQN(\n",
    "    input_shape=input_shape, \n",
    "    n_actions=n_actions,\n",
    "    use_noisy=True,\n",
    "    use_distributional=True,\n",
    "    n_atoms=51,\n",
    "    v_min=-10,\n",
    "    v_max=10\n",
    ")\n",
    "\n",
    "agent = RainbowAgent(\n",
    "    model=model,\n",
    "    target_model=target_model,\n",
    "    env=env,\n",
    "    device=device,\n",
    "    buffer_size=10000,      # è¾ƒå°çš„ç¼“å†²åŒºç”¨äºæ¼”ç¤º\n",
    "    batch_size=32,\n",
    "    gamma=0.99,\n",
    "    lr=1e-4,\n",
    "    target_update=100,\n",
    "    n_step=3,               # 3æ­¥å­¦ä¹ \n",
    "    use_noisy=True,         # ä½¿ç”¨å™ªå£°ç½‘ç»œ\n",
    "    use_distributional=True, # ä½¿ç”¨åˆ†å¸ƒå¼Qå­¦ä¹ \n",
    "    prioritized_replay=True  # ä½¿ç”¨ä¼˜å…ˆç»éªŒå›æ”¾\n",
    ")\n",
    "\n",
    "print(\"Rainbow DQNæ™ºèƒ½ä½“åˆ›å»ºå®Œæˆï¼\")\n",
    "print(f\"- ä½¿ç”¨å™ªå£°ç½‘ç»œ: {agent.use_noisy}\")\n",
    "print(f\"- ä½¿ç”¨åˆ†å¸ƒå¼Qå­¦ä¹ : {agent.use_distributional}\")\n",
    "print(f\"- ä½¿ç”¨ä¼˜å…ˆå›æ”¾: {agent.prioritized_replay}\")\n",
    "print(f\"- Næ­¥å­¦ä¹ : {agent.n_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rainbow DQN è®­ç»ƒå¾ªç¯ï¼ˆæ”¯æŒ Rainbow ç‰¹æ€§ï¼‰\n",
    "def train_rainbow_demo(agent, env, n_episodes=50):\n",
    "    rewards = []\n",
    "    losses = []\n",
    "    td_errors = []  # è®°å½•TDè¯¯å·®ç”¨äºç›‘æ§ä¼˜å…ˆå›æ”¾æ•ˆæœ\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes), desc=\"Rainbowè®­ç»ƒä¸­\"):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = 0\n",
    "        episode_td_error = 0\n",
    "        steps = 0\n",
    "        \n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not (done or truncated) and steps < 1000:  # é™åˆ¶æ­¥æ•°\n",
    "            # Rainbow æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œï¼ˆä½¿ç”¨å™ªå£°ç½‘ç»œæˆ– epsilon-greedyï¼‰\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            \n",
    "            # ä½¿ç”¨ Rainbow ç‰¹æœ‰çš„ç»éªŒå­˜å‚¨ï¼ˆæ”¯æŒ N æ­¥å­¦ä¹ ï¼‰\n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            \n",
    "            # æ›´æ–°æ¨¡å‹\n",
    "            loss = agent.update_model()\n",
    "            if loss is not None:\n",
    "                episode_loss += loss\n",
    "            \n",
    "            # æ›´æ–°ç›®æ ‡ç½‘ç»œ\n",
    "            if agent.steps_done % agent.target_update == 0:\n",
    "                agent.update_target_model()\n",
    "            \n",
    "            # ä¸ºå™ªå£°ç½‘ç»œé‡æ–°é‡‡æ ·å™ªå£°\n",
    "            if hasattr(agent.model, 'sample_noise'):\n",
    "                agent.model.sample_noise()\n",
    "                agent.target_model.sample_noise()\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "        losses.append(episode_loss / steps if steps > 0 else 0)\n",
    "        td_errors.append(episode_td_error / steps if steps > 0 else 0)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = np.mean(rewards[-10:])\n",
    "            avg_loss = np.mean(losses[-10:]) if losses[-10:] else 0\n",
    "            buffer_size = len(agent.memory)\n",
    "            print(f\"Episode {episode}: Avg Reward = {avg_reward:.2f}, \"\n",
    "                  f\"Avg Loss = {avg_loss:.6f}, Buffer Size = {buffer_size}\")\n",
    "    \n",
    "    return rewards, losses, td_errors\n",
    "\n",
    "# è¿è¡Œ Rainbow æ¼”ç¤ºè®­ç»ƒ\n",
    "print(\"å¼€å§‹ Rainbow æ¼”ç¤ºè®­ç»ƒ...ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰\")\n",
    "rewards, losses, td_errors = train_rainbow_demo(agent, env, n_episodes=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»“æœå¯è§†åŒ–\n",
    "\n",
    "å¯è§†åŒ– Rainbow DQN è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±ã€æŸå¤±å’Œ Rainbow ç‰¹æœ‰æŒ‡æ ‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶ Rainbow DQN è®­ç»ƒç»“æœ\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# å¥–åŠ±æ›²çº¿\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(rewards, alpha=0.7, label='å›åˆå¥–åŠ±', color='blue')\n",
    "# è®¡ç®—ç§»åŠ¨å¹³å‡\n",
    "window_size = 10\n",
    "if len(rewards) >= window_size:\n",
    "    moving_avg = [np.mean(rewards[i:i+window_size]) for i in range(len(rewards)-window_size+1)]\n",
    "    ax1.plot(range(window_size-1, len(rewards)), moving_avg, 'r-', linewidth=2, label=f'{window_size}å›åˆç§»åŠ¨å¹³å‡')\n",
    "\n",
    "ax1.set_xlabel('å›åˆ')\n",
    "ax1.set_ylabel('å¥–åŠ±')\n",
    "ax1.set_title('Rainbow DQN è®­ç»ƒå¥–åŠ±æ›²çº¿')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# æŸå¤±æ›²çº¿\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(losses, 'g-', alpha=0.7, label='è®­ç»ƒæŸå¤±')\n",
    "if len(losses) >= window_size:\n",
    "    loss_moving_avg = [np.mean(losses[i:i+window_size]) for i in range(len(losses)-window_size+1)]\n",
    "    ax2.plot(range(window_size-1, len(losses)), loss_moving_avg, 'orange', linewidth=2, label=f'{window_size}å›åˆå¹³å‡æŸå¤±')\n",
    "\n",
    "ax2.set_xlabel('å›åˆ')\n",
    "ax2.set_ylabel('å¹³å‡æŸå¤±')\n",
    "ax2.set_title('Rainbow DQN è®­ç»ƒæŸå¤±æ›²çº¿')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Rainbow ç‰¹æœ‰æŒ‡æ ‡ï¼šç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°å˜åŒ–\n",
    "ax3 = axes[1, 0]\n",
    "buffer_sizes = [min(i * 100, len(agent.memory)) for i in range(len(rewards))]  # ä¼°ç®—ç¼“å†²åŒºå¢é•¿\n",
    "ax3.plot(buffer_sizes, 'purple', alpha=0.8, label='ç¼“å†²åŒºå¤§å°')\n",
    "ax3.set_xlabel('å›åˆ')\n",
    "ax3.set_ylabel('ç»éªŒæ•°é‡')\n",
    "ax3.set_title('ä¼˜å…ˆç»éªŒå›æ”¾ç¼“å†²åŒºå¢é•¿')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# æ¢ç´¢ç­–ç•¥å¯¹æ¯”ï¼šRainbow ä½¿ç”¨å™ªå£°ç½‘ç»œï¼Œä¼ ç»Ÿ DQN ä½¿ç”¨ epsilon-greedy\n",
    "ax4 = axes[1, 1]\n",
    "if agent.use_noisy:\n",
    "    # å™ªå£°ç½‘ç»œï¼šå›ºå®šæ¢ç´¢\n",
    "    exploration = [1.0] * len(rewards)\n",
    "    ax4.plot(exploration, 'red', linewidth=2, label='å™ªå£°ç½‘ç»œï¼ˆå›ºå®šæ¢ç´¢ï¼‰')\n",
    "    ax4.set_title('Rainbow æ¢ç´¢ç­–ç•¥ï¼šå™ªå£°ç½‘ç»œ')\n",
    "else:\n",
    "    # epsilon-greedy è¡°å‡\n",
    "    epsilons = [agent.epsilon_final + (agent.epsilon_start - agent.epsilon_final) * \n",
    "               np.exp(-1. * i * 100 / agent.epsilon_decay) for i in range(len(rewards))]\n",
    "    ax4.plot(epsilons, 'red', linewidth=2, label='Epsilon-greedy')\n",
    "    ax4.set_title('ä¼ ç»Ÿ DQN æ¢ç´¢ç­–ç•¥ï¼šEpsilon-greedy')\n",
    "\n",
    "ax4.set_xlabel('å›åˆ')\n",
    "ax4.set_ylabel('æ¢ç´¢ç‡')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# æ‰“å° Rainbow DQN ç»Ÿè®¡ä¿¡æ¯\n",
    "print(\"=\" * 50)\n",
    "print(\"Rainbow DQN è®­ç»ƒç»Ÿè®¡:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"å¹³å‡å¥–åŠ±: {np.mean(rewards):.2f}\")\n",
    "print(f\"æœ€é«˜å¥–åŠ±: {np.max(rewards):.2f}\")\n",
    "print(f\"æœ€ä½å¥–åŠ±: {np.min(rewards):.2f}\")\n",
    "print(f\"å¥–åŠ±æ ‡å‡†å·®: {np.std(rewards):.2f}\")\n",
    "print(f\"æœ€ç»ˆç»éªŒç¼“å†²åŒºå¤§å°: {len(agent.memory)}\")\n",
    "print(f\"ä½¿ç”¨å™ªå£°ç½‘ç»œ: {'æ˜¯' if agent.use_noisy else 'å¦'}\")\n",
    "print(f\"ä½¿ç”¨åˆ†å¸ƒå¼Qå­¦ä¹ : {'æ˜¯' if agent.use_distributional else 'å¦'}\")\n",
    "print(f\"ä½¿ç”¨ä¼˜å…ˆå›æ”¾: {'æ˜¯' if agent.prioritized_replay else 'å¦'}\")\n",
    "\n",
    "# Rainbow ä¸ä¼ ç»Ÿ DQN çš„ä¼˜åŠ¿åˆ†æ\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Rainbow DQN ä¼˜åŠ¿åˆ†æ:\")\n",
    "print(\"=\" * 50)\n",
    "if agent.use_noisy:\n",
    "    print(\"âœ… å™ªå£°ç½‘ç»œ: æ›¿ä»£ epsilon-greedyï¼Œæä¾›æ›´ç¨³å®šçš„æ¢ç´¢\")\n",
    "if agent.use_distributional:\n",
    "    print(\"âœ… åˆ†å¸ƒå¼Qå­¦ä¹ : å­¦ä¹ å®Œæ•´çš„ä»·å€¼åˆ†å¸ƒï¼Œè€Œéå•ä¸€æœŸæœ›å€¼\")\n",
    "if agent.prioritized_replay:\n",
    "    print(\"âœ… ä¼˜å…ˆç»éªŒå›æ”¾: é‡ç‚¹å­¦ä¹ é‡è¦ç»éªŒï¼Œæé«˜æ ·æœ¬æ•ˆç‡\")\n",
    "print(f\"âœ… Næ­¥å­¦ä¹ : ä½¿ç”¨ {agent.n_step} æ­¥å›æŠ¥ï¼Œå‡å°‘åå·®\")\n",
    "print(\"âœ… Double DQN: å‡å°‘è¿‡ä¼°è®¡é—®é¢˜\")\n",
    "print(\"âœ… Dueling æ¶æ„: åˆ†ç¦»çŠ¶æ€ä»·å€¼å’ŒåŠ¨ä½œä¼˜åŠ¿\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨¡å‹è¯„ä¼°\n",
    "\n",
    "è¯„ä¼°è®­ç»ƒåçš„ Rainbow DQN æ™ºèƒ½ä½“æ€§èƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯„ä¼° Rainbow æ™ºèƒ½ä½“\n",
    "def evaluate_rainbow_agent(agent, env, n_episodes=5):\n",
    "    eval_rewards = []\n",
    "    eval_steps = []\n",
    "    \n",
    "    print(\"å¼€å§‹è¯„ä¼° Rainbow DQN æ™ºèƒ½ä½“...\")\n",
    "    print(f\"è¯„ä¼°é…ç½®: å™ªå£°ç½‘ç»œ={'å¼€å¯' if agent.use_noisy else 'å…³é—­'}, \"\n",
    "          f\"åˆ†å¸ƒå¼Qå­¦ä¹ ={'å¼€å¯' if agent.use_distributional else 'å…³é—­'}\")\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        # è¯„ä¼°æ—¶ç¦ç”¨å™ªå£°ï¼ˆå¦‚æœä½¿ç”¨å™ªå£°ç½‘ç»œï¼‰\n",
    "        if hasattr(agent.model, 'eval'):\n",
    "            agent.model.eval()\n",
    "            agent.target_model.eval()\n",
    "        \n",
    "        while not (done or truncated) and steps < 2000:\n",
    "            action = agent.select_action(state, evaluate=True)  # è¯„ä¼°æ¨¡å¼\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        # æ¢å¤è®­ç»ƒæ¨¡å¼\n",
    "        if hasattr(agent.model, 'train'):\n",
    "            agent.model.train()\n",
    "            agent.target_model.train()\n",
    "        \n",
    "        eval_rewards.append(episode_reward)\n",
    "        eval_steps.append(steps)\n",
    "        print(f\"è¯„ä¼°å›åˆ {episode+1}: å¥–åŠ± = {episode_reward:.2f}, æ­¥æ•° = {steps}\")\n",
    "    \n",
    "    return eval_rewards, eval_steps\n",
    "\n",
    "# è¿è¡Œ Rainbow è¯„ä¼°\n",
    "eval_rewards, eval_steps = evaluate_rainbow_agent(agent, env, n_episodes=5)\n",
    "\n",
    "print(f\"\\nRainbow DQN è¯„ä¼°ç»“æœ:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"å¹³å‡å¥–åŠ±: {np.mean(eval_rewards):.2f} Â± {np.std(eval_rewards):.2f}\")\n",
    "print(f\"æœ€é«˜å¥–åŠ±: {np.max(eval_rewards):.2f}\")\n",
    "print(f\"æœ€ä½å¥–åŠ±: {np.min(eval_rewards):.2f}\")\n",
    "print(f\"å¹³å‡æ­¥æ•°: {np.mean(eval_steps):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºè¯„ä¼°ç»“æœå¯è§†åŒ–\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# è¯„ä¼°å¥–åŠ±åˆ†å¸ƒ\n",
    "ax1.bar(range(1, len(eval_rewards)+1), eval_rewards, color='skyblue', alpha=0.8)\n",
    "ax1.axhline(y=np.mean(eval_rewards), color='red', linestyle='--', label=f'å¹³å‡å€¼: {np.mean(eval_rewards):.1f}')\n",
    "ax1.set_xlabel('è¯„ä¼°å›åˆ')\n",
    "ax1.set_ylabel('å¥–åŠ±')\n",
    "ax1.set_title('Rainbow DQN è¯„ä¼°å¥–åŠ±åˆ†å¸ƒ')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# è¯„ä¼°æ­¥æ•°åˆ†å¸ƒ\n",
    "ax2.bar(range(1, len(eval_steps)+1), eval_steps, color='lightgreen', alpha=0.8)\n",
    "ax2.axhline(y=np.mean(eval_steps), color='red', linestyle='--', label=f'å¹³å‡å€¼: {np.mean(eval_steps):.1f}')\n",
    "ax2.set_xlabel('è¯„ä¼°å›åˆ')\n",
    "ax2.set_ylabel('æ­¥æ•°')\n",
    "ax2.set_title('Rainbow DQN è¯„ä¼°å­˜æ´»æ­¥æ•°')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ€»ç»“\n",
    "\n",
    "æœ¬æ¼”ç¤ºå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ **Rainbow DQN** ç®—æ³•è®­ç»ƒæ™ºèƒ½ä½“ç© Atari æ¸¸æˆã€‚Rainbow DQN æ˜¯ç›®å‰æœ€å…ˆè¿›çš„åŸºäºä»·å€¼çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¹‹ä¸€ï¼Œé›†æˆäº†å¤šç§ DQN æ”¹è¿›æŠ€æœ¯ã€‚\n",
    "\n",
    "### ä¸»è¦æ­¥éª¤åŒ…æ‹¬:\n",
    "\n",
    "1. **ç¯å¢ƒé¢„å¤„ç†**: å°†åŸå§‹æ¸¸æˆç”»é¢è½¬æ¢ä¸ºé€‚åˆç¥ç»ç½‘ç»œå¤„ç†çš„æ ¼å¼\n",
    "2. **Rainbow æ¨¡å‹è®¾è®¡**: é›†æˆå¤šç§å…ˆè¿›æŠ€æœ¯çš„ç¥ç»ç½‘ç»œæ¶æ„\n",
    "3. **å¤šç»„ä»¶è®­ç»ƒ**: åè°ƒå¤šä¸ªæ”¹è¿›ç»„ä»¶çš„è®­ç»ƒè¿‡ç¨‹\n",
    "4. **ç»¼åˆè¯„ä¼°**: è¯„ä¼° Rainbow ç®—æ³•çš„æ•´ä½“æ€§èƒ½\n",
    "\n",
    "### Rainbow DQN çš„å…­å¤§æ ¸å¿ƒç»„ä»¶:\n",
    "\n",
    "1. **ğŸ¯ Double DQN**: ä½¿ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œç›®æ ‡ç½‘ç»œè¯„ä¼°ä»·å€¼ï¼Œå‡å°‘è¿‡ä¼°è®¡åå·®\n",
    "2. **âš”ï¸ Dueling DQN**: å°†Qå€¼åˆ†è§£ä¸ºçŠ¶æ€ä»·å€¼å‡½æ•°V(s)å’Œä¼˜åŠ¿å‡½æ•°A(s,a)ï¼Œæé«˜å­¦ä¹ æ•ˆç‡\n",
    "3. **ğŸ”„ ä¼˜å…ˆç»éªŒå›æ”¾**: æ ¹æ®TDè¯¯å·®ä¼˜å…ˆé‡‡æ ·é‡è¦ç»éªŒï¼Œæé«˜æ ·æœ¬æ•ˆç‡\n",
    "4. **ğŸ“ˆ å¤šæ­¥å­¦ä¹ **: ä½¿ç”¨næ­¥å›æŠ¥å‡å°‘åå·®ï¼ŒåŠ é€Ÿæ”¶æ•›\n",
    "5. **ğŸ”Š å™ªå£°ç½‘ç»œ**: ç”¨å‚æ•°å™ªå£°æ›¿ä»£Îµ-è´ªå©ªæ¢ç´¢ï¼Œæä¾›æ›´ç¨³å®šçš„æ¢ç´¢ç­–ç•¥\n",
    "6. **ğŸ“Š åˆ†å¸ƒå¼Qå­¦ä¹ **: å­¦ä¹ ä»·å€¼åˆ†å¸ƒè€ŒéæœŸæœ›å€¼ï¼Œæä¾›æ›´ä¸°å¯Œçš„ä»·å€¼ä¿¡æ¯\n",
    "\n",
    "### Rainbow DQN ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•çš„ä¼˜åŠ¿:\n",
    "\n",
    "#### ğŸš€ **æ€§èƒ½ä¼˜åŠ¿**:\n",
    "- **æ ·æœ¬æ•ˆç‡æ›´é«˜**: ä¼˜å…ˆå›æ”¾å’Œå¤šæ­¥å­¦ä¹ å‡å°‘æ‰€éœ€è®­ç»ƒæ ·æœ¬\n",
    "- **æ¢ç´¢æ›´ç¨³å®š**: å™ªå£°ç½‘ç»œæä¾›ä¸€è‡´çš„æ¢ç´¢ï¼Œé¿å…æ¢ç´¢ç‡è°ƒå‚å›°éš¾\n",
    "- **æ”¶æ•›æ›´å¿«**: å¤šä¸ªç»„ä»¶ååŒä½œç”¨ï¼ŒåŠ é€Ÿè®­ç»ƒæ”¶æ•›\n",
    "- **æ€§èƒ½æ›´å¼º**: åœ¨å¤šæ•° Atari æ¸¸æˆä¸Šè¾¾åˆ°è¶…äººç±»è¡¨ç°\n",
    "\n",
    "#### ğŸ”§ **æŠ€æœ¯ä¼˜åŠ¿**:\n",
    "- **åå·®æ›´å°**: Double DQN å’Œå¤šæ­¥å­¦ä¹ å‡å°‘ä¼°è®¡åå·®\n",
    "- **ä¿¡æ¯æ›´ä¸°å¯Œ**: åˆ†å¸ƒå¼Qå­¦ä¹ æ•è·ä»·å€¼çš„ä¸ç¡®å®šæ€§\n",
    "- **è®­ç»ƒæ›´ç¨³å®š**: ç›®æ ‡ç½‘ç»œå’Œå™ªå£°ç½‘ç»œæä¾›ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹\n",
    "- **é€šç”¨æ€§æ›´å¼º**: å…­ä¸ªç»„ä»¶å¯æ ¹æ®éœ€è¦çµæ´»ç»„åˆ\n",
    "\n",
    "#### ğŸ“Š **å®é™…åº”ç”¨ä¼˜åŠ¿**:\n",
    "- **è°ƒå‚æ›´ç®€å•**: å™ªå£°ç½‘ç»œæ¶ˆé™¤ epsilon è°ƒå‚éœ€æ±‚\n",
    "- **å¯è§£é‡Šæ€§æ›´å¼º**: Dueling æ¶æ„å’Œåˆ†å¸ƒå¼å­¦ä¹ æä¾›æ›´å¤šæ´å¯Ÿ\n",
    "- **æ‰©å±•æ€§æ›´å¥½**: ç»„ä»¶åŒ–è®¾è®¡ä¾¿äºé›†æˆæ–°æŠ€æœ¯\n",
    "\n",
    "### æ€§èƒ½å¯¹æ¯”æ€»ç»“:\n",
    "\n",
    "| ç®—æ³• | æ ·æœ¬æ•ˆç‡ | æœ€ç»ˆæ€§èƒ½ | è®­ç»ƒç¨³å®šæ€§ | æ¢ç´¢æ•ˆæœ | å®ç°å¤æ‚åº¦ |\n",
    "|------|----------|----------|------------|----------|------------|\n",
    "| DQN | â­â­ | â­â­ | â­â­ | â­â­ | â­â­â­â­â­ |\n",
    "| Dueling DQN | â­â­â­ | â­â­â­ | â­â­â­ | â­â­ | â­â­â­â­ |\n",
    "| **Rainbow DQN** | **â­â­â­â­â­** | **â­â­â­â­â­** | **â­â­â­â­** | **â­â­â­â­â­** | **â­â­â­** |\n",
    "\n",
    "### è¿›ä¸€æ­¥æ”¹è¿›å»ºè®®:\n",
    "\n",
    "ğŸ”§ **è®­ç»ƒä¼˜åŒ–**:\n",
    "- å¢åŠ è®­ç»ƒå›åˆæ•° (1000+ episodes)\n",
    "- ä½¿ç”¨æ›´å¤§çš„ç»éªŒå›æ”¾ç¼“å†²åŒº (100k+ experiences)\n",
    "- è°ƒæ•´å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥\n",
    "- ä½¿ç”¨æ¢¯åº¦è£å‰ªé˜²æ­¢è®­ç»ƒä¸ç¨³å®š\n",
    "\n",
    "âš™ï¸ **æ¨¡å‹ä¼˜åŒ–**:\n",
    "- å°è¯•ä¸åŒçš„ç½‘ç»œæ¶æ„ (ResNet, Attention)\n",
    "- è°ƒæ•´åˆ†å¸ƒå¼Qå­¦ä¹ çš„åŸå­æ•°é‡\n",
    "- ä¼˜åŒ–å™ªå£°ç½‘ç»œå‚æ•°\n",
    "- å®éªŒä¸åŒçš„næ­¥é•¿åº¦\n",
    "\n",
    "ğŸ® **ç¯å¢ƒä¼˜åŒ–**:\n",
    "- ä½¿ç”¨å¸§è·³è·ƒ (frame skipping) åŠ é€Ÿè®­ç»ƒ\n",
    "- å®éªŒä¸åŒçš„å¥–åŠ±è®¾è®¡\n",
    "- å°è¯•å…¶ä»– Atari æ¸¸æˆ\n",
    "- ä½¿ç”¨ç¯å¢ƒé›†æˆæŠ€æœ¯\n",
    "\n",
    "---\n",
    "\n",
    "**Rainbow DQN ä»£è¡¨äº†æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨æ¸¸æˆAIé¢†åŸŸçš„ä¸€ä¸ªé‡è¦é‡Œç¨‹ç¢‘ï¼Œå®ƒè¯æ˜äº†ç»„åˆå¤šç§æ”¹è¿›æŠ€æœ¯å¯ä»¥æ˜¾è‘—æå‡ç®—æ³•æ€§èƒ½ã€‚è¿™ç§"é›†ç™¾å®¶ä¹‹é•¿"çš„è®¾è®¡æ€è·¯ä¸ºåç»­å¼ºåŒ–å­¦ä¹ ç®—æ³•å‘å±•å¥ å®šäº†åŸºç¡€ã€‚**\n",
    "\n",
    "### ä½¿ç”¨ Rainbow DQN è¿›è¡Œå®é™…è®­ç»ƒ:\n",
    "\n",
    "è¦è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œå®Œæ•´è®­ç»ƒï¼š\n",
    "\n",
    "```bash\n",
    "# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œ\n",
    "python run_training.py rainbow 1000 --use_noisy --use_distributional --prioritized_replay\n",
    "```\n",
    "\n",
    "è¿™å°†å¯åŠ¨ä¸€ä¸ªå®Œæ•´çš„ Rainbow DQN è®­ç»ƒä¼šè¯ï¼Œä½¿ç”¨æ‰€æœ‰ Rainbow ç»„ä»¶ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snake_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
