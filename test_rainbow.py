#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Rainbow DQN å®ç°æµ‹è¯•è„šæœ¬
éªŒè¯æ‰€æœ‰ç»„ä»¶æ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œ
"""

import torch
import numpy as np
from src.model import RainbowDQN
from src.agent import RainbowAgent
from src.buffers.n_step_buffers import NStepBuffer
from src.utils import make_env

def test_noisy_linear():
    """æµ‹è¯•å™ªå£°çº¿æ€§å±‚"""
    print("æµ‹è¯•å™ªå£°çº¿æ€§å±‚...")
    from src.model import NoisyLinear
    
    layer = NoisyLinear(64, 32, factorised=True)
    x = torch.randn(10, 64)
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    output1 = layer(x)
    output2 = layer(x)
    
    # å™ªå£°ç½‘ç»œåœ¨æ²¡æœ‰é‡ç½®å™ªå£°çš„æƒ…å†µä¸‹ï¼Œè¿ç»­è°ƒç”¨åº”è¯¥äº§ç”Ÿç›¸åŒçš„ç»“æœ
    assert torch.equal(output1, output2), "å™ªå£°ç½‘ç»œåœ¨æ²¡æœ‰é‡ç½®å™ªå£°çš„æƒ…å†µä¸‹ï¼Œè¿ç»­è°ƒç”¨è¾“å‡ºåº”è¯¥ç›¸åŒ"
    
    # æµ‹è¯•é‡ç½®å™ªå£°
    layer.sample_noise() # Correct method name
    output3 = layer(x)
    assert not torch.equal(output1, output3), "é‡ç½®å™ªå£°åè¾“å‡ºåº”è¯¥ä¸åŒ"
    
    print("âœ… å™ªå£°çº¿æ€§å±‚æµ‹è¯•é€šè¿‡")

def test_rainbow_dqn():
    """æµ‹è¯• Rainbow DQN ç½‘ç»œ"""
    print("æµ‹è¯• Rainbow DQN ç½‘ç»œ...")
    
    input_shape = (4, 84, 84)
    n_actions = 6
    n_atoms = 51
    
    # æµ‹è¯•æ ‡å‡† Q ç½‘ç»œæ¨¡å¼
    model_standard = RainbowDQN(
        input_shape=input_shape,
        n_actions=n_actions,
        use_noisy=False,
        use_distributional=False
    )
    
    x = torch.randn(2, *input_shape)
    output = model_standard(x)
    assert output.shape == (2, n_actions), f"æ ‡å‡†æ¨¡å¼è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape}"
    
    # æµ‹è¯•åˆ†å¸ƒå¼ Q ç½‘ç»œæ¨¡å¼
    model_distributional = RainbowDQN(
        input_shape=input_shape,
        n_actions=n_actions,
        use_noisy=False,
        use_distributional=True,
        n_atoms=n_atoms
    )
    
    output = model_distributional(x)
    assert output.shape == (2, n_actions, n_atoms), f"åˆ†å¸ƒå¼æ¨¡å¼è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape}"
    
    # æµ‹è¯•å™ªå£°ç½‘ç»œ
    model_noisy = RainbowDQN(
        input_shape=input_shape,
        n_actions=n_actions,
        use_noisy=True,
        use_distributional=False
    )
    
    output1 = model_noisy(x)
    output2 = model_noisy(x)
    # å™ªå£°ç½‘ç»œåœ¨æ²¡æœ‰é‡ç½®å™ªå£°çš„æƒ…å†µä¸‹ï¼Œè¿ç»­è°ƒç”¨åº”è¯¥äº§ç”Ÿç›¸åŒçš„ç»“æœ
    assert torch.equal(output1, output2), "RainbowDQN (noisy) åœ¨æ²¡æœ‰é‡ç½®å™ªå£°çš„æƒ…å†µä¸‹ï¼Œè¿ç»­è°ƒç”¨è¾“å‡ºåº”è¯¥ç›¸åŒ"
    
    model_noisy.sample_noise() # Correct method name
    output3 = model_noisy(x)
    assert not torch.equal(output1, output3), "é‡ç½®å™ªå£°åè¾“å‡ºåº”è¯¥ä¸åŒ"
    
    print("âœ… Rainbow DQN ç½‘ç»œæµ‹è¯•é€šè¿‡")

def test_n_step_buffer():
    """æµ‹è¯• N æ­¥ç¼“å†²åŒº"""
    print("æµ‹è¯• N æ­¥ç¼“å†²åŒº...")
    
    buffer = NStepBuffer(n_step=3, gamma=0.99)
    
    # æ·»åŠ ç»éªŒ
    states = [np.random.randn(4, 84, 84) for _ in range(5)]
    actions = [0, 1, 2, 0, 1]
    rewards = [1.0, 2.0, -1.0, 0.5, 1.5]
    dones = [False, False, False, False, True]
    
    experiences = []
    for i in range(5):
        next_state = states[i+1] if i < 4 else states[i]
        exp = buffer.add(states[i], actions[i], rewards[i], next_state, dones[i])
        if exp is not None:
            experiences.append(exp)
    
    # æ¸¸æˆç»“æŸï¼Œè·å–å‰©ä½™ç»éªŒ
    remaining = buffer.get_last_n_step()
    experiences.extend(remaining)
    
    # éªŒè¯ n æ­¥å›æŠ¥è®¡ç®—
    assert len(experiences) > 0, "åº”è¯¥æœ‰ N æ­¥ç»éªŒ"
    
    # éªŒè¯ç¬¬ä¸€ä¸ª 3 æ­¥ç»éªŒçš„å›æŠ¥
    expected_reward = rewards[0] + 0.99 * rewards[1] + 0.99**2 * rewards[2]
    assert abs(experiences[0][2] - expected_reward) < 1e-6, f"Næ­¥å›æŠ¥è®¡ç®—é”™è¯¯: {experiences[0][2]} vs {expected_reward}"
    
    print("âœ… N æ­¥ç¼“å†²åŒºæµ‹è¯•é€šè¿‡")

def test_rainbow_agent():
    """æµ‹è¯• Rainbow æ™ºèƒ½ä½“"""
    print("æµ‹è¯• Rainbow æ™ºèƒ½ä½“...")
    
    # åˆ›å»ºç®€å•ç¯å¢ƒ
    try:
        env = make_env("ALE/Assault-v5")
        input_shape = env.observation_space.shape
        n_actions = env.action_space.n
    except:
        print("âš ï¸  æ— æ³•åˆ›å»º Atari ç¯å¢ƒï¼Œä½¿ç”¨æ¨¡æ‹Ÿå‚æ•°")
        input_shape = (4, 84, 84)
        n_actions = 6
        env = None
    
    # åˆ›å»ºæ¨¡å‹
    model = RainbowDQN(
        input_shape=input_shape,
        n_actions=n_actions,
        use_noisy=True,
        use_distributional=True,
        n_atoms=51
    )
    
    target_model = RainbowDQN(
        input_shape=input_shape,
        n_actions=n_actions,
        use_noisy=True,
        use_distributional=True,
        n_atoms=51
    )
    
    device = torch.device("cpu")
    
    # åˆ›å»º Rainbow æ™ºèƒ½ä½“
    agent = RainbowAgent(
        model=model,
        target_model=target_model,
        env=env,
        device=device,
        n_step=3,
        use_noisy=True,
        use_distributional=True,
        batch_size=4,  # å°æ‰¹é‡ä¾¿äºæµ‹è¯•
        buffer_size=1000
    )
    
    # æµ‹è¯•åŠ¨ä½œé€‰æ‹©
    state = np.random.randn(*input_shape)
    action = agent.select_action(state)
    assert 0 <= action < n_actions, f"åŠ¨ä½œè¶…å‡ºèŒƒå›´: {action}"
    
    # æµ‹è¯•ç»éªŒå­˜å‚¨
    next_state = np.random.randn(*input_shape)
    agent.store_experience(state, action, 1.0, next_state, False)
    
    # æ·»åŠ æ›´å¤šç»éªŒä»¥ä¾¿æµ‹è¯•æ›´æ–°
    for _ in range(20):
        state = np.random.randn(*input_shape)
        action = np.random.randint(n_actions)
        reward = np.random.randn()
        next_state = np.random.randn(*input_shape)
        done = np.random.random() < 0.1
        agent.store_experience(state, action, reward, next_state, done)
    
    # æµ‹è¯•æ¨¡å‹æ›´æ–°
    if len(agent.memory) >= agent.batch_size:
        loss = agent.update_model()
        assert loss >= 0, f"æŸå¤±åº”è¯¥éè´Ÿ: {loss}"
        print(f"  æ¨¡å‹æ›´æ–°æŸå¤±: {loss:.6f}")
    
    print("âœ… Rainbow æ™ºèƒ½ä½“æµ‹è¯•é€šè¿‡")

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("=" * 50)
    print("Rainbow DQN å®ç°æµ‹è¯•")
    print("=" * 50)
    
    try:
        test_noisy_linear()
        test_rainbow_dqn()
        test_n_step_buffer()
        test_rainbow_agent()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Rainbow DQN å®ç°æ­£ç¡®ï¼")
        print("=" * 50)
        
        print("\nä½¿ç”¨ç¤ºä¾‹:")
        print("python run_training.py rainbow 100 --use_noisy --use_distributional")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()