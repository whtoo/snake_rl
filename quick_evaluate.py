#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿæ¨¡å‹è¯„ä¼°è„šæœ¬

è¿™ä¸ªè„šæœ¬æä¾›äº†ä¸€ä¸ªç®€å•çš„æ¥å£æ¥å¿«é€Ÿè¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚
ä½¿ç”¨æ–¹æ³•:
    python quick_evaluate.py models/best_model.pth
    python quick_evaluate.py models/rainbow_model.pth --model rainbow
"""

import sys
import os
import argparse
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

from src.evaluate import evaluate_agent, parse_args


def quick_evaluate(model_path, model_type="dqn", episodes=20, render=False):
    """
    å¿«é€Ÿè¯„ä¼°æ¨¡å‹
    
    å‚æ•°:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        model_type: æ¨¡å‹ç±»å‹ ("dqn", "dueling", "rainbow")
        episodes: è¯„ä¼°å›åˆæ•°
        render: æ˜¯å¦æ˜¾ç¤ºæ¸¸æˆç”»é¢
    """
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯: æ¨¡å‹æ–‡ä»¶ {model_path} ä¸å­˜åœ¨!")
        return None
    
    print(f"ğŸ® å¼€å§‹è¯„ä¼°æ¨¡å‹: {model_path}")
    print(f"ğŸ“Š æ¨¡å‹ç±»å‹: {model_type}")
    print(f"ğŸ”¢ è¯„ä¼°å›åˆæ•°: {episodes}")
    print("="*50)
    
    # æ„é€ å‚æ•°
    class Args:
        def __init__(self):
            self.env = "ALE/Assault-v5"
            self.model = model_type
            self.model_path = model_path
            self.n_episodes = episodes
            self.max_steps = 10000
            self.render = render
            self.record_video = False
            self.video_path = "videos"
            self.seed = 42
    
    args = Args()
    
    try:
        # æ‰§è¡Œè¯„ä¼°
        episode_rewards, episode_lengths = evaluate_agent(args)
        
        # è®¡ç®—é¢å¤–ç»Ÿè®¡ä¿¡æ¯
        import numpy as np
        
        success_rate = len([r for r in episode_rewards if r > 0]) / len(episode_rewards)
        median_reward = np.median(episode_rewards)
        
        print("\nğŸ¯ é¢å¤–ç»Ÿè®¡ä¿¡æ¯:")
        print(f"æˆåŠŸç‡ (å¥–åŠ±>0): {success_rate:.1%}")
        print(f"ä¸­ä½æ•°å¥–åŠ±: {median_reward:.2f}")
        print(f"å¥–åŠ±èŒƒå›´: {min(episode_rewards):.2f} ~ {max(episode_rewards):.2f}")
        
        # æ€§èƒ½è¯„çº§
        mean_reward = np.mean(episode_rewards)
        if mean_reward > 1000:
            grade = "ğŸ† ä¼˜ç§€"
        elif mean_reward > 500:
            grade = "ğŸ¥ˆ è‰¯å¥½"
        elif mean_reward > 100:
            grade = "ğŸ¥‰ ä¸€èˆ¬"
        else:
            grade = "ğŸ“ˆ éœ€è¦æ”¹è¿›"
        
        print(f"\nğŸ“ˆ æ€§èƒ½è¯„çº§: {grade}")
        
        return {
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths,
            'mean_reward': mean_reward,
            'success_rate': success_rate,
            'grade': grade
        }
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return None


def auto_detect_model_type(model_path):
    """
    æ ¹æ®æ–‡ä»¶åè‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹
    
    å‚æ•°:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        æ¨¡å‹ç±»å‹å­—ç¬¦ä¸²
    """
    filename = os.path.basename(model_path).lower()
    
    if 'rainbow' in filename:
        return 'rainbow'
    elif 'dueling' in filename:
        return 'dueling'
    else:
        return 'dqn'


def main():
    parser = argparse.ArgumentParser(description="å¿«é€Ÿæ¨¡å‹è¯„ä¼°å·¥å…·")
    parser.add_argument("model_path", help="æ¨¡å‹æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--model", type=str, 
                       choices=["dqn", "dueling", "rainbow"],
                       help="æ¨¡å‹ç±»å‹ (å¦‚æœä¸æŒ‡å®šï¼Œå°†è‡ªåŠ¨æ£€æµ‹)")
    parser.add_argument("--episodes", type=int, default=20, 
                       help="è¯„ä¼°å›åˆæ•° (é»˜è®¤: 20)")
    parser.add_argument("--render", action="store_true", 
                       help="æ˜¾ç¤ºæ¸¸æˆç”»é¢")
    parser.add_argument("--compare", nargs="+", 
                       help="å¯¹æ¯”å¤šä¸ªæ¨¡å‹ (æä¾›å¤šä¸ªæ¨¡å‹è·¯å¾„)")
    
    args = parser.parse_args()
    
    if args.compare:
        # å¯¹æ¯”æ¨¡å¼
        print("ğŸ”„ å¯¹æ¯”æ¨¡å¼: è¯„ä¼°å¤šä¸ªæ¨¡å‹")
        results = []
        
        all_models = [args.model_path] + args.compare
        for model_path in all_models:
            model_type = args.model or auto_detect_model_type(model_path)
            result = quick_evaluate(model_path, model_type, args.episodes, args.render)
            if result:
                result['model_path'] = model_path
                result['model_type'] = model_type
                results.append(result)
        
        # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
        if results:
            print("\n" + "="*60)
            print("ğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœ")
            print("="*60)
            
            # æŒ‰å¹³å‡å¥–åŠ±æ’åº
            results.sort(key=lambda x: x['mean_reward'], reverse=True)
            
            for i, result in enumerate(results, 1):
                model_name = os.path.basename(result['model_path'])
                print(f"{i}. {model_name} ({result['model_type']})")
                print(f"   å¹³å‡å¥–åŠ±: {result['mean_reward']:.2f}")
                print(f"   æˆåŠŸç‡: {result['success_rate']:.1%}")
                print(f"   è¯„çº§: {result['grade']}")
                print()
    
    else:
        # å•æ¨¡å‹è¯„ä¼°
        model_type = args.model or auto_detect_model_type(args.model_path)
        quick_evaluate(args.model_path, model_type, args.episodes, args.render)


if __name__ == "__main__":
    main()