{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习 Atari 游戏 Demo\n",
    "\n",
    "本笔记本演示如何使用 DQN 算法训练智能体玩 Atari 游戏 Assault-v5。\n",
    "\n",
    "## 目录\n",
    "1. [环境设置](#环境设置)\n",
    "2. [数据预处理](#数据预处理)\n",
    "3. [模型架构](#模型架构)\n",
    "4. [训练过程](#训练过程)\n",
    "5. [结果可视化](#结果可视化)\n",
    "6. [模型评估](#模型评估)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from IPython.display import HTML, display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 导入自定义模块\n",
    "from model import DQN, DuelingDQN\n",
    "from agent import DQNAgent\n",
    "from utils import make_env, plot_rewards, record_video\n",
    "\n",
    "# 设置matplotlib中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"环境设置完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境设置\n",
    "\n",
    "首先创建游戏环境并查看其基本信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建环境\n",
    "env_name = 'ALE/Assault-v5'\n",
    "env = make_env(env_name)\n",
    "\n",
    "print(f\"环境名称: {env_name}\")\n",
    "print(f\"观测空间: {env.observation_space}\")\n",
    "print(f\"动作空间: {env.action_space}\")\n",
    "print(f\"动作数量: {env.action_space.n}\")\n",
    "\n",
    "# 重置环境并显示初始状态\n",
    "state, _ = env.reset()\n",
    "print(f\"状态形状: {state.shape}\")\n",
    "print(f\"状态数据类型: {state.dtype}\")\n",
    "print(f\"状态值范围: [{state.min()}, {state.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "查看预处理后的游戏画面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示预处理后的状态\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for i in range(4):\n",
    "    axes[i].imshow(state[i], cmap='gray')\n",
    "    axes[i].set_title(f'帧 {i+1}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('预处理后的游戏状态（4帧堆叠）')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型架构\n",
    "\n",
    "创建并查看 DQN 模型的架构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'使用设备: {device}')\n",
    "\n",
    "# 创建模型\n",
    "input_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# 创建标准DQN模型\n",
    "dqn_model = DQN(input_shape, n_actions)\n",
    "print(\"标准DQN模型:\")\n",
    "print(dqn_model)\n",
    "\n",
    "# 创建Dueling DQN模型\n",
    "dueling_model = DuelingDQN(input_shape, n_actions)\n",
    "print(\"\\nDueling DQN模型:\")\n",
    "print(dueling_model)\n",
    "\n",
    "# 计算模型参数数量\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n标准DQN参数数量: {count_parameters(dqn_model):,}\")\n",
    "print(f\"Dueling DQN参数数量: {count_parameters(dueling_model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练过程\n",
    "\n",
    "演示如何训练 DQN 智能体（简化版本，用于演示）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建智能体\n",
    "model = DQN(input_shape, n_actions)\n",
    "target_model = DQN(input_shape, n_actions)\n",
    "\n",
    "agent = DQNAgent(\n",
    "    model=model,\n",
    "    target_model=target_model,\n",
    "    env=env,\n",
    "    device=device,\n",
    "    buffer_size=10000,  # 较小的缓冲区用于演示\n",
    "    batch_size=32,\n",
    "    gamma=0.99,\n",
    "    lr=1e-4,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_final=0.1,\n",
    "    epsilon_decay=1000,\n",
    "    target_update=100\n",
    ")\n",
    "\n",
    "print(\"智能体创建完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简化的训练循环（仅用于演示）\n",
    "def train_demo(agent, env, n_episodes=50):\n",
    "    rewards = []\n",
    "    losses = []\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes), desc=\"训练中\"):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = 0\n",
    "        steps = 0\n",
    "        \n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not (done or truncated) and steps < 1000:  # 限制步数\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            \n",
    "            agent.memory.push(state, action, reward, next_state, done)\n",
    "            \n",
    "            loss = agent.update_model()\n",
    "            if loss is not None:\n",
    "                episode_loss += loss\n",
    "            \n",
    "            if agent.steps_done % agent.target_update == 0:\n",
    "                agent.update_target_model()\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "        losses.append(episode_loss / steps if steps > 0 else 0)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = np.mean(rewards[-10:])\n",
    "            print(f\"Episode {episode}: Avg Reward = {avg_reward:.2f}\")\n",
    "    \n",
    "    return rewards, losses\n",
    "\n",
    "# 运行演示训练\n",
    "print(\"开始演示训练...（这可能需要几分钟）\")\n",
    "rewards, losses = train_demo(agent, env, n_episodes=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果可视化\n",
    "\n",
    "可视化训练过程中的奖励和损失变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练结果\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# 奖励曲线\n",
    "ax1.plot(rewards, alpha=0.7, label='回合奖励')\n",
    "# 计算移动平均\n",
    "window_size = 10\n",
    "if len(rewards) >= window_size:\n",
    "    moving_avg = [np.mean(rewards[i:i+window_size]) for i in range(len(rewards)-window_size+1)]\n",
    "    ax1.plot(range(window_size-1, len(rewards)), moving_avg, 'r-', linewidth=2, label=f'{window_size}回合移动平均')\n",
    "\n",
    "ax1.set_xlabel('回合')\n",
    "ax1.set_ylabel('奖励')\n",
    "ax1.set_title('训练奖励曲线')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 损失曲线\n",
    "ax2.plot(losses, 'g-', alpha=0.7)\n",
    "ax2.set_xlabel('回合')\n",
    "ax2.set_ylabel('平均损失')\n",
    "ax2.set_title('训练损失曲线')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 打印统计信息\n",
    "print(f\"平均奖励: {np.mean(rewards):.2f}\")\n",
    "print(f\"最高奖励: {np.max(rewards):.2f}\")\n",
    "print(f\"最低奖励: {np.min(rewards):.2f}\")\n",
    "print(f\"奖励标准差: {np.std(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估\n",
    "\n",
    "评估训练后的智能体性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估智能体\n",
    "def evaluate_agent(agent, env, n_episodes=5):\n",
    "    eval_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        done = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not (done or truncated) and steps < 2000:\n",
    "            action = agent.select_action(state, evaluate=True)  # 评估模式\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        eval_rewards.append(episode_reward)\n",
    "        print(f\"评估回合 {episode+1}: 奖励 = {episode_reward:.2f}, 步数 = {steps}\")\n",
    "    \n",
    "    return eval_rewards\n",
    "\n",
    "# 运行评估\n",
    "print(\"开始评估智能体...\")\n",
    "eval_rewards = evaluate_agent(agent, env, n_episodes=5)\n",
    "\n",
    "print(f\"\\n评估结果:\")\n",
    "print(f\"平均奖励: {np.mean(eval_rewards):.2f} ± {np.std(eval_rewards):.2f}\")\n",
    "print(f\"最高奖励: {np.max(eval_rewards):.2f}\")\n",
    "print(f\"最低奖励: {np.min(eval_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本演示展示了如何使用 DQN 算法训练智能体玩 Atari 游戏。主要步骤包括:\n",
    "\n",
    "1. **环境预处理**: 将原始游戏画面转换为适合神经网络处理的格式\n",
    "2. **模型设计**: 使用卷积神经网络提取视觉特征\n",
    "3. **经验回放**: 存储和重用历史经验以提高学习效率\n",
    "4. **目标网络**: 使用固定的目标网络稳定训练过程\n",
    "5. **探索策略**: 使用ε-贪婪策略平衡探索和利用\n",
    "\n",
    "要获得更好的性能，建议:\n",
    "- 增加训练回合数\n",
    "- 使用更大的经验回放缓冲区\n",
    "- 尝试不同的网络架构（如 Dueling DQN）\n",
    "- 使用优先经验回放\n",
    "- 调整超参数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}