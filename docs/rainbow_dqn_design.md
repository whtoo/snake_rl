# Rainbow DQN å®ç°æ–‡æ¡£

## é¡¹ç›®æ¦‚è¿°

åŸºäºç°æœ‰çš„ Dueling DQN æ¶æ„ï¼ŒæˆåŠŸå®ç°äº†å®Œæ•´çš„ Rainbow DQNï¼Œé›†æˆäº†æ‰€æœ‰ 6 ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œæ˜¾è‘—æå‡äº†å¼ºåŒ–å­¦ä¹ æ€§èƒ½ã€‚

### å®ç°ç›®æ ‡ âœ…
- âœ… åœ¨ç°æœ‰ Dueling DQN åŸºç¡€ä¸Šæ‰©å±•ï¼Œä¿æŒå®Œå…¨å‘åå…¼å®¹
- âœ… å®ç° Rainbow DQN çš„ 6 ä¸ªæ ¸å¿ƒç»„ä»¶
- âœ… é‡ç‚¹å…³æ³¨æ€§èƒ½æå‡æœ€æ˜¾è‘—çš„ç»„ä»¶
- âœ… ä¿æŒä»£ç çš„å¯è¯»æ€§å’Œç»´æŠ¤æ€§

## æ¶æ„å®ç°çŠ¶æ€

### å·²å®Œæˆçš„æ‰€æœ‰ç»„ä»¶ âœ…
1. **Double DQN** - åœ¨ [`agent.py:264-267`](src/agent.py:264) å®ç°
2. **Dueling DQN** - åœ¨ [`model.py:70-117`](src/model.py:70) å®ç°
3. **Prioritized Experience Replay** - åœ¨ [`agent.py:59-149`](src/agent.py:59) å®ç°
4. **Multi-step Learning** - åœ¨ [`agent.py:324-467`](src/agent.py:324) å®ç° `NStepBuffer` ç±»
5. **Noisy Networks** - åœ¨ [`model.py:118-210`](src/model.py:118) å®ç° `NoisyLinear` ç±»
6. **Distributional DQN** - åœ¨ [`model.py:211-305`](src/model.py:211) å’Œ [`agent.py:470-781`](src/agent.py:470) å®Œæ•´å®ç°

## Rainbow DQN 6ä¸ªç»„ä»¶è¯¦è§£

### 1. Double DQN âœ…
**ä½œç”¨**: è§£å†³ Q å€¼è¿‡ä¼°è®¡é—®é¢˜
**å®ç°**: ä½¿ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œï¼Œç›®æ ‡ç½‘ç»œè¯„ä¼°åŠ¨ä½œä»·å€¼
```python
# ç°æœ‰å®ç°ä½ç½®: agent.py:264-267
next_q_values = self.model(next_states)
next_actions = next_q_values.max(1)[1].unsqueeze(1)
next_q_values_target = self.target_model(next_states).gather(1, next_actions)
```

### 2. Dueling DQN âœ…
**ä½œç”¨**: åˆ†ç¦»çŠ¶æ€ä»·å€¼å’ŒåŠ¨ä½œä¼˜åŠ¿
**å®ç°**: Q(s,a) = V(s) + (A(s,a) - mean(A(s,a')))
```python
# ç°æœ‰å®ç°ä½ç½®: model.py:113-117
value = self.value_stream(conv_out)
advantage = self.advantage_stream(conv_out)
return value + advantage - advantage.mean(dim=1, keepdim=True)
```

### 3. Prioritized Experience Replay âœ…
**ä½œç”¨**: æ ¹æ® TD è¯¯å·®ä¼˜å…ˆé‡‡æ ·é‡è¦ç»éªŒ
**å®ç°**: åŸºäºä¼˜å…ˆçº§çš„é‡‡æ ·å’Œé‡è¦æ€§æƒé‡
```python
# ç°æœ‰å®ç°ä½ç½®: agent.py:103-136
probs = prios ** self.alpha
indices = np.random.choice(len(self.buffer), batch_size, p=probs)
weights = (len(self.buffer) * probs[indices]) ** (-beta)
```

### 4. Multi-step Learning âœ…
**ä½œç”¨**: ä½¿ç”¨ n æ­¥å›æŠ¥å‡å°‘åå·®
**å…¬å¼**: R_t^(n) = r_t + Î³r_{t+1} + ... + Î³^{n-1}r_{t+n-1} + Î³^n Q(s_{t+n}, a_{t+n})
**å®ç°**: åœ¨ [`agent.py:324-467`](src/agent.py:324) å®ç° `NStepBuffer` ç±»
```python
# å®ç°ä½ç½®: agent.py:324-467
class NStepBuffer:
    def __init__(self, n_step=3, gamma=0.99):
        self.n_step = n_step
        self.gamma = gamma
        self.buffer = deque(maxlen=n_step)
```

### 5. Noisy Networks âœ…
**ä½œç”¨**: ç”¨å¯å­¦ä¹ çš„å™ªå£°æ›¿ä»£ Îµ-è´ªå¿ƒæ¢ç´¢
**å®ç°**: W = Î¼_W + Ïƒ_W âŠ™ Îµ_W, b = Î¼_b + Ïƒ_b âŠ™ Îµ_b
**å®ç°**: åœ¨ [`model.py:118-210`](src/model.py:118) å®ç° `NoisyLinear` ç±»
```python
# å®ç°ä½ç½®: model.py:118-210
class NoisyLinear(nn.Module):
    def __init__(self, in_features, out_features, sigma_init=0.4):
        # æƒé‡å‚æ•°ï¼šå‡å€¼å’Œæ ‡å‡†å·®
        self.mu_weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        self.sigma_weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
```

### 6. Distributional DQN âœ…
**ä½œç”¨**: å­¦ä¹ ä»·å€¼åˆ†å¸ƒè€ŒéæœŸæœ›å€¼
**å®ç°**: ä½¿ç”¨ C51 ç®—æ³•å®ç°åˆ†å¸ƒå¼ Q å­¦ä¹ 
**å®ç°**: åœ¨ [`model.py:211-305`](src/model.py:211) å’Œ [`agent.py:470-781`](src/agent.py:470) å®Œæ•´å®ç°
```python
# å®ç°ä½ç½®: model.py:211-305, agent.py:470-781
class RainbowDQN(nn.Module):
    def __init__(self, input_shape, n_actions, n_atoms=51, v_min=-10, v_max=10):
        # åˆ†å¸ƒå¼ Q å­¦ä¹ å‚æ•°
        self.n_atoms = n_atoms
        self.v_min = v_min
        self.v_max = v_max
```

## æŠ€æœ¯æ¶æ„è®¾è®¡

### æ¶æ„æµç¨‹å›¾
```mermaid
graph TB
    subgraph "è¾“å…¥å±‚"
        A[æ¸¸æˆçŠ¶æ€] --> B[CNNç‰¹å¾æå–]
    end
    
    subgraph "Rainbowç½‘ç»œå±‚"
        B --> C[NoisyLinearå±‚]
        C --> D[Duelingæ¶æ„]
        D --> E[ä»·å€¼æµV(s)]
        D --> F[ä¼˜åŠ¿æµA(s,a)]
        E --> G[Qå€¼åˆå¹¶]
        F --> G
        G --> H[åˆ†å¸ƒå¼è¾“å‡º/æ ‡é‡è¾“å‡º]
    end
    
    subgraph "å­¦ä¹ ç®—æ³•"
        I[Næ­¥ç»éªŒç¼“å†²] --> J[ä¼˜å…ˆç»éªŒå›æ”¾]
        J --> K[Double DQNç›®æ ‡]
        K --> L[åˆ†å¸ƒå¼æŸå¤±/MSEæŸå¤±]
    end
    
    H --> I
    L --> M[ç½‘ç»œæ›´æ–°]
    M --> C
```

### ç»„ä»¶é›†æˆç­–ç•¥
```mermaid
graph LR
    subgraph "ä¼˜å…ˆçº§1: æ ¸å¿ƒå¢å¼º"
        A[Multi-step Learning] --> B[æ˜¾è‘—å‡å°‘åå·®]
        C[Noisy Networks] --> D[æ›´å¥½çš„æ¢ç´¢ç­–ç•¥]
    end
    
    subgraph "ä¼˜å…ˆçº§2: é«˜çº§åŠŸèƒ½"
        E[Distributional DQN] --> F[æ›´å‡†ç¡®çš„ä»·å€¼ä¼°è®¡]
    end
    
    subgraph "é›†æˆæ•ˆæœ"
        B --> G[æ€§èƒ½æå‡]
        D --> G
        F --> G
        G --> H[Rainbow DQN]
    end
```

## è¯¦ç»†å®ç°æ–¹æ¡ˆ

### 1. æ–‡ä»¶ç»“æ„æ‰©å±•

```
src/
â”œâ”€â”€ model.py                  # æ‰©å±•ç°æœ‰æ¨¡å‹
â”‚   â”œâ”€â”€ DQN                   # ç°æœ‰
â”‚   â”œâ”€â”€ DuelingDQN           # ç°æœ‰  
â”‚   â”œâ”€â”€ NoisyLinear          # æ–°å¢ - å™ªå£°çº¿æ€§å±‚
â”‚   â”œâ”€â”€ RainbowDQN           # æ–°å¢ - Rainbowç½‘ç»œ
â”‚   â””â”€â”€ DistributionalHead   # æ–°å¢ - åˆ†å¸ƒå¼è¾“å‡ºå¤´
â”‚
â”œâ”€â”€ agent.py                 # æ‰©å±•ç°æœ‰æ™ºèƒ½ä½“
â”‚   â”œâ”€â”€ ReplayBuffer         # ç°æœ‰
â”‚   â”œâ”€â”€ PrioritizedReplayBuffer # ç°æœ‰
â”‚   â”œâ”€â”€ DQNAgent             # ç°æœ‰
â”‚   â”œâ”€â”€ NStepBuffer          # æ–°å¢ - Næ­¥ç¼“å†²åŒº
â”‚   â””â”€â”€ RainbowAgent         # æ–°å¢ - Rainbowæ™ºèƒ½ä½“
â”‚
â”œâ”€â”€ rainbow_utils.py         # æ–°å¢ - Rainbowå·¥å…·æ¨¡å—
â”‚   â”œâ”€â”€ NoisyUtils           # å™ªå£°ç½‘ç»œå·¥å…·
â”‚   â”œâ”€â”€ DistributionalUtils  # åˆ†å¸ƒå¼DQNå·¥å…·
â”‚   â””â”€â”€ MultiStepUtils       # å¤šæ­¥å­¦ä¹ å·¥å…·
â”‚
â””â”€â”€ train.py                 # ä¿®æ”¹è®­ç»ƒè„šæœ¬
    â””â”€â”€ æ·»åŠ  --model rainbow é€‰é¡¹
```

### 2. æ ¸å¿ƒç±»è®¾è®¡

#### 2.1 NoisyLinear å±‚
```python
class NoisyLinear(nn.Module):
    """
    å™ªå£°çº¿æ€§å±‚ - æ›¿ä»£ä¼ ç»Ÿçš„ epsilon-greedy æ¢ç´¢
    
    å‚æ•°:
        in_features: è¾“å…¥ç‰¹å¾æ•°
        out_features: è¾“å‡ºç‰¹å¾æ•°  
        sigma_init: å™ªå£°åˆå§‹åŒ–æ ‡å‡†å·®
        factorised: æ˜¯å¦ä½¿ç”¨å› å­åŒ–å™ªå£°
    """
    def __init__(self, in_features, out_features, sigma_init=0.4, factorised=True):
        super(NoisyLinear, self).__init__()
        
        # æƒé‡å‚æ•°ï¼šå‡å€¼å’Œæ ‡å‡†å·®
        self.mu_weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        self.sigma_weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        
        # åç½®å‚æ•°ï¼šå‡å€¼å’Œæ ‡å‡†å·®
        self.mu_bias = nn.Parameter(torch.FloatTensor(out_features))
        self.sigma_bias = nn.Parameter(torch.FloatTensor(out_features))
        
        # å™ªå£°ç¼“å­˜
        self.register_buffer('epsilon_weight', torch.FloatTensor(out_features, in_features))
        self.register_buffer('epsilon_bias', torch.FloatTensor(out_features))
        
        self.sigma_init = sigma_init
        self.factorised = factorised
        self.reset_parameters()
    
    def reset_parameters(self):
        """åˆå§‹åŒ–å‚æ•°"""
        mu_range = 1 / math.sqrt(self.mu_weight.size(1))
        self.mu_weight.data.uniform_(-mu_range, mu_range)
        self.mu_bias.data.uniform_(-mu_range, mu_range)
        
        self.sigma_weight.data.fill_(self.sigma_init / math.sqrt(self.sigma_weight.size(1)))
        self.sigma_bias.data.fill_(self.sigma_init / math.sqrt(self.sigma_bias.size(0)))
    
    def sample_noise(self):
        """é‡‡æ ·å™ªå£°"""
        if self.factorised:
            # å› å­åŒ–å™ªå£°ï¼šå‡å°‘å‚æ•°æ•°é‡
            epsilon_in = self._scale_noise(self.mu_weight.size(1))
            epsilon_out = self._scale_noise(self.mu_weight.size(0))
            self.epsilon_weight.copy_(epsilon_out.ger(epsilon_in))
            self.epsilon_bias.copy_(epsilon_out)
        else:
            # ç‹¬ç«‹å™ªå£°
            self.epsilon_weight.normal_()
            self.epsilon_bias.normal_()
    
    def _scale_noise(self, size):
        """ç¼©æ”¾å™ªå£°"""
        x = torch.randn(size)
        return x.sign().mul_(x.abs().sqrt_())
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        if self.training:
            # è®­ç»ƒæ—¶ä½¿ç”¨å™ªå£°
            self.sample_noise()
            weight = self.mu_weight + self.sigma_weight * self.epsilon_weight
            bias = self.mu_bias + self.sigma_bias * self.epsilon_bias
        else:
            # è¯„ä¼°æ—¶ä½¿ç”¨å‡å€¼
            weight = self.mu_weight
            bias = self.mu_bias
        
        return F.linear(x, weight, bias)
```

#### 2.2 RainbowDQN ç½‘ç»œ
```python
class RainbowDQN(nn.Module):
    """
    Rainbow DQN ç½‘ç»œæ¶æ„
    é›†æˆ Dueling + Noisy Networks + Distributional DQN
    """
    def __init__(self, input_shape, n_actions, n_atoms=51, v_min=-10, v_max=10, 
                 use_noisy=True, use_distributional=False):
        super(RainbowDQN, self).__init__()
        
        self.n_actions = n_actions
        self.n_atoms = n_atoms
        self.v_min = v_min
        self.v_max = v_max
        self.use_noisy = use_noisy
        self.use_distributional = use_distributional
        
        # å·ç§¯ç‰¹å¾æå–å±‚ï¼ˆå¤ç”¨ç°æœ‰è®¾è®¡ï¼‰
        self.conv = nn.Sequential(
            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=4, stride=2),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1),
            nn.ReLU()
        )
        
        conv_out_size = self._get_conv_out(input_shape)
        
        # é€‰æ‹©çº¿æ€§å±‚ç±»å‹
        LinearLayer = NoisyLinear if use_noisy else nn.Linear
        
        # Dueling æ¶æ„
        if use_distributional:
            # åˆ†å¸ƒå¼è¾“å‡º
            self.value_stream = nn.Sequential(
                LinearLayer(conv_out_size, 512),
                nn.ReLU(),
                LinearLayer(512, n_atoms)
            )
            self.advantage_stream = nn.Sequential(
                LinearLayer(conv_out_size, 512),
                nn.ReLU(),
                LinearLayer(512, n_actions * n_atoms)
            )
        else:
            # æ ‡é‡è¾“å‡º
            self.value_stream = nn.Sequential(
                LinearLayer(conv_out_size, 512),
                nn.ReLU(),
                LinearLayer(512, 1)
            )
            self.advantage_stream = nn.Sequential(
                LinearLayer(conv_out_size, 512),
                nn.ReLU(),
                LinearLayer(512, n_actions)
            )
    
    def _get_conv_out(self, shape):
        """è®¡ç®—å·ç§¯è¾“å‡ºå°ºå¯¸"""
        o = self.conv(torch.zeros(1, *shape))
        return int(np.prod(o.size()))
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        x = x.float() / 255.0
        conv_out = self.conv(x)
        conv_out = conv_out.view(conv_out.size(0), -1)
        
        value = self.value_stream(conv_out)
        advantage = self.advantage_stream(conv_out)
        
        if self.use_distributional:
            # åˆ†å¸ƒå¼ Dueling
            batch_size = x.size(0)
            value = value.view(batch_size, 1, self.n_atoms)
            advantage = advantage.view(batch_size, self.n_actions, self.n_atoms)
            
            # Dueling æ¶æ„åˆå¹¶
            q_dist = value + advantage - advantage.mean(dim=1, keepdim=True)
            # åº”ç”¨ softmax å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ
            q_dist = F.softmax(q_dist, dim=2)
            
            return q_dist
        else:
            # æ ‡é‡ Dueling
            return value + advantage - advantage.mean(dim=1, keepdim=True)
    
    def sample_noise(self):
        """ä¸ºæ‰€æœ‰å™ªå£°å±‚é‡‡æ ·æ–°çš„å™ªå£°"""
        if self.use_noisy:
            for module in self.modules():
                if isinstance(module, NoisyLinear):
                    module.sample_noise()
```

#### 2.3 Næ­¥å­¦ä¹ ç¼“å†²åŒº
```python
class NStepBuffer:
    """
    Næ­¥å­¦ä¹ ç¼“å†²åŒº
    è®¡ç®— n-step returns ä»¥å‡å°‘åå·®
    """
    def __init__(self, n_step=3, gamma=0.99):
        self.n_step = n_step
        self.gamma = gamma
        self.buffer = deque(maxlen=n_step)
        
    def append(self, state, action, reward, next_state, done):
        """æ·»åŠ ç»éªŒåˆ° n-step ç¼“å†²åŒº"""
        self.buffer.append([state, action, reward, next_state, done])
        
        if len(self.buffer) == self.n_step:
            return self._make_n_step_transition()
        return None
    
    def _make_n_step_transition(self):
        """æ„é€  n-step è½¬ç§»"""
        state, action = self.buffer[0][:2]
        
        # è®¡ç®— n-step return
        n_step_return = 0
        next_state, done = self.buffer[-1][3:]
        
        for i in range(self.n_step):
            reward = self.buffer[i][2]
            n_step_return += (self.gamma ** i) * reward
            
            # å¦‚æœåœ¨ n æ­¥å†…ç»“æŸï¼Œåœæ­¢ç´¯ç§¯
            if self.buffer[i][4]:  # done
                next_state = self.buffer[i][3]
                done = True
                break
        
        return state, action, n_step_return, next_state, done
    
    def reset(self):
        """é‡ç½®ç¼“å†²åŒº"""
        self.buffer.clear()
```

#### 2.4 Rainbowæ™ºèƒ½ä½“
```python
class RainbowAgent(DQNAgent):
    """
    Rainbow DQN æ™ºèƒ½ä½“
    é›†æˆæ‰€æœ‰ Rainbow ç»„ä»¶
    """
    def __init__(self, model, target_model, env, device, 
                 n_step=3, use_noisy=True, use_distributional=False,
                 **kwargs):
        
        # ç»§æ‰¿åŸºç¡€ DQNAgent
        super().__init__(model, target_model, env, device, **kwargs)
        
        self.n_step = n_step
        self.use_noisy = use_noisy
        self.use_distributional = use_distributional
        
        # Næ­¥å­¦ä¹ ç¼“å†²åŒº
        self.n_step_buffer = NStepBuffer(n_step, self.gamma)
        
        # åˆ†å¸ƒå¼DQNå‚æ•°
        if use_distributional:
            self.n_atoms = model.n_atoms
            self.v_min = model.v_min
            self.v_max = model.v_max
            self.delta_z = (self.v_max - self.v_min) / (self.n_atoms - 1)
            self.support = torch.linspace(self.v_min, self.v_max, self.n_atoms).to(device)
    
    def select_action(self, state, evaluate=False):
        """é€‰æ‹©åŠ¨ä½œ"""
        if self.use_noisy:
            # ä½¿ç”¨å™ªå£°ç½‘ç»œï¼Œä¸éœ€è¦ epsilon-greedy
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                
                if self.use_distributional:
                    # åˆ†å¸ƒå¼ï¼šè®¡ç®—æœŸæœ›Qå€¼
                    q_dist = self.model(state_tensor)
                    q_values = (q_dist * self.support).sum(dim=2)
                else:
                    q_values = self.model(state_tensor)
                
                return q_values.max(1)[1].item()
        else:
            # å›é€€åˆ°çˆ¶ç±»çš„ epsilon-greedy
            return super().select_action(state, evaluate)
    
    def store_transition(self, state, action, reward, next_state, done):
        """å­˜å‚¨è½¬ç§»ï¼ˆæ”¯æŒn-stepï¼‰"""
        n_step_transition = self.n_step_buffer.append(state, action, reward, next_state, done)
        
        if n_step_transition is not None:
            # å°† n-step è½¬ç§»å­˜å…¥ç»éªŒå›æ”¾ç¼“å†²åŒº
            self.memory.push(*n_step_transition)
    
    def update_model(self):
        """æ›´æ–°æ¨¡å‹ï¼ˆæ”¯æŒåˆ†å¸ƒå¼æŸå¤±ï¼‰"""
        if len(self.memory) < self.batch_size:
            return 0.0
        
        # é‡‡æ ·ç»éªŒ
        if self.prioritized_replay:
            states, actions, rewards, next_states, dones, indices, weights = self.memory.sample(self.batch_size)
            weights = weights.to(self.device)
        else:
            states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)
            weights = torch.ones(self.batch_size, 1).to(self.device)
        
        # è½¬ç§»åˆ°è®¾å¤‡
        states = states.to(self.device)
        actions = actions.to(self.device)
        rewards = rewards.to(self.device)
        next_states = next_states.to(self.device)
        dones = dones.to(self.device)
        
        if self.use_distributional:
            loss = self._distributional_loss(states, actions, rewards, next_states, dones)
        else:
            loss = self._standard_loss(states, actions, rewards, next_states, dones)
        
        # åº”ç”¨é‡è¦æ€§æƒé‡
        loss = (loss * weights).mean()
        
        # æ›´æ–°ä¼˜å…ˆçº§
        if self.prioritized_replay:
            with torch.no_grad():
                td_errors = self._compute_td_errors(states, actions, rewards, next_states, dones)
                priorities = td_errors.detach().cpu().numpy() + 1e-6
                self.memory.update_priorities(indices, priorities)
        
        # æ¢¯åº¦æ›´æ–°
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 10)
        self.optimizer.step()
        
        # ä¸ºå™ªå£°ç½‘ç»œé‡‡æ ·æ–°å™ªå£°
        if self.use_noisy:
            self.model.sample_noise()
            self.target_model.sample_noise()
        
        return loss.item()
    
    def _distributional_loss(self, states, actions, rewards, next_states, dones):
        """åˆ†å¸ƒå¼DQNæŸå¤±å‡½æ•°"""
        # å½“å‰çŠ¶æ€çš„åˆ†å¸ƒ
        current_dist = self.model(states)
        current_dist = current_dist[range(self.batch_size), actions.squeeze()]
        
        # ç›®æ ‡åˆ†å¸ƒ
        with torch.no_grad():
            # Double DQN: ä½¿ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œ
            next_dist = self.model(next_states)
            next_q_values = (next_dist * self.support).sum(dim=2)
            next_actions = next_q_values.max(1)[1]
            
            # ä½¿ç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°
            next_dist_target = self.target_model(next_states)
            next_dist_target = next_dist_target[range(self.batch_size), next_actions]
            
            # è®¡ç®—ç›®æ ‡åˆ†å¸ƒ
            t_z = rewards + (1 - dones) * (self.gamma ** self.n_step) * self.support
            t_z = t_z.clamp(self.v_min, self.v_max)
            
            # æŠ•å½±åˆ°æ”¯æŒé›†
            b = (t_z - self.v_min) / self.delta_z
            l = b.floor().long()
            u = b.ceil().long()
            
            target_dist = torch.zeros_like(next_dist_target)
            for i in range(self.batch_size):
                for j in range(self.n_atoms):
                    if l[i, j] == u[i, j]:
                        target_dist[i, l[i, j]] += next_dist_target[i, j]
                    else:
                        target_dist[i, l[i, j]] += next_dist_target[i, j] * (u[i, j] - b[i, j])
                        target_dist[i, u[i, j]] += next_dist_target[i, j] * (b[i, j] - l[i, j])
        
        # KLæ•£åº¦æŸå¤±
        loss = -(target_dist * current_dist.log()).sum(dim=1)
        return loss
    
    def _standard_loss(self, states, actions, rewards, next_states, dones):
        """æ ‡å‡†DQNæŸå¤±å‡½æ•°"""
        # å½“å‰Qå€¼
        q_values = self.model(states).gather(1, actions)
        
        # ç›®æ ‡Qå€¼ï¼ˆDouble DQNï¼‰
        with torch.no_grad():
            next_q_values = self.model(next_states)
            next_actions = next_q_values.max(1)[1].unsqueeze(1)
            next_q_values_target = self.target_model(next_states).gather(1, next_actions)
            
            target_q_values = rewards + (1 - dones) * (self.gamma ** self.n_step) * next_q_values_target
        
        # MSEæŸå¤±
        loss = F.mse_loss(q_values, target_q_values, reduction='none').squeeze()
        return loss
```

### 3. è®­ç»ƒè„šæœ¬é›†æˆ

#### ä¿®æ”¹ [`train.py`](src/train.py) 
```python
# åœ¨ parse_args() å‡½æ•°ä¸­æ·»åŠ 
parser.add_argument("--model", type=str, default="dqn", 
                   choices=["dqn", "dueling", "rainbow"], help="æ¨¡å‹ç±»å‹")
parser.add_argument("--n_step", type=int, default=3, help="Næ­¥å­¦ä¹ æ­¥æ•°")
parser.add_argument("--use_noisy", action="store_true", help="ä½¿ç”¨å™ªå£°ç½‘ç»œ")
parser.add_argument("--use_distributional", action="store_true", help="ä½¿ç”¨åˆ†å¸ƒå¼DQN")

# åœ¨ train() å‡½æ•°ä¸­ä¿®æ”¹æ¨¡å‹åˆ›å»º
if args.model == "dqn":
    model = DQN(input_shape, n_actions)
    target_model = DQN(input_shape, n_actions)
    agent_class = DQNAgent
elif args.model == "dueling":
    model = DuelingDQN(input_shape, n_actions)
    target_model = DuelingDQN(input_shape, n_actions)
    agent_class = DQNAgent
elif args.model == "rainbow":
    model = RainbowDQN(input_shape, n_actions, 
                      use_noisy=args.use_noisy,
                      use_distributional=args.use_distributional)
    target_model = RainbowDQN(input_shape, n_actions,
                             use_noisy=args.use_noisy,
                             use_distributional=args.use_distributional)
    agent_class = RainbowAgent

# åˆ›å»ºæ™ºèƒ½ä½“
if args.model == "rainbow":
    agent = RainbowAgent(
        model=model,
        target_model=target_model,
        env=env,
        device=device,
        n_step=args.n_step,
        use_noisy=args.use_noisy,
        use_distributional=args.use_distributional,
        # å…¶ä»–å‚æ•°...
    )
else:
    agent = DQNAgent(model=model, target_model=target_model, ...)
```

## å®ç°æ€»ç»“

### âœ… å·²å®Œæˆçš„æ ¸å¿ƒç»„ä»¶
1. **NoisyLinear å±‚** - åœ¨ [`model.py:118-210`](src/model.py:118) å®ç°
   - âœ… å› å­åŒ–å™ªå£°å’Œç‹¬ç«‹å™ªå£°
   - âœ… å‚æ•°åˆå§‹åŒ–å’Œå™ªå£°é‡‡æ ·
   - âœ… å®Œæ•´çš„å•å…ƒæµ‹è¯•

2. **NStepBuffer ç¼“å†²åŒº** - åœ¨ [`agent.py:324-467`](src/agent.py:324) å®ç°
   - âœ… n-step return è®¡ç®—
   - âœ… ä¸ç°æœ‰ç»éªŒå›æ”¾é›†æˆ
   - âœ… è¾¹ç•Œæƒ…å†µå¤„ç†

3. **RainbowDQN ç½‘ç»œ** - åœ¨ [`model.py:211-305`](src/model.py:211) å®ç°
   - âœ… é›†æˆ Dueling + Noisy æ¶æ„
   - âœ… åˆ†å¸ƒå¼è¾“å‡ºå¤´
   - âœ… å®Œæ•´çš„å‰å‘ä¼ æ’­é€»è¾‘

### âœ… å·²å®Œæˆçš„æ™ºèƒ½ä½“é›†æˆ
4. **RainbowAgent æ™ºèƒ½ä½“** - åœ¨ [`agent.py:470-781`](src/agent.py:470) å®ç°
   - âœ… ç»§æ‰¿ç°æœ‰ DQNAgent
   - âœ… é›†æˆ n-step å­¦ä¹ 
   - âœ… å™ªå£°ç½‘ç»œæ¢ç´¢ç­–ç•¥

5. **æŸå¤±å‡½æ•°å’Œæ›´æ–°é€»è¾‘** - å®Œæ•´å®ç°
   - âœ… æ ‡å‡†MSEæŸå¤±
   - âœ… åˆ†å¸ƒå¼KLæ•£åº¦æŸå¤±
   - âœ… ä¼˜å…ˆçº§æ›´æ–°æœºåˆ¶

6. **è®­ç»ƒè„šæœ¬é›†æˆ** - åœ¨ [`train.py`](src/train.py) å®Œæˆ
   - âœ… å‘½ä»¤è¡Œå‚æ•°æ‰©å±•
   - âœ… æ¨¡å‹åˆ›å»ºé€»è¾‘
   - âœ… å‘åå…¼å®¹æ€§ä¿è¯

### âœ… å·²å®Œæˆçš„æµ‹è¯•å’Œä¼˜åŒ–
7. **å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•** - åœ¨ [`test_rainbow.py`](test_rainbow.py) å’Œ [`tests/`](tests/) å®ç°
   - âœ… å„ç»„ä»¶åŠŸèƒ½æµ‹è¯•
   - âœ… ç«¯åˆ°ç«¯è®­ç»ƒæµ‹è¯•
   - âœ… æ€§èƒ½å¯¹æ¯”æµ‹è¯•

8. **æ–‡æ¡£å’Œä½¿ç”¨æŒ‡å—** - å®Œæ•´æ–‡æ¡£ä½“ç³»
   - âœ… ä»£ç æ–‡æ¡£å®Œå–„
   - âœ… è¶…å‚æ•°é…ç½®
   - âœ… ä½¿ç”¨æŒ‡å—æ›´æ–°

## æµ‹è¯•ç­–ç•¥

### 1. å•å…ƒæµ‹è¯•
```python
# tests/test_rainbow_components.py
class TestNoisyLinear:
    def test_noise_sampling(self):
        # æµ‹è¯•å™ªå£°é‡‡æ ·åŠŸèƒ½
        
    def test_forward_pass(self):
        # æµ‹è¯•å‰å‘ä¼ æ’­
        
    def test_parameter_initialization(self):
        # æµ‹è¯•å‚æ•°åˆå§‹åŒ–

class TestNStepBuffer:
    def test_n_step_calculation(self):
        # æµ‹è¯• n-step return è®¡ç®—
        
    def test_early_termination(self):
        # æµ‹è¯•æå‰ç»“æŸæƒ…å†µ
```

### 2. é›†æˆæµ‹è¯•
```python
# tests/test_rainbow_integration.py
class TestRainbowIntegration:
    def test_rainbow_vs_dueling_dqn(self):
        # å¯¹æ¯” Rainbow DQN å’Œ Dueling DQN æ€§èƒ½
        
    def test_backward_compatibility(self):
        # æµ‹è¯•å‘åå…¼å®¹æ€§
        
    def test_model_saving_loading(self):
        # æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
```

### 3. æ€§èƒ½æµ‹è¯•
```python
# æµ‹è¯•ç¯å¢ƒ: Atari Assault-v5
# åŸºå‡†: ç°æœ‰ Dueling DQN
# æŒ‡æ ‡: 
# - å¹³å‡å¥–åŠ±æå‡
# - è®­ç»ƒæ”¶æ•›é€Ÿåº¦
# - å†…å­˜ä½¿ç”¨æ•ˆç‡
# - è®¡ç®—å¼€é”€
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. å†…å­˜ä¼˜åŒ–
- ä½¿ç”¨ `torch.no_grad()` å‡å°‘å†…å­˜å ç”¨
- å®ç°ç»éªŒå›æ”¾ç¼“å†²åŒºçš„å†…å­˜æ˜ å°„
- ä¼˜åŒ–åˆ†å¸ƒå¼DQNçš„å†…å­˜ä½¿ç”¨

### 2. è®¡ç®—ä¼˜åŒ–
- æ‰¹é‡åŒ–å™ªå£°é‡‡æ ·
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
- GPUå†…å­˜é¢„åˆ†é…

### 3. è¶…å‚æ•°è°ƒä¼˜
```python
RAINBOW_HYPERPARAMS = {
    "n_step": 3,           # å¤šæ­¥å­¦ä¹ æ­¥æ•°
    "noisy_sigma": 0.4,    # å™ªå£°ç½‘ç»œåˆå§‹æ ‡å‡†å·®
    "n_atoms": 51,         # åˆ†å¸ƒå¼DQNåŸå­æ•°
    "v_min": -10,          # ä»·å€¼åˆ†å¸ƒä¸‹ç•Œ
    "v_max": 10,           # ä»·å€¼åˆ†å¸ƒä¸Šç•Œ
}
```

## å‘åå…¼å®¹æ€§ä¿è¯

### 1. ç°æœ‰æ¨¡å‹ç»§ç»­å·¥ä½œ
- æ‰€æœ‰ç°æœ‰çš„ `--model dqn` å’Œ `--model dueling` å‚æ•°ä¸å˜
- ç°æœ‰çš„è®­ç»ƒè„šæœ¬å’Œé…ç½®æ–‡ä»¶æ— éœ€ä¿®æ”¹
- å·²è®­ç»ƒçš„æ¨¡å‹å¯ä»¥æ­£å¸¸åŠ è½½å’Œè¯„ä¼°

### 2. æ¸è¿›å¼é‡‡ç”¨
- ç”¨æˆ·å¯ä»¥é€æ­¥å¯ç”¨ Rainbow ç»„ä»¶
- `--model rainbow` å¼€å¯åŸºç¡€ Rainbow DQN
- `--use_noisy` å¯ç”¨å™ªå£°ç½‘ç»œ
- `--use_distributional` å¯ç”¨åˆ†å¸ƒå¼DQN

### 3. é…ç½®è¿ç§»
```bash
# ç°æœ‰ç”¨æ³•ï¼ˆç»§ç»­æ”¯æŒï¼‰
python src/train.py --model dueling --prioritized_replay

# æ–°çš„Rainbowç”¨æ³•
python src/train.py --model rainbow --use_noisy
python src/train.py --model rainbow --use_noisy --use_distributional
```

## ä½¿ç”¨æŒ‡å—

### 1. åŸºç¡€Rainbow DQN
```bash
# ä½¿ç”¨ Multi-step Learning + ç°æœ‰ç»„ä»¶
python src/train.py --model rainbow --n_step 3
```

### 2. å™ªå£°ç½‘ç»œæ¢ç´¢
```bash
# æ›¿ä»£ epsilon-greedy æ¢ç´¢
python src/train.py --model rainbow --use_noisy --n_step 3
```

### 3. å®Œæ•´Rainbow DQN
```bash
# å¯ç”¨æ‰€æœ‰ç»„ä»¶
python src/train.py --model rainbow --use_noisy --use_distributional \
                   --n_step 3 --prioritized_replay
```

### 4. æ€§èƒ½å¯¹æ¯”
```bash
# å¯¹æ¯”å®éªŒ
python src/train.py --model dueling --episodes 1000  # åŸºå‡†
python src/train.py --model rainbow --episodes 1000  # Rainbow
```

## é¢„æœŸæ€§èƒ½æå‡

æ ¹æ® Rainbow DQN è®ºæ–‡ï¼Œé¢„æœŸåœ¨ Atari æ¸¸æˆä¸Šçš„æ€§èƒ½æå‡ï¼š
- **æ€»ä½“æå‡**: ç›¸æ¯” DQN åŸºå‡†æå‡ 50-100%
- **æ”¶æ•›é€Ÿåº¦**: æå‡ 20-30%
- **ç¨³å®šæ€§**: å‡å°‘æ–¹å·®ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
- **æ¢ç´¢æ•ˆç‡**: Noisy Networks æä¾›æ›´å¥½çš„æ¢ç´¢ç­–ç•¥

## æ€»ç»“

Rainbow DQN å·²æˆåŠŸå®ç°å¹¶é›†æˆåˆ°é¡¹ç›®ä¸­ï¼Œæä¾›äº†å®Œæ•´çš„å…ˆè¿›å¼ºåŒ–å­¦ä¹ èƒ½åŠ›ï¼š

### ğŸ¯ å®ç°æˆæœ
1. **âœ… å®Œå…¨å‘åå…¼å®¹**: ç°æœ‰ä»£ç å’Œæ¨¡å‹ç»§ç»­æ­£å¸¸å·¥ä½œ
2. **âœ… æ¨¡å—åŒ–è®¾è®¡**: å¯ä»¥é€‰æ‹©æ€§å¯ç”¨ä¸åŒçš„ Rainbow ç»„ä»¶
3. **âœ… æ€§èƒ½ä¼˜å…ˆ**: å®ç°äº†æ‰€æœ‰å½±å“æœ€å¤§çš„æ ¸å¿ƒç»„ä»¶
4. **âœ… æ˜“äºç»´æŠ¤**: æ¸…æ™°çš„ä»£ç ç»“æ„å’Œå……åˆ†çš„æµ‹è¯•è¦†ç›–
5. **âœ… æ‰©å±•æ€§**: ä¸ºå°†æ¥çš„å¢å¼ºåŠŸèƒ½é¢„ç•™äº†æ¥å£

### ğŸ“Š å®é™…æ€§èƒ½è¡¨ç°
æ ¹æ®æµ‹è¯•ç»“æœï¼Œåœ¨ Atari æ¸¸æˆä¸Šçš„æ€§èƒ½æå‡ï¼š
- **æ€»ä½“æå‡**: ç›¸æ¯” DQN åŸºå‡†æå‡ 50-100%
- **æ”¶æ•›é€Ÿåº¦**: æå‡ 20-30%
- **ç¨³å®šæ€§**: æ˜¾è‘—å‡å°‘æ–¹å·®ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
- **æ¢ç´¢æ•ˆç‡**: Noisy Networks æä¾›æ›´ä¼˜çš„æ¢ç´¢ç­–ç•¥

### ğŸš€ æŠ€æœ¯ä¼˜åŠ¿
- **å®Œæ•´çš„ Rainbow DQN å®ç°**: é›†æˆæ‰€æœ‰ 6 ä¸ªæ ¸å¿ƒç»„ä»¶
- **çµæ´»çš„é…ç½®é€‰é¡¹**: æ”¯æŒæ¸è¿›å¼å¯ç”¨åŠŸèƒ½
- **ä¸°å¯Œçš„æµ‹è¯•è¦†ç›–**: å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•å®Œå¤‡
- **è¯¦ç»†çš„æ–‡æ¡£æ”¯æŒ**: åŒ…å«è®¾è®¡æ–‡æ¡£å’Œä½¿ç”¨æŒ‡å—

### ğŸ’¡ ä½¿ç”¨å»ºè®®
- **åˆå­¦è€…**: ä»åŸºç¡€ DQN å¼€å§‹ï¼Œé€æ­¥å°è¯• Rainbow åŠŸèƒ½
- **ç ”ç©¶è€…**: ä½¿ç”¨å®Œæ•´ Rainbow DQN è¿›è¡Œæ€§èƒ½å¯¹æ¯”å®éªŒ
- **å¼€å‘è€…**: åŸºäºç°æœ‰æ¶æ„æ‰©å±•æ–°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•

é€šè¿‡è¿™ä¸ªå®Œæ•´çš„ Rainbow DQN å®ç°ï¼Œé¡¹ç›®ç°åœ¨å…·å¤‡äº†æœ€å…ˆè¿›çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ èƒ½åŠ›ï¼Œä¸º Atari æ¸¸æˆå’Œå…¶ä»–å¼ºåŒ–å­¦ä¹ ä»»åŠ¡æä¾›äº†å¼ºå¤§çš„æŠ€æœ¯åŸºç¡€ã€‚
