# ğŸ® è®­ç»ƒæ¨¡å‹è¯„ä¼°ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨è®­ç»ƒå¥½çš„DQN/Rainbowæ¨¡å‹è¿›è¡Œæ¸¸æˆè¯„ä¼°ï¼ŒåŒ…æ‹¬åŸºç¡€è¯„ä¼°ã€å¯è§†åŒ–æ¼”ç¤ºå’Œé«˜çº§åˆ†æåŠŸèƒ½ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºç¡€è¯„ä¼°å‘½ä»¤

```bash
# è¯„ä¼°DQNæ¨¡å‹ï¼ˆ10å›åˆï¼‰
python src/evaluate.py --model_path models/best_dqn_model.pth --n_episodes 10

# è¯„ä¼°Rainbowæ¨¡å‹
python src/evaluate.py --model_path models/best_rainbow_model.pth --model rainbow --n_episodes 10

# å¸¦å¯è§†åŒ–çš„è¯„ä¼°
python src/evaluate.py --model_path models/best_dqn_model.pth --render --n_episodes 5

# å½•åˆ¶æ¸¸æˆè§†é¢‘
python src/evaluate.py --model_path models/best_dqn_model.pth --record_video --video_path videos/
```

## ğŸ“Š è¯¦ç»†å‚æ•°è¯´æ˜

### å¿…éœ€å‚æ•°
- `--model_path`: è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆ.pthæ ¼å¼ï¼‰

### å¯é€‰å‚æ•°
- `--env`: æ¸¸æˆç¯å¢ƒåç§°ï¼ˆé»˜è®¤ï¼š"ALE/Assault-v5"ï¼‰
- `--model`: æ¨¡å‹ç±»å‹ï¼Œå¯é€‰ "dqn", "dueling", "rainbow"ï¼ˆé»˜è®¤ï¼š"dqn"ï¼‰
- `--n_episodes`: è¯„ä¼°å›åˆæ•°ï¼ˆé»˜è®¤ï¼š10ï¼‰
- `--max_steps`: æ¯å›åˆæœ€å¤§æ­¥æ•°ï¼ˆé»˜è®¤ï¼š10000ï¼‰
- `--render`: æ˜¯å¦æ˜¾ç¤ºæ¸¸æˆç”»é¢
- `--record_video`: æ˜¯å¦å½•åˆ¶æ¸¸æˆè§†é¢‘
- `--video_path`: è§†é¢‘ä¿å­˜ç›®å½•ï¼ˆé»˜è®¤ï¼š"videos"ï¼‰
- `--seed`: éšæœºç§å­ï¼ˆé»˜è®¤ï¼š42ï¼‰

## ğŸ¯ ä½¿ç”¨åœºæ™¯ç¤ºä¾‹

### 1. æ¨¡å‹æ€§èƒ½è¯„ä¼°

```bash
# å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆ100å›åˆï¼‰
python src/evaluate.py \
    --model_path models/best_rainbow_model.pth \
    --model rainbow \
    --n_episodes 100 \
    --seed 42
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```
==================================================
EVALUATION RESULTS
==================================================
Environment: ALE/Assault-v5
Model: rainbow
Episodes: 100
Mean Reward: 1247.35 Â± 234.67
Mean Episode Length: 2341.23 Â± 456.78
Best Episode Reward: 1856.00
Worst Episode Reward: 678.00
==================================================
```

### 2. å¯è§†åŒ–æ¼”ç¤º

```bash
# å®æ—¶è§‚çœ‹æ™ºèƒ½ä½“æ¸¸æˆï¼ˆé€‚åˆæ¼”ç¤ºï¼‰
python src/evaluate.py \
    --model_path models/best_dqn_model.pth \
    --render \
    --n_episodes 3 \
    --max_steps 5000
```

### 3. è§†é¢‘å½•åˆ¶

```bash
# å½•åˆ¶é«˜è´¨é‡æ¸¸æˆè§†é¢‘
python src/evaluate.py \
    --model_path models/best_rainbow_model.pth \
    --model rainbow \
    --record_video \
    --video_path videos/rainbow_demo \
    --n_episodes 5
```

### 4. ä¸åŒç¯å¢ƒæµ‹è¯•

```bash
# åœ¨ä¸åŒæ¸¸æˆç¯å¢ƒä¸­æµ‹è¯•æ¨¡å‹æ³›åŒ–èƒ½åŠ›
python src/evaluate.py \
    --model_path models/best_dqn_model.pth \
    --env "ALE/Breakout-v5" \
    --n_episodes 20
```

## ğŸ”§ é«˜çº§è¯„ä¼°åŠŸèƒ½

### åˆ›å»ºæ‰¹é‡è¯„ä¼°è„šæœ¬

```python
# batch_evaluate.py
import subprocess
import os
import json
from datetime import datetime

def batch_evaluate(model_configs, output_dir="evaluation_results"):
    """
    æ‰¹é‡è¯„ä¼°å¤šä¸ªæ¨¡å‹
    
    å‚æ•°:
        model_configs: æ¨¡å‹é…ç½®åˆ—è¡¨
        output_dir: ç»“æœä¿å­˜ç›®å½•
    """
    os.makedirs(output_dir, exist_ok=True)
    results = []
    
    for config in model_configs:
        print(f"è¯„ä¼°æ¨¡å‹: {config['name']}")
        
        # æ„å»ºè¯„ä¼°å‘½ä»¤
        cmd = [
            "python", "src/evaluate.py",
            "--model_path", config["model_path"],
            "--model", config["model_type"],
            "--n_episodes", str(config["episodes"]),
            "--seed", str(config["seed"])
        ]
        
        # æ‰§è¡Œè¯„ä¼°
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        # è§£æç»“æœ
        if result.returncode == 0:
            # è¿™é‡Œå¯ä»¥è§£æè¾“å‡ºè·å–å…·ä½“æŒ‡æ ‡
            results.append({
                "model_name": config["name"],
                "model_path": config["model_path"],
                "evaluation_time": datetime.now().isoformat(),
                "stdout": result.stdout,
                "success": True
            })
        else:
            results.append({
                "model_name": config["name"],
                "error": result.stderr,
                "success": False
            })
    
    # ä¿å­˜ç»“æœ
    with open(os.path.join(output_dir, "batch_results.json"), "w") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    configs = [
        {
            "name": "DQN_v1",
            "model_path": "models/dqn_episode_1000.pth",
            "model_type": "dqn",
            "episodes": 50,
            "seed": 42
        },
        {
            "name": "Rainbow_v1",
            "model_path": "models/rainbow_episode_1000.pth",
            "model_type": "rainbow",
            "episodes": 50,
            "seed": 42
        }
    ]
    
    results = batch_evaluate(configs)
    print(f"æ‰¹é‡è¯„ä¼°å®Œæˆï¼Œå…±è¯„ä¼° {len(configs)} ä¸ªæ¨¡å‹")
```

### åˆ›å»ºæ€§èƒ½å¯¹æ¯”åˆ†æè„šæœ¬

```python
# performance_analysis.py
import matplotlib.pyplot as plt
import numpy as np
import json
from typing import List, Dict

def analyze_model_performance(results_file: str):
    """
    åˆ†ææ¨¡å‹æ€§èƒ½å¹¶ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    
    å‚æ•°:
        results_file: æ‰¹é‡è¯„ä¼°ç»“æœæ–‡ä»¶
    """
    with open(results_file, 'r') as f:
        results = json.load(f)
    
    # æå–æ€§èƒ½æŒ‡æ ‡ï¼ˆéœ€è¦æ ¹æ®å®é™…è¾“å‡ºæ ¼å¼è°ƒæ•´ï¼‰
    model_names = []
    mean_rewards = []
    std_rewards = []
    
    for result in results:
        if result['success']:
            # è§£æstdoutè·å–æŒ‡æ ‡
            stdout = result['stdout']
            # è¿™é‡Œéœ€è¦å®ç°å…·ä½“çš„è§£æé€»è¾‘
            # ç¤ºä¾‹ï¼šä»è¾“å‡ºä¸­æå–Mean Reward
            lines = stdout.split('\n')
            for line in lines:
                if 'Mean Reward:' in line:
                    # è§£æ "Mean Reward: 1247.35 Â± 234.67"
                    parts = line.split(':')
                    if len(parts) > 1:
                        reward_part = parts[1].strip()
                        mean_val = float(reward_part.split('Â±')[0].strip())
                        std_val = float(reward_part.split('Â±')[1].strip())
                        
                        model_names.append(result['model_name'])
                        mean_rewards.append(mean_val)
                        std_rewards.append(std_val)
                        break
    
    # åˆ›å»ºå¯¹æ¯”å›¾è¡¨
    plt.figure(figsize=(12, 8))
    
    # å­å›¾1ï¼šå¹³å‡å¥–åŠ±å¯¹æ¯”
    plt.subplot(2, 2, 1)
    bars = plt.bar(model_names, mean_rewards, yerr=std_rewards, capsize=5)
    plt.title('æ¨¡å‹å¹³å‡å¥–åŠ±å¯¹æ¯”')
    plt.ylabel('å¹³å‡å¥–åŠ±')
    plt.xticks(rotation=45)
    
    # ä¸ºæ¯ä¸ªæŸ±å­æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, mean_val in zip(bars, mean_rewards):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,
                f'{mean_val:.1f}', ha='center', va='bottom')
    
    # å­å›¾2ï¼šç¨³å®šæ€§å¯¹æ¯”ï¼ˆæ ‡å‡†å·®ï¼‰
    plt.subplot(2, 2, 2)
    plt.bar(model_names, std_rewards, color='orange')
    plt.title('æ¨¡å‹ç¨³å®šæ€§å¯¹æ¯”ï¼ˆæ ‡å‡†å·®è¶Šå°è¶Šç¨³å®šï¼‰')
    plt.ylabel('å¥–åŠ±æ ‡å‡†å·®')
    plt.xticks(rotation=45)
    
    # å­å›¾3ï¼šæ•ˆç‡æŒ‡æ ‡ï¼ˆå¥–åŠ±/æ ‡å‡†å·®æ¯”ç‡ï¼‰
    plt.subplot(2, 2, 3)
    efficiency = [m/s if s > 0 else 0 for m, s in zip(mean_rewards, std_rewards)]
    plt.bar(model_names, efficiency, color='green')
    plt.title('æ¨¡å‹æ•ˆç‡æŒ‡æ ‡ï¼ˆå¥–åŠ±/æ ‡å‡†å·®ï¼‰')
    plt.ylabel('æ•ˆç‡æ¯”ç‡')
    plt.xticks(rotation=45)
    
    # å­å›¾4ï¼šç»¼åˆè¯„åˆ†
    plt.subplot(2, 2, 4)
    # å½’ä¸€åŒ–åˆ†æ•°ï¼š(å¥–åŠ± - æœ€å°å¥–åŠ±) / (æœ€å¤§å¥–åŠ± - æœ€å°å¥–åŠ±)
    if len(mean_rewards) > 1:
        min_reward = min(mean_rewards)
        max_reward = max(mean_rewards)
        normalized_scores = [(r - min_reward) / (max_reward - min_reward) 
                           for r in mean_rewards]
    else:
        normalized_scores = [1.0] * len(mean_rewards)
    
    colors = plt.cm.viridis(normalized_scores)
    bars = plt.bar(model_names, normalized_scores, color=colors)
    plt.title('æ¨¡å‹ç»¼åˆè¯„åˆ†ï¼ˆå½’ä¸€åŒ–ï¼‰')
    plt.ylabel('å½’ä¸€åŒ–åˆ†æ•°')
    plt.xticks(rotation=45)
    plt.ylim(0, 1)
    
    plt.tight_layout()
    plt.savefig('model_performance_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    generate_performance_report(model_names, mean_rewards, std_rewards)

def generate_performance_report(names, means, stds):
    """
    ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š
    """
    report = "# æ¨¡å‹æ€§èƒ½è¯„ä¼°æŠ¥å‘Š\n\n"
    report += f"è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    
    # æ’åºæ¨¡å‹ï¼ˆæŒ‰å¹³å‡å¥–åŠ±ï¼‰
    sorted_data = sorted(zip(names, means, stds), key=lambda x: x[1], reverse=True)
    
    report += "## æ¨¡å‹æ’å\n\n"
    for i, (name, mean, std) in enumerate(sorted_data, 1):
        report += f"{i}. **{name}**: {mean:.2f} Â± {std:.2f}\n"
    
    report += "\n## è¯¦ç»†åˆ†æ\n\n"
    
    best_model = sorted_data[0]
    report += f"### æœ€ä½³æ¨¡å‹: {best_model[0]}\n"
    report += f"- å¹³å‡å¥–åŠ±: {best_model[1]:.2f}\n"
    report += f"- æ ‡å‡†å·®: {best_model[2]:.2f}\n"
    report += f"- ç¨³å®šæ€§: {'é«˜' if best_model[2] < np.mean(stds) else 'ä¸­ç­‰'}\n\n"
    
    if len(sorted_data) > 1:
        improvement = best_model[1] - sorted_data[1][1]
        report += f"### æ€§èƒ½æå‡\n"
        report += f"æœ€ä½³æ¨¡å‹æ¯”ç¬¬äºŒåé«˜å‡º: {improvement:.2f} ({improvement/sorted_data[1][1]*100:.1f}%)\n\n"
    
    # ä¿å­˜æŠ¥å‘Š
    with open('performance_report.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ° performance_report.md")
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

1. **æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨**
   ```
   Error: Model file models/best_model.pth not found!
   ```
   - æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤è®­ç»ƒå·²å®Œæˆå¹¶ä¿å­˜äº†æ¨¡å‹

2. **æ¨¡å‹ç±»å‹ä¸åŒ¹é…**
   ```
   RuntimeError: Error(s) in loading state_dict
   ```
   - ç¡®ä¿ `--model` å‚æ•°ä¸è®­ç»ƒæ—¶ä½¿ç”¨çš„æ¨¡å‹ç±»å‹ä¸€è‡´
   - DQNæ¨¡å‹ä½¿ç”¨ `--model dqn`
   - Rainbowæ¨¡å‹ä½¿ç”¨ `--model rainbow`

3. **ç¯å¢ƒä¸å…¼å®¹**
   ```
   gym.error.UnregisteredEnv
   ```
   - æ£€æŸ¥ç¯å¢ƒåç§°æ˜¯å¦æ­£ç¡®
   - ç¡®ä¿å®‰è£…äº†ç›¸åº”çš„æ¸¸æˆç¯å¢ƒ

4. **æ˜¾å­˜ä¸è¶³**
   ```
   CUDA out of memory
   ```
   - å‡å°‘æ‰¹å¤„ç†å¤§å°
   - ä½¿ç”¨CPUè¯„ä¼°ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ `CUDA_VISIBLE_DEVICES=""`

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡è§£è¯»

### å…³é”®æŒ‡æ ‡è¯´æ˜

- **Mean Reward**: å¹³å‡å¥–åŠ±ï¼Œè¶Šé«˜è¶Šå¥½
- **Standard Deviation**: æ ‡å‡†å·®ï¼Œè¶Šå°è¡¨ç¤ºæ€§èƒ½è¶Šç¨³å®š
- **Best/Worst Episode**: æœ€å¥½/æœ€å·®å•å›åˆè¡¨ç°
- **Episode Length**: å›åˆé•¿åº¦ï¼Œé€šå¸¸è¶Šé•¿è¡¨ç¤ºæ™ºèƒ½ä½“å­˜æ´»æ—¶é—´è¶Šä¹…

### è¯„ä¼°å»ºè®®

1. **å……è¶³çš„è¯„ä¼°å›åˆæ•°**: å»ºè®®è‡³å°‘50-100å›åˆä»¥è·å¾—å¯é ç»Ÿè®¡
2. **å¤šç§å­æµ‹è¯•**: ä½¿ç”¨ä¸åŒéšæœºç§å­è¯„ä¼°æ¨¡å‹é²æ£’æ€§
3. **é•¿æœŸç¨³å®šæ€§**: è§‚å¯Ÿé•¿æ—¶é—´è¯„ä¼°ä¸­çš„æ€§èƒ½å˜åŒ–
4. **ç¯å¢ƒæ³›åŒ–**: åœ¨ç›¸ä¼¼ä½†ä¸åŒçš„ç¯å¢ƒä¸­æµ‹è¯•æ¨¡å‹

## ğŸ¯ æœ€ä½³å®è·µ

1. **å®šæœŸè¯„ä¼°**: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸè¯„ä¼°æ¨¡å‹æ€§èƒ½
2. **ä¿å­˜æœ€ä½³æ¨¡å‹**: åŸºäºè¯„ä¼°ç»“æœä¿å­˜è¡¨ç°æœ€å¥½çš„æ¨¡å‹
3. **æ€§èƒ½ç›‘æ§**: å»ºç«‹æ€§èƒ½ç›‘æ§ç³»ç»Ÿè·Ÿè¸ªæ¨¡å‹è¡¨ç°
4. **å¯¹æ¯”åˆ†æ**: å¯¹æ¯”ä¸åŒç®—æ³•å’Œè¶…å‚æ•°çš„æ•ˆæœ
5. **å¯è§†åŒ–åˆ†æ**: ä½¿ç”¨å›¾è¡¨ç›´è§‚å±•ç¤ºæ¨¡å‹æ€§èƒ½

é€šè¿‡æœ¬æŒ‡å—ï¼Œæ‚¨å¯ä»¥å…¨é¢è¯„ä¼°è®­ç»ƒå¥½çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œè·å¾—è¯¦ç»†çš„æ€§èƒ½åˆ†æï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥çš„æ¨¡å‹ä¼˜åŒ–æä¾›æ•°æ®æ”¯æŒã€‚