import os
import torch
import numpy as np
import argparse
import time
import os
from tqdm import tqdm

from model import DQN, DuelingDQN, RainbowDQN
from agent import DQNAgent, RainbowAgent
from utils import make_env, make_env_with_render, record_video, display_video


def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    """
    parser = argparse.ArgumentParser(description="DQNè¯„ä¼°è„šæœ¬")
    parser.add_argument("--env", type=str,
                        default="ALE/Assault-v5", help="Gymç¯å¢ƒåç§°")
    parser.add_argument("--model", type=str, default="dqn",
                        choices=["dqn", "dueling", "rainbow"], help="æ¨¡å‹ç±»å‹")
    parser.add_argument("--model_path", type=str, required=True, help="æ¨¡å‹æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--n_episodes", type=int, default=10, help="è¯„ä¼°å›åˆæ•°")
    parser.add_argument("--max_steps", type=int, default=10000, help="æ¯å›åˆæœ€å¤§æ­¥æ•°")
    parser.add_argument("--render", action="store_true", help="æ˜¯å¦æ¸²æŸ“æ¸¸æˆç”»é¢")
    parser.add_argument("--record_video", action="store_true", help="æ˜¯å¦å½•åˆ¶è§†é¢‘")
    parser.add_argument("--video_path", type=str,
                        default="videos", help="è§†é¢‘ä¿å­˜ç›®å½•")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")

    return parser.parse_args()


def evaluate_agent(args):
    """
    è¯„ä¼°è®­ç»ƒå¥½çš„DQNæ™ºèƒ½ä½“

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    # è®¾ç½®éšæœºç§å­
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    # åˆ›å»ºç¯å¢ƒ
    if args.render:
        # å¦‚æœéœ€è¦æ˜¾ç¤ºç”»é¢ï¼Œä½¿ç”¨humanæ¸²æŸ“æ¨¡å¼
        env = make_env_with_render(args.env, render_mode="human")
    else:
        # å¦åˆ™ä½¿ç”¨é»˜è®¤çš„rgb_arrayæ¨¡å¼
        env = make_env(args.env)

    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # åˆ›å»ºæ¨¡å‹
    input_shape = env.observation_space.shape
    
    # ä¿®å¤åŠ¨ä½œæ•°é‡è·å–é€»è¾‘
    if hasattr(env.action_space, 'n'):
        n_actions = env.action_space.n
    elif hasattr(env.action_space, 'shape') and len(env.action_space.shape) > 0:
        n_actions = env.action_space.shape[0]
    else:
        raise ValueError(f"æ— æ³•ç¡®å®šåŠ¨ä½œæ•°é‡ï¼ŒåŠ¨ä½œç©ºé—´ç±»å‹: {type(env.action_space)}")
    
    print(f"è¾“å…¥å½¢çŠ¶: {input_shape}, åŠ¨ä½œæ•°é‡: {n_actions}")

    if args.model == "dqn":
        model = DQN(input_shape, n_actions)
        target_model = DQN(input_shape, n_actions)
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = DQNAgent(
            model=model,
            target_model=target_model,
            env=env,
            device=device
        )
    elif args.model == "dueling":
        model = DuelingDQN(input_shape, n_actions)
        target_model = DuelingDQN(input_shape, n_actions)
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = DQNAgent(
            model=model,
            target_model=target_model,
            env=env,
            device=device
        )
    else:  # rainbow
        model = RainbowDQN(input_shape, n_actions)
        target_model = RainbowDQN(input_shape, n_actions)
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = RainbowAgent(
            model=model,
            target_model=target_model,
            env=env,
            device=device
        )

    # åŠ è½½æ¨¡å‹
    if not os.path.exists(args.model_path):
        print(f"Error: Model file {args.model_path} not found!")
        return

    print(f"Loading model from {args.model_path}")
    agent.load_model(args.model_path)

    # è¯„ä¼°ç»Ÿè®¡
    episode_rewards = []
    episode_lengths = []

    # åˆ›å»ºè§†é¢‘ä¿å­˜ç›®å½•
    if args.record_video:
        os.makedirs(args.video_path, exist_ok=True)

    print(f"Evaluating agent for {args.n_episodes} episodes...")
    if args.render:
        print("ğŸ® æ¸¸æˆç”»é¢æ˜¾ç¤ºå·²å¯ç”¨ - æ‚¨å°†çœ‹åˆ°æ™ºèƒ½ä½“çš„å®æ—¶æ¸¸æˆè¿‡ç¨‹")
        print("ğŸ’¡ æç¤º: å¯ä»¥æŒ‰Ctrl+Cæ¥æå‰åœæ­¢è¯„ä¼°")
    if args.record_video:
        print(f"ğŸ“¹ è§†é¢‘å½•åˆ¶å·²å¯ç”¨ - è§†é¢‘å°†ä¿å­˜åˆ°: {args.video_path}")

    # è¯„ä¼°å¾ªç¯
    for episode in tqdm(range(1, args.n_episodes + 1)):
        state, _ = env.reset()
        episode_reward = 0.0
        episode_length = 0
        frames = []

        done = False
        truncated = False

        # å•å›åˆå¾ªç¯
        for step in range(args.max_steps):
            # è®°å½•å¸§ï¼ˆå¦‚æœéœ€è¦å½•åˆ¶è§†é¢‘ï¼‰
            if args.record_video:
                frame = env.render()
                frames.append(frame)

            # é€‰æ‹©åŠ¨ä½œï¼ˆè¯„ä¼°æ¨¡å¼ï¼Œä¸ä½¿ç”¨æ¢ç´¢ï¼‰
            action = agent.select_action(state, evaluate=True)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, truncated, _ = env.step(action)

            # æ›´æ–°çŠ¶æ€å’Œç»Ÿè®¡
            state = next_state
            episode_reward += float(reward)
            episode_length += 1

            # å¦‚æœéœ€è¦æ¸²æŸ“ï¼Œæ˜¾ç¤ºæ¸¸æˆç”»é¢
            if args.render:
                env.render()
                time.sleep(0.05)  # é€‚å½“å»¶è¿Ÿä»¥ä¾¿è§‚çœ‹ï¼Œé¿å…ç”»é¢è¿‡å¿«

            if done or truncated:
                break

        # è®°å½•å›åˆç»Ÿè®¡
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)

        # ä¿å­˜è§†é¢‘ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if args.record_video and len(frames) > 0:
            video_filename = os.path.join(
                args.video_path, f"episode_{episode}.mp4")
            save_video(frames, video_filename)

        print(
            f"Episode {episode}: Reward = {episode_reward:.2f}, Length = {episode_length}")

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    mean_reward = np.mean(episode_rewards)
    std_reward = np.std(episode_rewards)
    mean_length = np.mean(episode_lengths)
    std_length = np.std(episode_lengths)

    print("\n" + "="*50)
    print("EVALUATION RESULTS")
    print("="*50)
    print(f"Environment: {args.env}")
    print(f"Model: {args.model}")
    print(f"Episodes: {args.n_episodes}")
    print(f"Mean Reward: {mean_reward:.2f} Â± {std_reward:.2f}")
    print(f"Mean Episode Length: {mean_length:.2f} Â± {std_length:.2f}")
    print(f"Best Episode Reward: {max(episode_rewards):.2f}")
    print(f"Worst Episode Reward: {min(episode_rewards):.2f}")
    print("="*50)

    return episode_rewards, episode_lengths


def save_video(frames, filename, fps=30):
    """
    ä¿å­˜è§†é¢‘æ–‡ä»¶

    å‚æ•°:
        frames: å¸§åˆ—è¡¨
        filename: è¾“å‡ºæ–‡ä»¶å
        fps: æ¯ç§’å¸§æ•°
    """
    try:
        import cv2

        # è·å–å¸§å°ºå¯¸
        height, width, layers = frames[0].shape

        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
        fourcc = cv2.VideoWriter.fourcc(*'mp4v')
        video_writer = cv2.VideoWriter(filename, fourcc, fps, (width, height))

        # å†™å…¥å¸§
        for frame in frames:
            # OpenCVä½¿ç”¨BGRæ ¼å¼ï¼Œéœ€è¦è½¬æ¢
            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            video_writer.write(frame_bgr)

        # é‡Šæ”¾èµ„æº
        video_writer.release()
        print(f"Video saved to {filename}")

    except ImportError:
        print("Warning: OpenCV not available, cannot save video")
    except Exception as e:
        print(f"Error saving video: {e}")


def demo_agent(args):
    """
    æ¼”ç¤ºæ™ºèƒ½ä½“ç©æ¸¸æˆ

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    # è®¾ç½®éšæœºç§å­
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    # åˆ›å»ºç¯å¢ƒï¼ˆç”¨äºæ¼”ç¤ºçš„ç¯å¢ƒéœ€è¦æ”¯æŒæ¸²æŸ“ï¼‰
    if args.render:
        env = make_env_with_render(args.env, render_mode="human")
    else:
        env = make_env(args.env)

    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # åˆ›å»ºæ¨¡å‹
    input_shape = env.observation_space.shape
    
    # ä¿®å¤åŠ¨ä½œæ•°é‡è·å–é€»è¾‘
    if hasattr(env.action_space, 'n'):
        n_actions = env.action_space.n
    elif hasattr(env.action_space, 'shape') and len(env.action_space.shape) > 0:
        n_actions = env.action_space.shape[0]
    else:
        raise ValueError(f"æ— æ³•ç¡®å®šåŠ¨ä½œæ•°é‡ï¼ŒåŠ¨ä½œç©ºé—´ç±»å‹: {type(env.action_space)}")
    
    print(f"è¾“å…¥å½¢çŠ¶: {input_shape}, åŠ¨ä½œæ•°é‡: {n_actions}")

    if args.model == "dqn":
        model = DQN(input_shape, n_actions)
        target_model = DQN(input_shape, n_actions)
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = DQNAgent(
            model=model,
            target_model=target_model,
            env=env,
            device=device
        )
    elif args.model == "dueling":
        model = DuelingDQN(input_shape, n_actions)
        target_model = DuelingDQN(input_shape, n_actions)
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = DQNAgent(
            model=model,
            target_model=target_model,
            env=env,
            device=device
        )
    else:  # rainbow
        model = RainbowDQN(input_shape, n_actions)
        target_model = RainbowDQN(input_shape, n_actions)
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = RainbowAgent(
            model=model,
            target_model=target_model,
            env=env,
            device=device
        )

    # åŠ è½½æ¨¡å‹
    print(f"Loading model from {args.model_path}")
    agent.load_model(args.model_path)

    # å½•åˆ¶è§†é¢‘
    print("Recording gameplay...")
    frames, total_reward = record_video(
        env, agent, device, max_steps=args.max_steps)

    print(f"Total reward: {total_reward}")
    print(f"Recorded {len(frames)} frames")

    # æ˜¾ç¤ºè§†é¢‘
    if frames:
        print("Displaying gameplay...")
        display_video(frames)

    return frames, total_reward


if __name__ == "__main__":
    args = parse_args()

    try:
        if args.record_video and not args.render:
            # å¦‚æœåªéœ€è¦å½•åˆ¶è§†é¢‘è€Œä¸æ˜¾ç¤ºç”»é¢ï¼Œè¿è¡Œæ¼”ç¤ºæ¨¡å¼
            print("ğŸ¬ è¿è¡Œæ¼”ç¤ºæ¨¡å¼ - å½•åˆ¶è§†é¢‘ä½†ä¸æ˜¾ç¤ºç”»é¢")
            demo_agent(args)
        else:
            # è¿è¡Œæ ‡å‡†è¯„ä¼°æ¨¡å¼ï¼ˆæ”¯æŒæ˜¾ç¤ºç”»é¢ï¼‰
            print("ğŸ“Š è¿è¡Œè¯„ä¼°æ¨¡å¼")
            evaluate_agent(args)
    except KeyboardInterrupt:
        print("\nâ¹ï¸  è¯„ä¼°å·²è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
