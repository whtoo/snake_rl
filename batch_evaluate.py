#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡æ¨¡å‹è¯„ä¼°å’Œå¯¹æ¯”è„šæœ¬

è¿™ä¸ªè„šæœ¬å¯ä»¥æ‰¹é‡è¯„ä¼°å¤šä¸ªæ¨¡å‹å¹¶ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”æŠ¥å‘Šã€‚
ä½¿ç”¨æ–¹æ³•:
    python batch_evaluate.py --models_dir models/
    python batch_evaluate.py --config evaluation_config.json
"""

import os
import sys
import json
import argparse
import subprocess
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))


def find_model_files(models_dir: str) -> List[Dict[str, str]]:
    """
    åœ¨æŒ‡å®šç›®å½•ä¸­æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
    
    å‚æ•°:
        models_dir: æ¨¡å‹ç›®å½•è·¯å¾„
    
    è¿”å›:
        æ¨¡å‹ä¿¡æ¯åˆ—è¡¨
    """
    model_files = []
    
    for file in os.listdir(models_dir):
        if file.endswith('.pth'):
            file_path = os.path.join(models_dir, file)
            
            # è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹
            if 'rainbow' in file.lower():
                model_type = 'rainbow'
            elif 'dueling' in file.lower():
                model_type = 'dueling'
            else:
                model_type = 'dqn'
            
            model_files.append({
                'name': file.replace('.pth', ''),
                'path': file_path,
                'type': model_type
            })
    
    return model_files


def run_evaluation(model_config: Dict[str, str], episodes: int = 50) -> Dict[str, Any]:
    """
    è¿è¡Œå•ä¸ªæ¨¡å‹çš„è¯„ä¼°
    
    å‚æ•°:
        model_config: æ¨¡å‹é…ç½®
        episodes: è¯„ä¼°å›åˆæ•°
    
    è¿”å›:
        è¯„ä¼°ç»“æœ
    """
    print(f"ğŸ”„ è¯„ä¼°æ¨¡å‹: {model_config['name']}")
    
    # æ„å»ºè¯„ä¼°å‘½ä»¤
    cmd = [
        sys.executable, "src/evaluate.py",
        "--model_path", model_config['path'],
        "--model", model_config['type'],
        "--n_episodes", str(episodes),
        "--seed", "42"
    ]
    
    try:
        # æ‰§è¡Œè¯„ä¼°
        start_time = time.time()
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)
        evaluation_time = time.time() - start_time
        
        if result.returncode == 0:
            # è§£æè¾“å‡º
            stdout = result.stdout
            metrics = parse_evaluation_output(stdout)
            
            return {
                'name': model_config['name'],
                'type': model_config['type'],
                'path': model_config['path'],
                'success': True,
                'evaluation_time': evaluation_time,
                'episodes': episodes,
                **metrics
            }
        else:
            return {
                'name': model_config['name'],
                'type': model_config['type'],
                'path': model_config['path'],
                'success': False,
                'error': result.stderr,
                'evaluation_time': evaluation_time
            }
    
    except subprocess.TimeoutExpired:
        return {
            'name': model_config['name'],
            'type': model_config['type'],
            'path': model_config['path'],
            'success': False,
            'error': 'Evaluation timeout (>10 minutes)',
            'evaluation_time': 600
        }
    except Exception as e:
        return {
            'name': model_config['name'],
            'type': model_config['type'],
            'path': model_config['path'],
            'success': False,
            'error': str(e),
            'evaluation_time': 0
        }


def parse_evaluation_output(stdout: str) -> Dict[str, float]:
    """
    è§£æè¯„ä¼°è¾“å‡ºè·å–æŒ‡æ ‡
    
    å‚æ•°:
        stdout: è¯„ä¼°è„šæœ¬çš„æ ‡å‡†è¾“å‡º
    
    è¿”å›:
        è§£æå‡ºçš„æŒ‡æ ‡å­—å…¸
    """
    metrics = {}
    
    lines = stdout.split('\n')
    for line in lines:
        line = line.strip()
        
        if 'Mean Reward:' in line:
            # è§£æ "Mean Reward: 1247.35 Â± 234.67"
            parts = line.split(':')
            if len(parts) > 1:
                reward_part = parts[1].strip()
                if 'Â±' in reward_part:
                    mean_val = float(reward_part.split('Â±')[0].strip())
                    std_val = float(reward_part.split('Â±')[1].strip())
                    metrics['mean_reward'] = mean_val
                    metrics['std_reward'] = std_val
        
        elif 'Mean Episode Length:' in line:
            # è§£æ "Mean Episode Length: 2341.23 Â± 456.78"
            parts = line.split(':')
            if len(parts) > 1:
                length_part = parts[1].strip()
                if 'Â±' in length_part:
                    mean_val = float(length_part.split('Â±')[0].strip())
                    std_val = float(length_part.split('Â±')[1].strip())
                    metrics['mean_length'] = mean_val
                    metrics['std_length'] = std_val
        
        elif 'Best Episode Reward:' in line:
            parts = line.split(':')
            if len(parts) > 1:
                metrics['best_reward'] = float(parts[1].strip())
        
        elif 'Worst Episode Reward:' in line:
            parts = line.split(':')
            if len(parts) > 1:
                metrics['worst_reward'] = float(parts[1].strip())
    
    return metrics


def generate_comparison_report(results: List[Dict[str, Any]], output_dir: str):
    """
    ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    
    å‚æ•°:
        results: è¯„ä¼°ç»“æœåˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # è¿‡æ»¤æˆåŠŸçš„ç»“æœ
    successful_results = [r for r in results if r['success']]
    
    if not successful_results:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„è¯„ä¼°ç»“æœï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
        return
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(successful_results)
    
    # ç”Ÿæˆå›¾è¡¨
    create_comparison_plots(df, output_dir)
    
    # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
    create_text_report(df, output_dir)
    
    # ä¿å­˜è¯¦ç»†æ•°æ®
    df.to_csv(os.path.join(output_dir, 'detailed_results.csv'), index=False)
    
    with open(os.path.join(output_dir, 'raw_results.json'), 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“Š æŠ¥å‘Šå·²ç”Ÿæˆåˆ°ç›®å½•: {output_dir}")


def create_comparison_plots(df: pd.DataFrame, output_dir: str):
    """
    åˆ›å»ºå¯¹æ¯”å›¾è¡¨
    
    å‚æ•°:
        df: ç»“æœæ•°æ®æ¡†
        output_dir: è¾“å‡ºç›®å½•
    """
    plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('æ¨¡å‹æ€§èƒ½å¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold')
    
    # 1. å¹³å‡å¥–åŠ±å¯¹æ¯”
    ax1 = axes[0, 0]
    bars1 = ax1.bar(df['name'], df['mean_reward'], yerr=df['std_reward'], 
                   capsize=5, alpha=0.8, color='skyblue')
    ax1.set_title('å¹³å‡å¥–åŠ±å¯¹æ¯”')
    ax1.set_ylabel('å¹³å‡å¥–åŠ±')
    ax1.tick_params(axis='x', rotation=45)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, value in zip(bars1, df['mean_reward']):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,
                f'{value:.1f}', ha='center', va='bottom')
    
    # 2. ç¨³å®šæ€§å¯¹æ¯”ï¼ˆå˜å¼‚ç³»æ•°ï¼‰
    ax2 = axes[0, 1]
    cv = df['std_reward'] / df['mean_reward']  # å˜å¼‚ç³»æ•°
    bars2 = ax2.bar(df['name'], cv, alpha=0.8, color='lightcoral')
    ax2.set_title('ç¨³å®šæ€§å¯¹æ¯”ï¼ˆå˜å¼‚ç³»æ•°ï¼Œè¶Šå°è¶Šç¨³å®šï¼‰')
    ax2.set_ylabel('å˜å¼‚ç³»æ•°')
    ax2.tick_params(axis='x', rotation=45)
    
    for bar, value in zip(bars2, cv):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{value:.3f}', ha='center', va='bottom')
    
    # 3. æœ€ä½³è¡¨ç°å¯¹æ¯”
    ax3 = axes[1, 0]
    bars3 = ax3.bar(df['name'], df['best_reward'], alpha=0.8, color='lightgreen')
    ax3.set_title('æœ€ä½³å•å›åˆè¡¨ç°')
    ax3.set_ylabel('æœ€ä½³å¥–åŠ±')
    ax3.tick_params(axis='x', rotation=45)
    
    for bar, value in zip(bars3, df['best_reward']):
        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,
                f'{value:.1f}', ha='center', va='bottom')
    
    # 4. ç»¼åˆè¯„åˆ†ï¼ˆå½’ä¸€åŒ–ï¼‰
    ax4 = axes[1, 1]
    # è®¡ç®—ç»¼åˆè¯„åˆ†ï¼šå¹³å‡å¥–åŠ±æƒé‡0.6 + ç¨³å®šæ€§æƒé‡0.4
    normalized_reward = (df['mean_reward'] - df['mean_reward'].min()) / (df['mean_reward'].max() - df['mean_reward'].min())
    normalized_stability = 1 - ((cv - cv.min()) / (cv.max() - cv.min()))  # ç¨³å®šæ€§è¶Šé«˜è¶Šå¥½
    composite_score = 0.6 * normalized_reward + 0.4 * normalized_stability
    
    colors = plt.cm.viridis(composite_score)
    bars4 = ax4.bar(df['name'], composite_score, color=colors, alpha=0.8)
    ax4.set_title('ç»¼åˆè¯„åˆ†ï¼ˆå¥–åŠ±60% + ç¨³å®šæ€§40%ï¼‰')
    ax4.set_ylabel('ç»¼åˆè¯„åˆ†')
    ax4.set_ylim(0, 1)
    ax4.tick_params(axis='x', rotation=45)
    
    for bar, value in zip(bars4, composite_score):
        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                f'{value:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'model_comparison.png'), 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # åˆ›å»ºè¯¦ç»†çš„æ€§èƒ½åˆ†å¸ƒå›¾
    create_performance_distribution_plot(df, output_dir)


def create_performance_distribution_plot(df: pd.DataFrame, output_dir: str):
    """
    åˆ›å»ºæ€§èƒ½åˆ†å¸ƒå›¾
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # å¥–åŠ±åˆ†å¸ƒç®±çº¿å›¾
    model_names = df['name'].tolist()
    reward_data = []
    
    # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨æ­£æ€åˆ†å¸ƒæ¨¡æ‹Ÿæ¯ä¸ªæ¨¡å‹çš„å¥–åŠ±åˆ†å¸ƒ
    for _, row in df.iterrows():
        # åŸºäºå‡å€¼å’Œæ ‡å‡†å·®ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        simulated_rewards = np.random.normal(row['mean_reward'], row['std_reward'], 100)
        reward_data.append(simulated_rewards)
    
    ax1.boxplot(reward_data, labels=model_names)
    ax1.set_title('å¥–åŠ±åˆ†å¸ƒå¯¹æ¯”')
    ax1.set_ylabel('å¥–åŠ±å€¼')
    ax1.tick_params(axis='x', rotation=45)
    
    # æ€§èƒ½é›·è¾¾å›¾ï¼ˆå¦‚æœæ¨¡å‹æ•°é‡é€‚ä¸­ï¼‰
    if len(df) <= 6:
        create_radar_chart(df, ax2)
    else:
        # å¦‚æœæ¨¡å‹å¤ªå¤šï¼Œæ˜¾ç¤ºæ’åå›¾
        df_sorted = df.sort_values('mean_reward', ascending=True)
        ax2.barh(range(len(df_sorted)), df_sorted['mean_reward'])
        ax2.set_yticks(range(len(df_sorted)))
        ax2.set_yticklabels(df_sorted['name'])
        ax2.set_title('æ¨¡å‹æ’åï¼ˆæŒ‰å¹³å‡å¥–åŠ±ï¼‰')
        ax2.set_xlabel('å¹³å‡å¥–åŠ±')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'performance_distribution.png'), 
                dpi=300, bbox_inches='tight')
    plt.close()


def create_radar_chart(df: pd.DataFrame, ax):
    """
    åˆ›å»ºé›·è¾¾å›¾
    """
    # å‡†å¤‡æ•°æ®
    categories = ['å¹³å‡å¥–åŠ±', 'ç¨³å®šæ€§', 'æœ€ä½³è¡¨ç°', 'è¯„ä¼°é€Ÿåº¦']
    
    # å½’ä¸€åŒ–æ•°æ®
    normalized_data = []
    for _, row in df.iterrows():
        reward_norm = (row['mean_reward'] - df['mean_reward'].min()) / (df['mean_reward'].max() - df['mean_reward'].min())
        stability_norm = 1 - ((row['std_reward'] / row['mean_reward']) - (df['std_reward'] / df['mean_reward']).min()) / ((df['std_reward'] / df['mean_reward']).max() - (df['std_reward'] / df['mean_reward']).min())
        best_norm = (row['best_reward'] - df['best_reward'].min()) / (df['best_reward'].max() - df['best_reward'].min())
        speed_norm = 1 - ((row['evaluation_time'] - df['evaluation_time'].min()) / (df['evaluation_time'].max() - df['evaluation_time'].min()))
        
        normalized_data.append([reward_norm, stability_norm, best_norm, speed_norm])
    
    # ç»˜åˆ¶é›·è¾¾å›¾
    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆå›¾å½¢
    
    ax.set_theta_offset(np.pi / 2)
    ax.set_theta_direction(-1)
    ax.set_thetagrids(np.degrees(angles[:-1]), categories)
    
    colors = plt.cm.Set3(np.linspace(0, 1, len(df)))
    
    for i, (_, row) in enumerate(df.iterrows()):
        values = normalized_data[i] + normalized_data[i][:1]  # é—­åˆæ•°æ®
        ax.plot(angles, values, 'o-', linewidth=2, label=row['name'], color=colors[i])
        ax.fill(angles, values, alpha=0.25, color=colors[i])
    
    ax.set_ylim(0, 1)
    ax.set_title('ç»¼åˆæ€§èƒ½é›·è¾¾å›¾')
    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))


def create_text_report(df: pd.DataFrame, output_dir: str):
    """
    åˆ›å»ºæ–‡æœ¬æŠ¥å‘Š
    """
    report = []
    report.append("# æ¨¡å‹æ€§èƒ½è¯„ä¼°æŠ¥å‘Š\n")
    report.append(f"è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    report.append(f"è¯„ä¼°æ¨¡å‹æ•°é‡: {len(df)}\n\n")
    
    # æ’åºæ¨¡å‹
    df_sorted = df.sort_values('mean_reward', ascending=False)
    
    report.append("## ğŸ“Š æ¨¡å‹æ’åï¼ˆæŒ‰å¹³å‡å¥–åŠ±ï¼‰\n\n")
    for i, (_, row) in enumerate(df_sorted.iterrows(), 1):
        report.append(f"{i}. **{row['name']}** ({row['type']})\n")
        report.append(f"   - å¹³å‡å¥–åŠ±: {row['mean_reward']:.2f} Â± {row['std_reward']:.2f}\n")
        report.append(f"   - æœ€ä½³è¡¨ç°: {row['best_reward']:.2f}\n")
        report.append(f"   - æœ€å·®è¡¨ç°: {row['worst_reward']:.2f}\n")
        report.append(f"   - å˜å¼‚ç³»æ•°: {row['std_reward']/row['mean_reward']:.3f}\n")
        report.append(f"   - è¯„ä¼°æ—¶é—´: {row['evaluation_time']:.1f}ç§’\n\n")
    
    # ç»Ÿè®¡åˆ†æ
    report.append("## ğŸ“ˆ ç»Ÿè®¡åˆ†æ\n\n")
    
    best_model = df_sorted.iloc[0]
    report.append(f"### æœ€ä½³æ¨¡å‹: {best_model['name']}\n")
    report.append(f"- æ¨¡å‹ç±»å‹: {best_model['type']}\n")
    report.append(f"- å¹³å‡å¥–åŠ±: {best_model['mean_reward']:.2f}\n")
    report.append(f"- ç¨³å®šæ€§è¯„çº§: {'ä¼˜ç§€' if best_model['std_reward']/best_model['mean_reward'] < 0.3 else 'è‰¯å¥½' if best_model['std_reward']/best_model['mean_reward'] < 0.5 else 'ä¸€èˆ¬'}\n\n")
    
    if len(df_sorted) > 1:
        second_best = df_sorted.iloc[1]
        improvement = best_model['mean_reward'] - second_best['mean_reward']
        improvement_pct = improvement / second_best['mean_reward'] * 100
        report.append(f"### æ€§èƒ½æå‡\n")
        report.append(f"æœ€ä½³æ¨¡å‹æ¯”ç¬¬äºŒåé«˜å‡º: {improvement:.2f} ({improvement_pct:.1f}%)\n\n")
    
    # æ¨¡å‹ç±»å‹åˆ†æ
    type_analysis = df.groupby('type').agg({
        'mean_reward': ['mean', 'std', 'count'],
        'std_reward': 'mean'
    }).round(2)
    
    report.append("### æ¨¡å‹ç±»å‹åˆ†æ\n\n")
    for model_type in type_analysis.index:
        count = type_analysis.loc[model_type, ('mean_reward', 'count')]
        avg_reward = type_analysis.loc[model_type, ('mean_reward', 'mean')]
        std_reward = type_analysis.loc[model_type, ('mean_reward', 'std')]
        report.append(f"- **{model_type.upper()}** ({count}ä¸ªæ¨¡å‹):\n")
        report.append(f"  - å¹³å‡å¥–åŠ±: {avg_reward:.2f} Â± {std_reward:.2f}\n")
    
    report.append("\n## ğŸ¯ å»ºè®®\n\n")
    
    # åŸºäºç»“æœç»™å‡ºå»ºè®®
    cv_threshold = 0.4
    high_variance_models = df[df['std_reward'] / df['mean_reward'] > cv_threshold]
    
    if len(high_variance_models) > 0:
        report.append("### ç¨³å®šæ€§æ”¹è¿›å»ºè®®\n")
        report.append("ä»¥ä¸‹æ¨¡å‹è¡¨ç°ä¸å¤Ÿç¨³å®šï¼Œå»ºè®®ï¼š\n")
        for _, model in high_variance_models.iterrows():
            report.append(f"- **{model['name']}**: è€ƒè™‘è°ƒæ•´è¶…å‚æ•°æˆ–å¢åŠ è®­ç»ƒæ—¶é—´\n")
        report.append("\n")
    
    # ä¿å­˜æŠ¥å‘Š
    with open(os.path.join(output_dir, 'evaluation_report.md'), 'w', encoding='utf-8') as f:
        f.writelines(report)


def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡æ¨¡å‹è¯„ä¼°å·¥å…·")
    parser.add_argument("--models_dir", type=str, help="æ¨¡å‹ç›®å½•è·¯å¾„")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--episodes", type=int, default=50, help="æ¯ä¸ªæ¨¡å‹çš„è¯„ä¼°å›åˆæ•°")
    parser.add_argument("--output_dir", type=str, default="evaluation_results", 
                       help="ç»“æœè¾“å‡ºç›®å½•")
    parser.add_argument("--parallel", type=int, default=1, 
                       help="å¹¶è¡Œè¯„ä¼°è¿›ç¨‹æ•°ï¼ˆæš‚æœªå®ç°ï¼‰")
    
    args = parser.parse_args()
    
    # è·å–æ¨¡å‹åˆ—è¡¨
    if args.config:
        # ä»é…ç½®æ–‡ä»¶åŠ è½½
        with open(args.config, 'r', encoding='utf-8') as f:
            config = json.load(f)
        model_configs = config['models']
        episodes = config.get('episodes', args.episodes)
    elif args.models_dir:
        # ä»ç›®å½•è‡ªåŠ¨å‘ç°
        model_configs = find_model_files(args.models_dir)
        episodes = args.episodes
    else:
        print("âŒ è¯·æŒ‡å®š --models_dir æˆ– --config å‚æ•°")
        return
    
    if not model_configs:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶")
        return
    
    print(f"ğŸ” å‘ç° {len(model_configs)} ä¸ªæ¨¡å‹")
    print(f"ğŸ“Š æ¯ä¸ªæ¨¡å‹å°†è¯„ä¼° {episodes} å›åˆ")
    print("="*50)
    
    # æ‰§è¡Œæ‰¹é‡è¯„ä¼°
    results = []
    for i, model_config in enumerate(model_configs, 1):
        print(f"\n[{i}/{len(model_configs)}] ", end="")
        result = run_evaluation(model_config, episodes)
        results.append(result)
        
        if result['success']:
            print(f"âœ… å®Œæˆ - å¹³å‡å¥–åŠ±: {result['mean_reward']:.2f}")
        else:
            print(f"âŒ å¤±è´¥ - {result['error']}")
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "="*50)
    print("ğŸ“Š ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
    generate_comparison_report(results, args.output_dir)
    
    # æ˜¾ç¤ºç®€è¦æ€»ç»“
    successful_results = [r for r in results if r['success']]
    if successful_results:
        best_model = max(successful_results, key=lambda x: x['mean_reward'])
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model['name']} (å¹³å‡å¥–åŠ±: {best_model['mean_reward']:.2f})")
    
    print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output_dir}")


if __name__ == "__main__":
    main()